{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90df98d0-80d2-4fb0-9ce5-aac0fd62aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88aea39b-1b78-4023-81ca-a9d9df8c0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "\n",
    "\n",
    "def get_run_logdir(parName,parValue): \n",
    "    run_id = str(time.time()) + \"_\" + str(parName) + \"_\" + str(parValue)\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "#run_logdir = get_run_logdir()\n",
    "#tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c53018-a1f4-4889-b77a-de2b155134cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden, n_neurons, optimizer, learning_rate, momentum=0):   # regression\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    \n",
    "    model.add(InputLayer(input_shape=(X_train.shape[1], ), name='Input_Layer'))\n",
    "\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        \n",
    "        \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    optimizer.learning_rate = learning_rate\n",
    "    \n",
    "    if momentum != 0:\n",
    "        optimizer.momentum = momentum\n",
    "        \n",
    "    #optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=optimizer,metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d00e112-9d1b-495e-b548-789da6316aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we clean session before experiments\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cleanSession():\n",
    "    # we clean session before experiments\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a18ca4e-4eb1-4ff1-9721-0da6e68aad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,verbose=1,min_delta=1)\n",
    "\n",
    "defaultNOL = 1\n",
    "defaultLR = 0.00001\n",
    "defaultNeuronsOnLayer = 25\n",
    "optimizer = keras.optimizers.SGD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aded9882-1dd2-40ae-a509-196caf88696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 117101.7812 - mae: 123.7935 - val_loss: 108.1399 - val_mae: 8.5337\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 163.8253 - mae: 9.5988 - val_loss: 68.6999 - val_mae: 6.7683\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 113.2871 - mae: 7.8936 - val_loss: 49.6265 - val_mae: 5.5180\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.5250 - mae: 7.2386 - val_loss: 44.5197 - val_mae: 5.2958\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.5391 - mae: 7.0076 - val_loss: 42.7563 - val_mae: 5.2981\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.4378 - mae: 7.0010 - val_loss: 42.5495 - val_mae: 5.0830\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.9827 - mae: 6.7954 - val_loss: 41.1674 - val_mae: 5.1446\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.6662 - mae: 6.7517 - val_loss: 44.3108 - val_mae: 5.4583\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.2038 - mae: 6.9364 - val_loss: 40.8578 - val_mae: 5.0159\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.3688 - mae: 6.7019 - val_loss: 44.2814 - val_mae: 5.4409\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.3176 - mae: 6.8699 - val_loss: 40.8691 - val_mae: 5.1381\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.3014 - mae: 6.7988 - val_loss: 46.5182 - val_mae: 5.6370\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.0942 - mae: 6.7629 - val_loss: 43.3289 - val_mae: 5.3657\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.8896 - mae: 6.8204 - val_loss: 40.6521 - val_mae: 5.0992\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.5033 - mae: 6.7824 - val_loss: 40.2261 - val_mae: 5.0088\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.2646 - mae: 6.7375 - val_loss: 41.3100 - val_mae: 4.9884\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.2253 - mae: 6.5342 - val_loss: 74.1952 - val_mae: 7.3707\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "# we do a trail experiment\n",
    "\n",
    "run_logdir = get_run_logdir(\"NOL\",defaultNOL)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model = build_model(defaultNOL,defaultNeuronsOnLayer,optimizer,defaultLR)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,validation_split = 0.1, callbacks=[early_stopping_cb,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f69d39e3-a0a0-4d09-a5d3-a378977228fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 4096.3271 - mae: 52.9450 - val_loss: 518.8672 - val_mae: 19.9679\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 383.5276 - mae: 16.5246 - val_loss: 252.1320 - val_mae: 13.7516\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 292.2751 - mae: 14.2545 - val_loss: 207.5724 - val_mae: 12.1838\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 269.8958 - mae: 13.5324 - val_loss: 189.7364 - val_mae: 11.4592\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 250.9415 - mae: 12.9621 - val_loss: 175.8954 - val_mae: 10.8495\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 233.2527 - mae: 12.3469 - val_loss: 165.9992 - val_mae: 10.4661\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 216.2453 - mae: 11.7595 - val_loss: 148.8138 - val_mae: 9.6089\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 205.2328 - mae: 11.4024 - val_loss: 140.2636 - val_mae: 9.2815\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 194.6108 - mae: 11.0539 - val_loss: 132.9387 - val_mae: 9.0525\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 184.9934 - mae: 10.7585 - val_loss: 126.3792 - val_mae: 8.7792\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 178.4395 - mae: 10.4784 - val_loss: 121.7285 - val_mae: 8.6168\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 170.8442 - mae: 10.1847 - val_loss: 114.9725 - val_mae: 8.3284\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 165.0835 - mae: 9.9493 - val_loss: 107.5997 - val_mae: 8.0336\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 158.3348 - mae: 9.7845 - val_loss: 103.4164 - val_mae: 7.8739\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 153.2291 - mae: 9.6478 - val_loss: 101.2452 - val_mae: 7.7734\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 148.9353 - mae: 9.3815 - val_loss: 95.8061 - val_mae: 7.5583\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 144.4723 - mae: 9.2077 - val_loss: 90.2536 - val_mae: 7.2505\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 141.0471 - mae: 9.2543 - val_loss: 86.1535 - val_mae: 7.1276\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 136.0867 - mae: 9.0232 - val_loss: 83.7605 - val_mae: 7.0151\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 132.8932 - mae: 8.8329 - val_loss: 80.7214 - val_mae: 6.8747\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 129.2280 - mae: 8.6261 - val_loss: 78.1643 - val_mae: 6.7915\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 127.3839 - mae: 8.7387 - val_loss: 74.4831 - val_mae: 6.6338\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 124.2436 - mae: 8.5018 - val_loss: 72.1079 - val_mae: 6.5173\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 122.0525 - mae: 8.4328 - val_loss: 70.6866 - val_mae: 6.4401\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 118.9229 - mae: 8.2484 - val_loss: 67.9987 - val_mae: 6.3445\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 116.2936 - mae: 8.1773 - val_loss: 66.0894 - val_mae: 6.2546\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 114.5285 - mae: 8.0871 - val_loss: 64.1046 - val_mae: 6.1673\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 112.8472 - mae: 8.0456 - val_loss: 62.6871 - val_mae: 6.1019\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 110.7878 - mae: 7.9539 - val_loss: 61.1512 - val_mae: 6.0308\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 108.8908 - mae: 7.8304 - val_loss: 60.0991 - val_mae: 5.9961\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 108.4626 - mae: 7.8667 - val_loss: 58.5848 - val_mae: 5.9278\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 106.2896 - mae: 7.7658 - val_loss: 57.5487 - val_mae: 5.8732\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 105.4503 - mae: 7.7847 - val_loss: 55.9806 - val_mae: 5.7896\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 103.6273 - mae: 7.6975 - val_loss: 54.6880 - val_mae: 5.7257\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 102.4297 - mae: 7.6092 - val_loss: 53.4230 - val_mae: 5.6657\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 101.3095 - mae: 7.6010 - val_loss: 52.1073 - val_mae: 5.5894\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 99.7458 - mae: 7.4850 - val_loss: 51.1589 - val_mae: 5.5465\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 98.7434 - mae: 7.4926 - val_loss: 50.0886 - val_mae: 5.4877\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.7784 - mae: 7.3528 - val_loss: 49.3774 - val_mae: 5.4656\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.1218 - mae: 7.4585 - val_loss: 48.1659 - val_mae: 5.3839\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 95.8636 - mae: 7.2896 - val_loss: 47.0947 - val_mae: 5.3411\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.6603 - mae: 7.3274 - val_loss: 46.2868 - val_mae: 5.2914\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.0573 - mae: 7.2532 - val_loss: 45.5182 - val_mae: 5.2577\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 93.2147 - mae: 7.1848 - val_loss: 44.7763 - val_mae: 5.2311\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 92.3976 - mae: 7.2071 - val_loss: 44.1425 - val_mae: 5.1912\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.7501 - mae: 7.1649 - val_loss: 43.5403 - val_mae: 5.1457\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.2932 - mae: 7.1613 - val_loss: 42.8852 - val_mae: 5.1542\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 90.3491 - mae: 7.1059 - val_loss: 42.4343 - val_mae: 5.0816\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.9770 - mae: 7.0379 - val_loss: 41.8225 - val_mae: 5.0564\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 89.4655 - mae: 6.9714 - val_loss: 41.8056 - val_mae: 5.1257\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.0604 - mae: 7.0725 - val_loss: 41.3184 - val_mae: 5.0982\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.9648 - mae: 6.9764 - val_loss: 40.5908 - val_mae: 5.0564\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.3446 - mae: 6.9756 - val_loss: 39.9802 - val_mae: 5.0146\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.6086 - mae: 6.9429 - val_loss: 39.5035 - val_mae: 4.9862\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.1889 - mae: 6.9299 - val_loss: 38.9171 - val_mae: 4.9362\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.2460 - mae: 6.8773 - val_loss: 38.5089 - val_mae: 4.8895\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.7047 - mae: 6.8136 - val_loss: 38.3135 - val_mae: 4.9085\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 84.9725 - mae: 6.8493 - val_loss: 37.8553 - val_mae: 4.8478\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.9527 - mae: 6.7607 - val_loss: 38.0235 - val_mae: 4.9045\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.3649 - mae: 6.8252 - val_loss: 37.4431 - val_mae: 4.8600\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.2968 - mae: 6.7679 - val_loss: 37.4086 - val_mae: 4.8668\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.6266 - mae: 6.7617 - val_loss: 36.5059 - val_mae: 4.7738\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.7718 - mae: 6.7029 - val_loss: 37.4196 - val_mae: 4.9130\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 83.0666 - mae: 6.7883 - val_loss: 36.6177 - val_mae: 4.8352\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 83.1327 - mae: 6.7579 - val_loss: 35.6220 - val_mae: 4.7254\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 82.0876 - mae: 6.6189 - val_loss: 35.8229 - val_mae: 4.7674\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 82.3331 - mae: 6.6238 - val_loss: 35.7737 - val_mae: 4.7863\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 81.5658 - mae: 6.6455 - val_loss: 35.5561 - val_mae: 4.7747\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 85.9762 - mae: 6.85 - 0s 2ms/step - loss: 80.9480 - mae: 6.6529 - val_loss: 35.1808 - val_mae: 4.7401\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.6489 - mae: 6.5368 - val_loss: 36.3956 - val_mae: 4.9041\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.7105 - mae: 6.6705 - val_loss: 34.4705 - val_mae: 4.6750\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.0918 - mae: 6.5734 - val_loss: 34.1701 - val_mae: 4.6644\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.1812 - mae: 6.5387 - val_loss: 34.9231 - val_mae: 4.7737\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.5574 - mae: 6.5938 - val_loss: 33.7341 - val_mae: 4.6361\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.7513 - mae: 6.5258 - val_loss: 33.5602 - val_mae: 4.6318\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.2537 - mae: 6.5011 - val_loss: 35.0209 - val_mae: 4.8155\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.7886 - mae: 6.5955 - val_loss: 33.0026 - val_mae: 4.5911\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.6693 - mae: 6.4540 - val_loss: 33.1897 - val_mae: 4.6354\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.2194 - mae: 6.4199 - val_loss: 33.2253 - val_mae: 4.6503\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.1857 - mae: 6.4587 - val_loss: 33.4988 - val_mae: 4.6908\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.1344 - mae: 6.4478 - val_loss: 33.6754 - val_mae: 4.7154\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.2657 - mae: 6.4818 - val_loss: 33.2705 - val_mae: 4.6807\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.6901 - mae: 6.4634 - val_loss: 32.0438 - val_mae: 4.5601\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.4161 - mae: 6.3554 - val_loss: 32.3329 - val_mae: 4.6064\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.3450 - mae: 6.4060 - val_loss: 33.0492 - val_mae: 4.6848\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.3299 - mae: 6.4813 - val_loss: 31.4237 - val_mae: 4.4963\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.7859 - mae: 6.3321 - val_loss: 31.4865 - val_mae: 4.5420\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.5703 - mae: 6.3781 - val_loss: 31.2529 - val_mae: 4.5160\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.4193 - mae: 6.3294 - val_loss: 31.2666 - val_mae: 4.5359\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.1666 - mae: 6.3252 - val_loss: 30.9632 - val_mae: 4.4977\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.7272 - mae: 6.2681 - val_loss: 32.0039 - val_mae: 4.6245\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.8154 - mae: 6.3581 - val_loss: 33.1172 - val_mae: 4.7542\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.8077 - mae: 6.4323 - val_loss: 30.5668 - val_mae: 4.4835\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.4736 - mae: 6.2833 - val_loss: 30.5726 - val_mae: 4.4989\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.1027 - mae: 6.2841 - val_loss: 31.0749 - val_mae: 4.5629\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.4805 - mae: 6.3458 - val_loss: 30.1364 - val_mae: 4.4436\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.1733 - mae: 6.2695 - val_loss: 30.2671 - val_mae: 4.4892\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.9492 - mae: 6.2713 - val_loss: 31.2609 - val_mae: 4.6047\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 74.6842 - mae: 6.2767 - val_loss: 31.2176 - val_mae: 4.6009\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 74.4623 - mae: 6.3371 - val_loss: 30.0064 - val_mae: 4.4825\n",
      "74.46226501464844 : loss history ----\n",
      "6.268055438995361 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 154952.1719 - mae: 139.2766 - val_loss: 93.1317 - val_mae: 8.3935\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 126.7701 - mae: 8.8418 - val_loss: 67.5398 - val_mae: 7.0448\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 104.5025 - mae: 7.9278 - val_loss: 54.3043 - val_mae: 6.0271\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.7789 - mae: 7.5516 - val_loss: 49.8669 - val_mae: 5.7833\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.8657 - mae: 7.4181 - val_loss: 48.3643 - val_mae: 5.7447\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.5563 - mae: 7.3624 - val_loss: 46.4693 - val_mae: 5.4405\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.2230 - mae: 7.2277 - val_loss: 45.8077 - val_mae: 5.4510\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 90.8821 - mae: 7.1939 - val_loss: 50.5788 - val_mae: 5.9678\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.1642 - mae: 7.3192 - val_loss: 45.1478 - val_mae: 5.3295\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 90.1453 - mae: 7.1598 - val_loss: 48.7451 - val_mae: 5.8150\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.3457 - mae: 7.3031 - val_loss: 44.9786 - val_mae: 5.3916\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 90.9154 - mae: 7.1688 - val_loss: 50.6042 - val_mae: 5.9953\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 90.9057 - mae: 7.2004 - val_loss: 45.6782 - val_mae: 5.5222\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.5755 - mae: 7.1986 - val_loss: 44.5379 - val_mae: 5.3355\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.0729 - mae: 7.1896 - val_loss: 44.4055 - val_mae: 5.3161\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.4409 - mae: 7.1454 - val_loss: 46.2777 - val_mae: 5.3605\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 88.4232 - mae: 6.9615 - val_loss: 78.0521 - val_mae: 7.6669\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.4249 - mae: 7.4176 - val_loss: 44.6298 - val_mae: 5.2828\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.4636 - mae: 7.0313 - val_loss: 50.9623 - val_mae: 6.0317\n",
      "Epoch 00019: early stopping\n",
      "88.42317962646484 : loss history ----\n",
      "6.961516380310059 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 884318656.0000 - mae: 11736.8398 - val_loss: 683.2122 - val_mae: 25.3087\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 778.6398 - mae: 26.2564 - val_loss: 680.0544 - val_mae: 25.2463\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 775.3668 - mae: 26.1936 - val_loss: 676.9111 - val_mae: 25.1840\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 772.1083 - mae: 26.1313 - val_loss: 673.7555 - val_mae: 25.1212\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 768.8371 - mae: 26.0687 - val_loss: 670.6077 - val_mae: 25.0585\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 765.5745 - mae: 26.0054 - val_loss: 667.4983 - val_mae: 24.9964\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 762.3500 - mae: 25.9441 - val_loss: 664.3867 - val_mae: 24.9341\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 759.1233 - mae: 25.8820 - val_loss: 661.2794 - val_mae: 24.8717\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 755.9027 - mae: 25.8198 - val_loss: 658.2285 - val_mae: 24.8103\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 752.7368 - mae: 25.7581 - val_loss: 655.1450 - val_mae: 24.7480\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 749.5394 - mae: 25.6956 - val_loss: 652.0958 - val_mae: 24.6864\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 746.3771 - mae: 25.6340 - val_loss: 649.0667 - val_mae: 24.6249\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 743.2349 - mae: 25.5730 - val_loss: 646.0626 - val_mae: 24.5639\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 740.1172 - mae: 25.5116 - val_loss: 643.0538 - val_mae: 24.5025\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 736.9955 - mae: 25.4505 - val_loss: 640.0645 - val_mae: 24.4415\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 733.8945 - mae: 25.3897 - val_loss: 637.1050 - val_mae: 24.3808\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 730.8210 - mae: 25.3295 - val_loss: 634.1030 - val_mae: 24.3192\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 727.7080 - mae: 25.2676 - val_loss: 631.1818 - val_mae: 24.2591\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 724.6743 - mae: 25.2073 - val_loss: 628.2346 - val_mae: 24.1982\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 721.6154 - mae: 25.1465 - val_loss: 625.3298 - val_mae: 24.1381\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 718.5986 - mae: 25.0865 - val_loss: 622.3874 - val_mae: 24.0771\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 715.5447 - mae: 25.0262 - val_loss: 619.4946 - val_mae: 24.0170\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 712.5414 - mae: 24.9658 - val_loss: 616.6320 - val_mae: 23.9573\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 709.5682 - mae: 24.9057 - val_loss: 613.7658 - val_mae: 23.8974\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 706.5916 - mae: 24.8461 - val_loss: 610.9131 - val_mae: 23.8376\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 703.6281 - mae: 24.7862 - val_loss: 608.0808 - val_mae: 23.7782\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 700.6868 - mae: 24.7271 - val_loss: 605.2809 - val_mae: 23.7192\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 697.7769 - mae: 24.6681 - val_loss: 602.4587 - val_mae: 23.6597\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 694.8460 - mae: 24.6084 - val_loss: 599.6724 - val_mae: 23.6007\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 691.9509 - mae: 24.5501 - val_loss: 596.9073 - val_mae: 23.5420\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 689.0764 - mae: 24.4917 - val_loss: 594.1285 - val_mae: 23.4829\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 686.1886 - mae: 24.4325 - val_loss: 591.3799 - val_mae: 23.4244\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 683.3318 - mae: 24.3739 - val_loss: 588.6418 - val_mae: 23.3658\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 680.4846 - mae: 24.3152 - val_loss: 585.8918 - val_mae: 23.3069\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 677.6264 - mae: 24.2563 - val_loss: 583.1712 - val_mae: 23.2485\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 674.7987 - mae: 24.1978 - val_loss: 580.4868 - val_mae: 23.1907\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 672.0063 - mae: 24.1403 - val_loss: 577.7931 - val_mae: 23.1325\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 669.2062 - mae: 24.0819 - val_loss: 575.1378 - val_mae: 23.0751\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 666.4437 - mae: 24.0248 - val_loss: 572.4745 - val_mae: 23.0173\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 663.6746 - mae: 23.9670 - val_loss: 569.8549 - val_mae: 22.9603\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 660.9484 - mae: 23.9096 - val_loss: 567.2001 - val_mae: 22.9024\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 658.1871 - mae: 23.8515 - val_loss: 564.5833 - val_mae: 22.8452\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 655.4641 - mae: 23.7951 - val_loss: 561.9748 - val_mae: 22.7880\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 652.7504 - mae: 23.7385 - val_loss: 559.3881 - val_mae: 22.7312\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 650.0583 - mae: 23.6807 - val_loss: 556.8100 - val_mae: 22.6744\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 647.3759 - mae: 23.6241 - val_loss: 554.2618 - val_mae: 22.6182\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 644.7227 - mae: 23.5678 - val_loss: 551.6956 - val_mae: 22.5614\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 642.0522 - mae: 23.5117 - val_loss: 549.1824 - val_mae: 22.5056\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 639.4354 - mae: 23.4555 - val_loss: 546.6624 - val_mae: 22.4496\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 636.8115 - mae: 23.4001 - val_loss: 544.1452 - val_mae: 22.3934\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 634.1896 - mae: 23.3437 - val_loss: 541.6321 - val_mae: 22.3372\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 631.5728 - mae: 23.2878 - val_loss: 539.1402 - val_mae: 22.2814\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 628.9777 - mae: 23.2315 - val_loss: 536.6574 - val_mae: 22.2256\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 626.3914 - mae: 23.1760 - val_loss: 534.1730 - val_mae: 22.1697\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 623.8029 - mae: 23.1197 - val_loss: 531.7126 - val_mae: 22.1141\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 621.2403 - mae: 23.0642 - val_loss: 529.2723 - val_mae: 22.0588\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 618.6967 - mae: 23.0093 - val_loss: 526.8259 - val_mae: 22.0033\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 616.1479 - mae: 22.9537 - val_loss: 524.4119 - val_mae: 21.9484\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 613.6320 - mae: 22.8993 - val_loss: 521.9950 - val_mae: 21.8933\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 611.1125 - mae: 22.8439 - val_loss: 519.5916 - val_mae: 21.8383\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 608.6077 - mae: 22.7891 - val_loss: 517.2018 - val_mae: 21.7835\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 606.1165 - mae: 22.7343 - val_loss: 514.8394 - val_mae: 21.7292\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 603.6531 - mae: 22.6803 - val_loss: 512.4659 - val_mae: 21.6746\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 601.1788 - mae: 22.6251 - val_loss: 510.1240 - val_mae: 21.6205\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 598.7368 - mae: 22.5711 - val_loss: 507.7986 - val_mae: 21.5666\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 596.3102 - mae: 22.5176 - val_loss: 505.4547 - val_mae: 21.5122\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 593.8658 - mae: 22.4638 - val_loss: 503.1339 - val_mae: 21.4582\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 591.4444 - mae: 22.4094 - val_loss: 500.8107 - val_mae: 21.4040\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 589.0204 - mae: 22.3551 - val_loss: 498.5054 - val_mae: 21.3501\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 586.6154 - mae: 22.3021 - val_loss: 496.2176 - val_mae: 21.2964\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.2285 - mae: 22.2480 - val_loss: 493.9607 - val_mae: 21.2434\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.8731 - mae: 22.1945 - val_loss: 491.7133 - val_mae: 21.1904\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.5271 - mae: 22.1415 - val_loss: 489.4606 - val_mae: 21.1372\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.1756 - mae: 22.0883 - val_loss: 487.2270 - val_mae: 21.0843\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.8440 - mae: 22.0356 - val_loss: 485.0085 - val_mae: 21.0316\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.5270 - mae: 21.9831 - val_loss: 482.7842 - val_mae: 20.9787\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 570.2049 - mae: 21.9299 - val_loss: 480.5860 - val_mae: 20.9262\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 567.9088 - mae: 21.8775 - val_loss: 478.3848 - val_mae: 20.8736\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 565.6102 - mae: 21.8256 - val_loss: 476.2147 - val_mae: 20.8215\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 563.3427 - mae: 21.7735 - val_loss: 474.0384 - val_mae: 20.7692\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 561.0689 - mae: 21.7213 - val_loss: 471.8620 - val_mae: 20.7167\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 558.7958 - mae: 21.6688 - val_loss: 469.7089 - val_mae: 20.6647\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 556.5457 - mae: 21.6165 - val_loss: 467.5710 - val_mae: 20.6129\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 554.3113 - mae: 21.5650 - val_loss: 465.4383 - val_mae: 20.5611\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 552.0822 - mae: 21.5132 - val_loss: 463.3063 - val_mae: 20.5092\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 549.8550 - mae: 21.4610 - val_loss: 461.2130 - val_mae: 20.4581\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 547.6656 - mae: 21.4104 - val_loss: 459.1138 - val_mae: 20.4067\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 545.4706 - mae: 21.3589 - val_loss: 457.0239 - val_mae: 20.3554\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 543.2857 - mae: 21.3078 - val_loss: 454.9531 - val_mae: 20.3045\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 541.1207 - mae: 21.2573 - val_loss: 452.9000 - val_mae: 20.2539\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 538.9725 - mae: 21.2064 - val_loss: 450.8374 - val_mae: 20.2029\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 536.8146 - mae: 21.1553 - val_loss: 448.7746 - val_mae: 20.1518\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 534.6570 - mae: 21.1039 - val_loss: 446.7335 - val_mae: 20.1011\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 532.5227 - mae: 21.0536 - val_loss: 444.7326 - val_mae: 20.0513\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 530.4279 - mae: 21.0036 - val_loss: 442.7150 - val_mae: 20.0009\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 528.3171 - mae: 20.9538 - val_loss: 440.7088 - val_mae: 19.9507\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 526.2169 - mae: 20.9032 - val_loss: 438.7006 - val_mae: 19.9003\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 524.1143 - mae: 20.8531 - val_loss: 436.6877 - val_mae: 19.8496\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 522.0079 - mae: 20.8030 - val_loss: 434.7171 - val_mae: 19.7999\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 519.9441 - mae: 20.7531 - val_loss: 432.7440 - val_mae: 19.7500\n",
      "519.944091796875 : loss history ----\n",
      "20.753137588500977 : mae history ----\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# first experiment\n",
    "\n",
    "# we do a trail experiment\n",
    "\n",
    "firstExpResults = []\n",
    "learningRates = [10**-6,10**-5,10**-4]\n",
    "\n",
    "for lr in learningRates:\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    run_logdir = get_run_logdir(\"lr\",lr)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    model = build_model(defaultNOL,defaultNeuronsOnLayer,optimizer,lr)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100,validation_split = 0.1, callbacks=[early_stopping_cb,tensorboard_cb])\n",
    "    history.history['loss'].sort()\n",
    "    \n",
    "    lossHis = history.history[\"loss\"][0]\n",
    "    print(str(lossHis) + \" : loss history ----\")\n",
    "    \n",
    "    history.history['mae'].sort()\n",
    "    maeHis = history.history['mae'][0]\n",
    "    print(str(maeHis) + \" : mae history ----\")\n",
    "\n",
    "    firstExpResults.append((lr,lossHis,maeHis))\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "pickle_out = open(\"lr.pickle\",\"wb\")\n",
    "pickle.dump(firstExpResults, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "cleanSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781606d-8ca7-4564-b715-757f54820dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72845da-b423-460f-8de7-1b8dde283602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1583794685325869056.0000 - mae: 328082368.0000 - val_loss: 1595458574053665669120.0000 - val_mae: 39163842560.0000\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 323098329424236652833538550108520448.0000 - mae: 147227355738275840.0000 - val_loss: inf - val_mae: 18374885491276251136.0000\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: inf - mae: 75301947433693732382179328.0000 - val_loss: inf - val_mae: 7314203680131459543010902016.0000\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: inf - mae: 29802695868212980309918623595495424.0000 - val_loss: inf - val_mae: inf\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "1.583794685325869e+18 : loss history ----\n",
      "328082368.0 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 49304.4023 - mae: 102.0660 - val_loss: 488.8295 - val_mae: 20.5514\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 583.0430 - mae: 21.8162 - val_loss: 488.2701 - val_mae: 20.5375\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 582.6531 - mae: 21.8071 - val_loss: 487.6323 - val_mae: 20.5197\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 582.1759 - mae: 21.7912 - val_loss: 486.8323 - val_mae: 20.4965\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 581.5822 - mae: 21.7717 - val_loss: 485.5856 - val_mae: 20.4583\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 580.6550 - mae: 21.7454 - val_loss: 483.8947 - val_mae: 20.4186\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.0948 - mae: 21.6138 - val_loss: 473.9366 - val_mae: 20.1609\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 552.8912 - mae: 20.9574 - val_loss: 462.1070 - val_mae: 19.7701\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 535.5048 - mae: 20.3271 - val_loss: 454.1260 - val_mae: 19.2399\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 503.5667 - mae: 19.3917 - val_loss: 397.8847 - val_mae: 17.8716\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 384.1223 - mae: 16.7030 - val_loss: 248.6538 - val_mae: 14.1612\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 286.3555 - mae: 14.0167 - val_loss: 200.2126 - val_mae: 12.5357\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 244.1942 - mae: 12.8545 - val_loss: 173.7252 - val_mae: 11.5918\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 220.7020 - mae: 12.1026 - val_loss: 154.7316 - val_mae: 10.6691\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 204.0836 - mae: 11.5318 - val_loss: 137.0617 - val_mae: 10.1831\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 195.5248 - mae: 11.0895 - val_loss: 125.3913 - val_mae: 9.6664\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 179.5619 - mae: 10.5806 - val_loss: 151.0830 - val_mae: 9.6412\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 189.6973 - mae: 10.7641 - val_loss: 137.0270 - val_mae: 9.8983\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 169.4805 - mae: 10.0488 - val_loss: 101.7683 - val_mae: 8.6582\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 159.9448 - mae: 9.7695 - val_loss: 113.4023 - val_mae: 8.9676\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 155.3405 - mae: 9.5243 - val_loss: 168.5399 - val_mae: 9.8885\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 154.1544 - mae: 9.5168 - val_loss: 87.5542 - val_mae: 7.9544\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 150.4753 - mae: 9.3874 - val_loss: 80.6711 - val_mae: 7.6631\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 147.3715 - mae: 9.0627 - val_loss: 80.3487 - val_mae: 7.5928\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 135.9271 - mae: 8.7070 - val_loss: 73.9266 - val_mae: 7.0000\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 127.2668 - mae: 8.3381 - val_loss: 70.2020 - val_mae: 6.8132\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 124.5458 - mae: 8.2382 - val_loss: 65.1815 - val_mae: 6.8581\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.3288 - mae: 8.0048 - val_loss: 74.8985 - val_mae: 6.7143\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 124.4066 - mae: 8.0748 - val_loss: 59.0421 - val_mae: 6.2634\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 112.7915 - mae: 7.6410 - val_loss: 57.2772 - val_mae: 6.3285\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 110.8473 - mae: 7.5915 - val_loss: 60.3074 - val_mae: 6.1071\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 108.1169 - mae: 7.4578 - val_loss: 50.6345 - val_mae: 5.8165\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 107.0421 - mae: 7.4291 - val_loss: 50.5111 - val_mae: 5.6482\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 103.9725 - mae: 7.3341 - val_loss: 55.1834 - val_mae: 5.8256\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 101.1799 - mae: 7.1666 - val_loss: 53.2039 - val_mae: 5.7522\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 101.5577 - mae: 7.4365 - val_loss: 44.6918 - val_mae: 5.3749\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 97.3084 - mae: 7.1515 - val_loss: 51.4545 - val_mae: 5.6959\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 98.3035 - mae: 7.2335 - val_loss: 45.8942 - val_mae: 5.3762\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 96.0263 - mae: 7.1254 - val_loss: 46.0631 - val_mae: 5.3613\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.6381 - mae: 7.2996 - val_loss: 48.9876 - val_mae: 5.5382\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.2663 - mae: 7.1442 - val_loss: 49.3654 - val_mae: 5.6179\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.4560 - mae: 7.2286 - val_loss: 41.8924 - val_mae: 5.0343\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.9080 - mae: 7.0746 - val_loss: 41.8574 - val_mae: 5.0429\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.6354 - mae: 7.1268 - val_loss: 42.6107 - val_mae: 5.0874\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 94.4363 - mae: 7.1297 - val_loss: 41.8570 - val_mae: 5.0458\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.6871 - mae: 7.1924 - val_loss: 42.1948 - val_mae: 5.0433\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.8822 - mae: 7.2155 - val_loss: 44.3407 - val_mae: 5.2312\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.6571 - mae: 7.2427 - val_loss: 48.6483 - val_mae: 5.5421\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 95.7448 - mae: 7.1669 - val_loss: 43.2836 - val_mae: 5.1168\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.1163 - mae: 7.1620 - val_loss: 44.2962 - val_mae: 5.2377\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.1232 - mae: 7.2536 - val_loss: 45.1906 - val_mae: 5.3369\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.2150 - mae: 7.1854 - val_loss: 43.3781 - val_mae: 5.1480\n",
      "Epoch 00052: early stopping\n",
      "93.21497344970703 : loss history ----\n",
      "7.074613571166992 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 26896.7793 - mae: 85.0626 - val_loss: 563.0822 - val_mae: 22.9134\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 620.7211 - mae: 23.2015 - val_loss: 519.9012 - val_mae: 21.9447\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 602.3135 - mae: 22.7079 - val_loss: 515.5817 - val_mae: 21.8417\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 597.4949 - mae: 22.5563 - val_loss: 502.7249 - val_mae: 21.3111\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 590.5579 - mae: 22.2405 - val_loss: 497.2986 - val_mae: 21.0783\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 588.1007 - mae: 22.1074 - val_loss: 494.2667 - val_mae: 20.9082\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 586.3676 - mae: 21.9971 - val_loss: 494.1682 - val_mae: 20.8529\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 585.0400 - mae: 21.9279 - val_loss: 491.9170 - val_mae: 20.7457\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.1975 - mae: 21.8887 - val_loss: 491.2035 - val_mae: 20.7002\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 583.5989 - mae: 21.8633 - val_loss: 489.5533 - val_mae: 20.6562\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 583.0451 - mae: 21.8515 - val_loss: 489.0247 - val_mae: 20.6165\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 582.7079 - mae: 21.8323 - val_loss: 490.6124 - val_mae: 20.6955\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 582.6535 - mae: 21.8514 - val_loss: 488.5239 - val_mae: 20.5995\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.8026 - mae: 21.8151 - val_loss: 487.5606 - val_mae: 20.5496\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.3427 - mae: 21.7962 - val_loss: 487.6847 - val_mae: 20.5607\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.0897 - mae: 21.7869 - val_loss: 487.0311 - val_mae: 20.5304\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.7014 - mae: 21.7815 - val_loss: 486.4013 - val_mae: 20.4976\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.4240 - mae: 21.7710 - val_loss: 486.0453 - val_mae: 20.4900\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.0085 - mae: 21.7597 - val_loss: 486.2045 - val_mae: 20.5088\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.7666 - mae: 21.7592 - val_loss: 485.9199 - val_mae: 20.5019\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.4434 - mae: 21.7509 - val_loss: 485.1614 - val_mae: 20.4708\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.0247 - mae: 21.7402 - val_loss: 484.9649 - val_mae: 20.4678\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 578.7264 - mae: 21.7336 - val_loss: 484.5259 - val_mae: 20.4545\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 578.4498 - mae: 21.7263 - val_loss: 484.3831 - val_mae: 20.4524\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 578.1217 - mae: 21.7227 - val_loss: 484.1908 - val_mae: 20.4460\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.8810 - mae: 21.7153 - val_loss: 484.1394 - val_mae: 20.4488\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.4555 - mae: 21.7005 - val_loss: 483.3876 - val_mae: 20.4302\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.0310 - mae: 21.6929 - val_loss: 483.7978 - val_mae: 20.4438\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.6938 - mae: 21.6847 - val_loss: 483.7142 - val_mae: 20.4434\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.5439 - mae: 21.6827 - val_loss: 484.4748 - val_mae: 20.4960\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.4750 - mae: 21.6880 - val_loss: 482.5692 - val_mae: 20.4132\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.8006 - mae: 21.6671 - val_loss: 482.2126 - val_mae: 20.4026\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.4807 - mae: 21.6617 - val_loss: 481.5307 - val_mae: 20.3836\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.1749 - mae: 21.6566 - val_loss: 481.4630 - val_mae: 20.3835\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.8620 - mae: 21.6436 - val_loss: 481.0301 - val_mae: 20.3718\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.4808 - mae: 21.6341 - val_loss: 480.6797 - val_mae: 20.3620\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.1685 - mae: 21.6302 - val_loss: 480.4126 - val_mae: 20.3538\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.8270 - mae: 21.6184 - val_loss: 480.3111 - val_mae: 20.3514\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.5535 - mae: 21.6142 - val_loss: 480.2722 - val_mae: 20.3633\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.1639 - mae: 21.6043 - val_loss: 479.8627 - val_mae: 20.3497\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.8523 - mae: 21.5936 - val_loss: 479.4469 - val_mae: 20.3304\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.6762 - mae: 21.5933 - val_loss: 479.2248 - val_mae: 20.3320\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.2216 - mae: 21.5813 - val_loss: 478.8813 - val_mae: 20.3183\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 571.8312 - mae: 21.5700 - val_loss: 478.9114 - val_mae: 20.3208\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 571.5553 - mae: 21.5640 - val_loss: 478.6332 - val_mae: 20.3132\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 571.2181 - mae: 21.5579 - val_loss: 478.2215 - val_mae: 20.3174\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 570.9126 - mae: 21.5463 - val_loss: 478.1225 - val_mae: 20.3098\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 570.5683 - mae: 21.5365 - val_loss: 478.1393 - val_mae: 20.3023\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 570.1552 - mae: 21.5271 - val_loss: 478.0493 - val_mae: 20.2994\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 570.0034 - mae: 21.5256 - val_loss: 477.1828 - val_mae: 20.2888\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 569.5832 - mae: 21.5132 - val_loss: 477.2882 - val_mae: 20.2959\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 569.2235 - mae: 21.5027 - val_loss: 477.1130 - val_mae: 20.2884\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 569.1816 - mae: 21.5050 - val_loss: 476.1478 - val_mae: 20.2707\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 568.6310 - mae: 21.4947 - val_loss: 476.8240 - val_mae: 20.2994\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 568.7166 - mae: 21.4983 - val_loss: 475.6972 - val_mae: 20.2614\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 567.9083 - mae: 21.4735 - val_loss: 476.2517 - val_mae: 20.2928\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 567.7026 - mae: 21.4740 - val_loss: 475.3703 - val_mae: 20.2565\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 567.3118 - mae: 21.4573 - val_loss: 474.7494 - val_mae: 20.2333\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 567.1732 - mae: 21.4588 - val_loss: 475.1840 - val_mae: 20.2640\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 566.6205 - mae: 21.4425 - val_loss: 475.2732 - val_mae: 20.2799\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 566.3176 - mae: 21.4333 - val_loss: 475.0030 - val_mae: 20.2681\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 565.9517 - mae: 21.4267 - val_loss: 474.2799 - val_mae: 20.2431\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 565.6487 - mae: 21.4185 - val_loss: 474.6044 - val_mae: 20.2665\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 565.2764 - mae: 21.4076 - val_loss: 475.0730 - val_mae: 20.3107\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 564.9275 - mae: 21.3998 - val_loss: 474.4041 - val_mae: 20.2741\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 564.5986 - mae: 21.3909 - val_loss: 474.4525 - val_mae: 20.2839\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 564.2714 - mae: 21.3838 - val_loss: 474.0928 - val_mae: 20.2783\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 563.8945 - mae: 21.3722 - val_loss: 473.8163 - val_mae: 20.2700\n",
      "Epoch 00068: early stopping\n",
      "563.8944702148438 : loss history ----\n",
      "21.37218475341797 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 349.2815 - mae: 12.4938 - val_loss: 33.0264 - val_mae: 4.6350\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.2855 - mae: 6.1716 - val_loss: 35.6434 - val_mae: 4.7325\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 79.5201 - mae: 6.3435 - val_loss: 27.7873 - val_mae: 4.1295\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 74.3246 - mae: 6.1413 - val_loss: 22.3567 - val_mae: 3.8187\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 71.4366 - mae: 5.9450 - val_loss: 22.1939 - val_mae: 3.9467\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.0334 - mae: 6.0485 - val_loss: 22.6660 - val_mae: 3.8040\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 71.3505 - mae: 5.9677 - val_loss: 22.7125 - val_mae: 4.0190\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 68.5652 - mae: 5.9261 - val_loss: 27.7178 - val_mae: 4.4316\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 72.7429 - mae: 6.1549 - val_loss: 27.5141 - val_mae: 4.1501\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.5971 - mae: 5.8881 - val_loss: 36.0424 - val_mae: 5.0285\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.6418 - mae: 6.1666 - val_loss: 29.0694 - val_mae: 4.5140\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.6730 - mae: 5.9459 - val_loss: 29.6221 - val_mae: 4.5862\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.0792 - mae: 5.9134 - val_loss: 21.3145 - val_mae: 3.9152\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.0754 - mae: 5.9176 - val_loss: 24.0647 - val_mae: 4.1395\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 67.7140 - mae: 5.8648 - val_loss: 20.5396 - val_mae: 3.8378\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 68.6944 - mae: 5.8395 - val_loss: 39.4421 - val_mae: 5.1124\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 67.1093 - mae: 5.6281 - val_loss: 91.5940 - val_mae: 8.3855\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.5728 - mae: 6.4955 - val_loss: 24.9404 - val_mae: 3.9731\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.8591 - mae: 5.8025 - val_loss: 29.5197 - val_mae: 4.5458\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 66.7807 - mae: 5.8587 - val_loss: 20.0770 - val_mae: 3.7175\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 65.3242 - mae: 5.6723 - val_loss: 83.5143 - val_mae: 7.9422\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 71.1803 - mae: 6.1537 - val_loss: 30.4253 - val_mae: 4.6375\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 67.3053 - mae: 5.8794 - val_loss: 19.9409 - val_mae: 3.7199\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.9549 - mae: 5.8759 - val_loss: 28.4374 - val_mae: 4.4701\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 71.2243 - mae: 6.1423 - val_loss: 26.1394 - val_mae: 4.3608\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 64.7149 - mae: 5.7147 - val_loss: 25.6572 - val_mae: 4.0374\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 64.1118 - mae: 5.5742 - val_loss: 32.7368 - val_mae: 4.6285\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 66.1182 - mae: 5.6020 - val_loss: 22.5609 - val_mae: 4.0256\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 66.5547 - mae: 5.8085 - val_loss: 24.0894 - val_mae: 3.8794\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 65.5323 - mae: 5.6966 - val_loss: 28.6485 - val_mae: 4.5156\n",
      "Epoch 00030: early stopping\n",
      "64.11175537109375 : loss history ----\n",
      "5.574166774749756 : mae history ----\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# second experiment\n",
    "\n",
    "# we do a trail experiment\n",
    "\n",
    "ExpResults = []\n",
    "\n",
    "for hiddenLayers in range (0,4):\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    run_logdir = get_run_logdir(\"hl\",hiddenLayers)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    model = build_model(hiddenLayers,defaultNeuronsOnLayer,optimizer,defaultLR)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100,validation_split = 0.1, callbacks=[early_stopping_cb,tensorboard_cb])\n",
    "    history.history['loss'].sort()\n",
    "    \n",
    "    lossHis = history.history[\"loss\"][0]\n",
    "    print(str(lossHis) + \" : loss history ----\")\n",
    "    \n",
    "    history.history['mae'].sort()\n",
    "    maeHis = history.history['mae'][0]\n",
    "    print(str(maeHis) + \" : mae history ----\")\n",
    "\n",
    "    ExpResults.append((hiddenLayers,lossHis,maeHis))\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "pickle_out = open(\"hl.pickle\",\"wb\")\n",
    "pickle.dump(ExpResults, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "cleanSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76304907-1a33-4009-b17e-862f394b62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03814a13-9bb8-4178-869a-4db97487a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 6524.3066 - mae: 51.6576 - val_loss: 505.4943 - val_mae: 21.5131\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 594.8486 - mae: 22.4853 - val_loss: 505.2645 - val_mae: 21.5078\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 594.6085 - mae: 22.4799 - val_loss: 505.0348 - val_mae: 21.5024\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 594.3683 - mae: 22.4745 - val_loss: 504.8028 - val_mae: 21.4970\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 594.1259 - mae: 22.4692 - val_loss: 504.5702 - val_mae: 21.4916\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 593.8828 - mae: 22.4637 - val_loss: 504.3397 - val_mae: 21.4863\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 593.6420 - mae: 22.4584 - val_loss: 504.1078 - val_mae: 21.4809\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 593.3996 - mae: 22.4530 - val_loss: 503.8753 - val_mae: 21.4755\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 593.1566 - mae: 22.4476 - val_loss: 503.6462 - val_mae: 21.4701\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 592.9171 - mae: 22.4422 - val_loss: 503.4132 - val_mae: 21.4647\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 592.6736 - mae: 22.4368 - val_loss: 503.1821 - val_mae: 21.4593\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 592.4320 - mae: 22.4314 - val_loss: 502.9514 - val_mae: 21.4539\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 592.1910 - mae: 22.4261 - val_loss: 502.7218 - val_mae: 21.4486\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 591.9509 - mae: 22.4207 - val_loss: 502.4906 - val_mae: 21.4432\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 591.7092 - mae: 22.4153 - val_loss: 502.2599 - val_mae: 21.4378\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 591.4683 - mae: 22.4099 - val_loss: 502.0306 - val_mae: 21.4325\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 591.2285 - mae: 22.4046 - val_loss: 501.7965 - val_mae: 21.4270\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 590.9839 - mae: 22.3991 - val_loss: 501.5684 - val_mae: 21.4217\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 590.7452 - mae: 22.3938 - val_loss: 501.3367 - val_mae: 21.4163\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 590.5032 - mae: 22.3884 - val_loss: 501.1077 - val_mae: 21.4109\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 590.2637 - mae: 22.3830 - val_loss: 500.8741 - val_mae: 21.4055\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 590.0197 - mae: 22.3776 - val_loss: 500.6439 - val_mae: 21.4001\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 589.7790 - mae: 22.3722 - val_loss: 500.4151 - val_mae: 21.3948\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 589.5399 - mae: 22.3668 - val_loss: 500.1850 - val_mae: 21.3894\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 589.2993 - mae: 22.3615 - val_loss: 499.9548 - val_mae: 21.3840\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 589.0587 - mae: 22.3561 - val_loss: 499.7254 - val_mae: 21.3786\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 588.8188 - mae: 22.3508 - val_loss: 499.4978 - val_mae: 21.3733\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 588.5807 - mae: 22.3454 - val_loss: 499.2670 - val_mae: 21.3679\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 588.3397 - mae: 22.3400 - val_loss: 499.0382 - val_mae: 21.3626\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 588.1005 - mae: 22.3347 - val_loss: 498.8104 - val_mae: 21.3572\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 587.8622 - mae: 22.3294 - val_loss: 498.5801 - val_mae: 21.3518\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 587.6216 - mae: 22.3240 - val_loss: 498.3516 - val_mae: 21.3465\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 587.3826 - mae: 22.3186 - val_loss: 498.1229 - val_mae: 21.3411\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 587.1434 - mae: 22.3132 - val_loss: 497.8919 - val_mae: 21.3357\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 586.9020 - mae: 22.3078 - val_loss: 497.6624 - val_mae: 21.3303\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 586.6622 - mae: 22.3024 - val_loss: 497.4353 - val_mae: 21.3250\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 586.4246 - mae: 22.2971 - val_loss: 497.2061 - val_mae: 21.3196\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 586.1852 - mae: 22.2917 - val_loss: 496.9794 - val_mae: 21.3143\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 585.9481 - mae: 22.2865 - val_loss: 496.7509 - val_mae: 21.3090\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 585.7092 - mae: 22.2811 - val_loss: 496.5253 - val_mae: 21.3037\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 585.4733 - mae: 22.2757 - val_loss: 496.2955 - val_mae: 21.2983\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 585.2330 - mae: 22.2703 - val_loss: 496.0680 - val_mae: 21.2929\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.9952 - mae: 22.2651 - val_loss: 495.8402 - val_mae: 21.2876\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.7570 - mae: 22.2598 - val_loss: 495.6134 - val_mae: 21.2822\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.5199 - mae: 22.2543 - val_loss: 495.3863 - val_mae: 21.2769\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.2825 - mae: 22.2490 - val_loss: 495.1610 - val_mae: 21.2716\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 584.0468 - mae: 22.2437 - val_loss: 494.9329 - val_mae: 21.2662\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 583.8083 - mae: 22.2384 - val_loss: 494.7087 - val_mae: 21.2610\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 583.5739 - mae: 22.2331 - val_loss: 494.4828 - val_mae: 21.2557\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 583.3378 - mae: 22.2278 - val_loss: 494.2561 - val_mae: 21.2503\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 583.1007 - mae: 22.2225 - val_loss: 494.0288 - val_mae: 21.2450\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 582.8630 - mae: 22.2171 - val_loss: 493.8025 - val_mae: 21.2397\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 582.6264 - mae: 22.2118 - val_loss: 493.5759 - val_mae: 21.2343\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 582.3894 - mae: 22.2064 - val_loss: 493.3481 - val_mae: 21.2290\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 582.1512 - mae: 22.2010 - val_loss: 493.1216 - val_mae: 21.2236\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.9144 - mae: 22.1957 - val_loss: 492.8960 - val_mae: 21.2183\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.6783 - mae: 22.1904 - val_loss: 492.6687 - val_mae: 21.2129\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.4408 - mae: 22.1851 - val_loss: 492.4435 - val_mae: 21.2076\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 581.2053 - mae: 22.1798 - val_loss: 492.2170 - val_mae: 21.2023\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.9684 - mae: 22.1744 - val_loss: 491.9908 - val_mae: 21.1970\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.7319 - mae: 22.1691 - val_loss: 491.7649 - val_mae: 21.1916\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.4956 - mae: 22.1638 - val_loss: 491.5406 - val_mae: 21.1863\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.2610 - mae: 22.1585 - val_loss: 491.3142 - val_mae: 21.1810\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 580.0244 - mae: 22.1531 - val_loss: 491.0898 - val_mae: 21.1757\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.7898 - mae: 22.1478 - val_loss: 490.8661 - val_mae: 21.1704\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.5557 - mae: 22.1425 - val_loss: 490.6396 - val_mae: 21.1651\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.3188 - mae: 22.1372 - val_loss: 490.4143 - val_mae: 21.1597\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 579.0832 - mae: 22.1319 - val_loss: 490.1878 - val_mae: 21.1544\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 578.8463 - mae: 22.1265 - val_loss: 489.9620 - val_mae: 21.1491\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 578.6102 - mae: 22.1213 - val_loss: 489.7369 - val_mae: 21.1437\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 578.3748 - mae: 22.1159 - val_loss: 489.5139 - val_mae: 21.1385\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 578.1415 - mae: 22.1106 - val_loss: 489.2909 - val_mae: 21.1332\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.9082 - mae: 22.1053 - val_loss: 489.0663 - val_mae: 21.1279\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.6734 - mae: 22.1000 - val_loss: 488.8427 - val_mae: 21.1226\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.4396 - mae: 22.0947 - val_loss: 488.6196 - val_mae: 21.1173\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 577.2061 - mae: 22.0894 - val_loss: 488.3949 - val_mae: 21.1120\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.9712 - mae: 22.0841 - val_loss: 488.1719 - val_mae: 21.1067\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.7378 - mae: 22.0788 - val_loss: 487.9475 - val_mae: 21.1014\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.5032 - mae: 22.0735 - val_loss: 487.7254 - val_mae: 21.0961\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.2708 - mae: 22.0683 - val_loss: 487.5016 - val_mae: 21.0908\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 576.0368 - mae: 22.0630 - val_loss: 487.2770 - val_mae: 21.0855\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.8018 - mae: 22.0576 - val_loss: 487.0536 - val_mae: 21.0802\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.5681 - mae: 22.0523 - val_loss: 486.8308 - val_mae: 21.0749\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.3351 - mae: 22.0470 - val_loss: 486.6077 - val_mae: 21.0696\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 575.1016 - mae: 22.0417 - val_loss: 486.3836 - val_mae: 21.0643\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.8674 - mae: 22.0364 - val_loss: 486.1624 - val_mae: 21.0590\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.6360 - mae: 22.0312 - val_loss: 485.9399 - val_mae: 21.0537\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.4031 - mae: 22.0259 - val_loss: 485.7173 - val_mae: 21.0485\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 574.1703 - mae: 22.0206 - val_loss: 485.4957 - val_mae: 21.0432\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.9385 - mae: 22.0154 - val_loss: 485.2750 - val_mae: 21.0380\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.7076 - mae: 22.0101 - val_loss: 485.0524 - val_mae: 21.0327\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 573.4747 - mae: 22.0048 - val_loss: 484.8288 - val_mae: 21.0273\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.2408 - mae: 21.9994 - val_loss: 484.6065 - val_mae: 21.0221\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 573.0084 - mae: 21.9942 - val_loss: 484.3875 - val_mae: 21.0168\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.7791 - mae: 21.9890 - val_loss: 484.1659 - val_mae: 21.0116\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.5474 - mae: 21.9837 - val_loss: 483.9445 - val_mae: 21.0063\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 572.3157 - mae: 21.9784 - val_loss: 483.7219 - val_mae: 21.0010\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 572.0828 - mae: 21.9732 - val_loss: 483.4980 - val_mae: 20.9957\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 571.8486 - mae: 21.9679 - val_loss: 483.2775 - val_mae: 20.9904\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 571.6180 - mae: 21.9626 - val_loss: 483.0559 - val_mae: 20.9851\n",
      "571.6179809570312 : loss history ----\n",
      "21.962604522705078 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1976.4628 - mae: 24.9624 - val_loss: 92.0228 - val_mae: 7.7692\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 111.3110 - mae: 7.6819 - val_loss: 38.3741 - val_mae: 4.5534\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.0192 - mae: 6.5419 - val_loss: 31.7850 - val_mae: 4.6930\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 83.0455 - mae: 6.5096 - val_loss: 30.8344 - val_mae: 4.6713\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 82.4864 - mae: 6.4415 - val_loss: 31.4124 - val_mae: 4.8137\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.1410 - mae: 6.5360 - val_loss: 29.9031 - val_mae: 4.5418\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.9173 - mae: 6.3912 - val_loss: 30.3189 - val_mae: 4.6978\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.4935 - mae: 6.3390 - val_loss: 37.7037 - val_mae: 5.4374\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 82.0350 - mae: 6.5326 - val_loss: 29.2614 - val_mae: 4.5477\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.9287 - mae: 6.2680 - val_loss: 34.5033 - val_mae: 5.1616\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.5781 - mae: 6.5108 - val_loss: 32.0883 - val_mae: 4.9524\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.9088 - mae: 6.3714 - val_loss: 38.1970 - val_mae: 5.4811\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.3287 - mae: 6.3712 - val_loss: 32.6121 - val_mae: 5.0095\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.9394 - mae: 6.3860 - val_loss: 30.0855 - val_mae: 4.7426\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.8264 - mae: 6.3397 - val_loss: 28.9605 - val_mae: 4.5890\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.5452 - mae: 6.3133 - val_loss: 28.4151 - val_mae: 4.4244\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.0681 - mae: 6.0639 - val_loss: 68.0476 - val_mae: 7.2430\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.5498 - mae: 6.6186 - val_loss: 28.0641 - val_mae: 4.3907\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.0146 - mae: 6.1898 - val_loss: 36.7233 - val_mae: 5.3336\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.0698 - mae: 6.3658 - val_loss: 29.3982 - val_mae: 4.7000\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.6914 - mae: 6.2153 - val_loss: 47.3790 - val_mae: 6.0967\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.1919 - mae: 6.3097 - val_loss: 40.8570 - val_mae: 5.6469\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.0124 - mae: 6.3530 - val_loss: 27.7698 - val_mae: 4.3153\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.5965 - mae: 6.1591 - val_loss: 32.9371 - val_mae: 5.0513\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.9863 - mae: 6.2906 - val_loss: 31.6053 - val_mae: 4.9367\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.0251 - mae: 6.2000 - val_loss: 27.4938 - val_mae: 4.4206\n",
      "Epoch 00026: early stopping\n",
      "75.02506256103516 : loss history ----\n",
      "6.063873291015625 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 28943.6289 - mae: 90.1803 - val_loss: 548.5040 - val_mae: 22.6461\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 434.2610 - mae: 18.0023 - val_loss: 79.9385 - val_mae: 7.4435\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 111.1567 - mae: 7.8071 - val_loss: 56.0746 - val_mae: 6.2338\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 102.3948 - mae: 7.3587 - val_loss: 50.0320 - val_mae: 5.7763\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 96.9542 - mae: 7.2359 - val_loss: 45.3648 - val_mae: 5.4360\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.0435 - mae: 7.1692 - val_loss: 47.1530 - val_mae: 5.4582\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 88.6580 - mae: 6.9695 - val_loss: 41.4956 - val_mae: 5.1403\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 87.1790 - mae: 6.9454 - val_loss: 40.1904 - val_mae: 5.0193\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.6577 - mae: 6.8979 - val_loss: 40.4005 - val_mae: 5.0918\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 83.8311 - mae: 6.7224 - val_loss: 37.1267 - val_mae: 4.8339\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.0285 - mae: 6.8285 - val_loss: 36.0687 - val_mae: 4.8605\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 82.4521 - mae: 6.6471 - val_loss: 35.0958 - val_mae: 4.7439\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 81.3745 - mae: 6.6355 - val_loss: 34.0217 - val_mae: 4.7137\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.4974 - mae: 6.5701 - val_loss: 33.9922 - val_mae: 4.7326\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.1032 - mae: 6.5405 - val_loss: 33.0663 - val_mae: 4.6762\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.5985 - mae: 6.4546 - val_loss: 33.0163 - val_mae: 4.6560\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.4847 - mae: 6.2294 - val_loss: 61.1977 - val_mae: 6.4659\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.9556 - mae: 6.6810 - val_loss: 30.8825 - val_mae: 4.5447\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.4360 - mae: 6.1863 - val_loss: 29.4217 - val_mae: 4.3916\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 73.8228 - mae: 6.3158 - val_loss: 28.3489 - val_mae: 4.4095\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 72.4204 - mae: 6.1695 - val_loss: 50.1863 - val_mae: 5.8247\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.7079 - mae: 6.3019 - val_loss: 31.2498 - val_mae: 4.5783\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 73.0538 - mae: 6.2086 - val_loss: 26.8221 - val_mae: 4.3192\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 74.2619 - mae: 6.1634 - val_loss: 27.2258 - val_mae: 4.3333\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 73.1460 - mae: 6.2642 - val_loss: 27.4284 - val_mae: 4.3216\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.0439 - mae: 6.0568 - val_loss: 25.3234 - val_mae: 4.1692\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.0480 - mae: 5.9864 - val_loss: 29.0766 - val_mae: 4.3956\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 71.5064 - mae: 5.9173 - val_loss: 24.9528 - val_mae: 4.1352\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.8777 - mae: 6.0874 - val_loss: 25.4980 - val_mae: 4.1796\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.2656 - mae: 5.9381 - val_loss: 29.9082 - val_mae: 4.4858\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 71.5140 - mae: 6.1026 - val_loss: 24.3146 - val_mae: 4.0436\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 68.6556 - mae: 5.8666 - val_loss: 25.8670 - val_mae: 4.1801\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 70.8637 - mae: 6.0456 - val_loss: 23.6916 - val_mae: 3.9941\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.6575 - mae: 5.9491 - val_loss: 29.1996 - val_mae: 4.4753\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.7035 - mae: 5.9673 - val_loss: 25.8843 - val_mae: 4.2241\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 69.0746 - mae: 6.0124 - val_loss: 24.4742 - val_mae: 4.0509\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 68.6637 - mae: 5.8724 - val_loss: 24.3810 - val_mae: 4.0955\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.0101 - mae: 5.9628 - val_loss: 26.5569 - val_mae: 4.1652\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 69.0231 - mae: 5.8447 - val_loss: 28.4350 - val_mae: 4.4333\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 70.2662 - mae: 6.0861 - val_loss: 28.4097 - val_mae: 4.2537\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 69.7358 - mae: 5.9083 - val_loss: 28.9803 - val_mae: 4.4634\n",
      "Epoch 00041: early stopping\n",
      "68.65560150146484 : loss history ----\n",
      "5.844681262969971 : mae history ----\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# third experiment\n",
    "\n",
    "# we do a trail experiment\n",
    "\n",
    "ExpResults = []\n",
    "\n",
    "neuronsNumber = [ 5,25,125]\n",
    "\n",
    "for nn in neuronsNumber:\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    run_logdir = get_run_logdir(\"nn\",nn)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    model = build_model(defaultNOL,nn,optimizer,defaultLR)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100,validation_split = 0.1, callbacks=[early_stopping_cb,tensorboard_cb])\n",
    "    history.history['loss'].sort()\n",
    "    \n",
    "    lossHis = history.history[\"loss\"][0]\n",
    "    print(str(lossHis) + \" : loss history ----\")\n",
    "    \n",
    "    history.history['mae'].sort()\n",
    "    maeHis = history.history['mae'][0]\n",
    "    print(str(maeHis) + \" : mae history ----\")\n",
    "\n",
    "    ExpResults.append((nn,lossHis,maeHis))\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "pickle_out = open(\"nn.pickle\",\"wb\")\n",
    "pickle.dump(ExpResults, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "cleanSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626c5998-ed57-4637-b835-dd5762030e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 117101.7812 - mae: 123.7935 - val_loss: 108.1399 - val_mae: 8.5337\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 163.8253 - mae: 9.5988 - val_loss: 68.6999 - val_mae: 6.7683\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 113.2871 - mae: 7.8936 - val_loss: 49.6265 - val_mae: 5.5180\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 97.5250 - mae: 7.2386 - val_loss: 44.5197 - val_mae: 5.2958\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 91.5391 - mae: 7.0076 - val_loss: 42.7563 - val_mae: 5.2981\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.4378 - mae: 7.0010 - val_loss: 42.5495 - val_mae: 5.0830\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.9827 - mae: 6.7954 - val_loss: 41.1674 - val_mae: 5.1446\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.6662 - mae: 6.7517 - val_loss: 44.3108 - val_mae: 5.4583\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 87.2038 - mae: 6.9364 - val_loss: 40.8578 - val_mae: 5.0159\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.3688 - mae: 6.7019 - val_loss: 44.2814 - val_mae: 5.4409\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 86.3176 - mae: 6.8699 - val_loss: 40.8691 - val_mae: 5.1381\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 86.3014 - mae: 6.7988 - val_loss: 46.5182 - val_mae: 5.6370\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 86.0942 - mae: 6.7629 - val_loss: 43.3289 - val_mae: 5.3657\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 84.8896 - mae: 6.8204 - val_loss: 40.6521 - val_mae: 5.0992\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 84.5033 - mae: 6.7824 - val_loss: 40.2261 - val_mae: 5.0088\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.2646 - mae: 6.7375 - val_loss: 41.3100 - val_mae: 4.9884\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 84.2253 - mae: 6.5342 - val_loss: 74.1952 - val_mae: 7.3707\n",
      "Epoch 00017: early stopping\n",
      "84.22534942626953 : loss history ----\n",
      "6.534229278564453 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 14ms/step - loss: 1976.4628 - mae: 24.9624 - val_loss: 92.0228 - val_mae: 7.7692\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 111.3110 - mae: 7.6819 - val_loss: 38.3741 - val_mae: 4.5534\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 87.0192 - mae: 6.5419 - val_loss: 31.7850 - val_mae: 4.6930\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.0455 - mae: 6.5096 - val_loss: 30.8344 - val_mae: 4.6713\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 82.4864 - mae: 6.4415 - val_loss: 31.4124 - val_mae: 4.8137\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.1410 - mae: 6.5360 - val_loss: 29.9031 - val_mae: 4.5418\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.9173 - mae: 6.3912 - val_loss: 30.3189 - val_mae: 4.6978\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 79.4935 - mae: 6.3390 - val_loss: 37.7037 - val_mae: 5.4374\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 82.0350 - mae: 6.5326 - val_loss: 29.2614 - val_mae: 4.5477\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.9287 - mae: 6.2680 - val_loss: 34.5033 - val_mae: 5.1616\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.5781 - mae: 6.5108 - val_loss: 32.0883 - val_mae: 4.9524\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 79.9088 - mae: 6.3714 - val_loss: 38.1970 - val_mae: 5.4811\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.3287 - mae: 6.3712 - val_loss: 32.6121 - val_mae: 5.0095\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.9394 - mae: 6.3860 - val_loss: 30.0855 - val_mae: 4.7426\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.8264 - mae: 6.3397 - val_loss: 28.9605 - val_mae: 4.5890\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.5452 - mae: 6.3133 - val_loss: 28.4151 - val_mae: 4.4244\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.0681 - mae: 6.0639 - val_loss: 68.0476 - val_mae: 7.2430\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.5498 - mae: 6.6186 - val_loss: 28.0641 - val_mae: 4.3907\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.0146 - mae: 6.1898 - val_loss: 36.7233 - val_mae: 5.3336\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.0698 - mae: 6.3658 - val_loss: 29.3982 - val_mae: 4.7000\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 76.6914 - mae: 6.2153 - val_loss: 47.3790 - val_mae: 6.0967\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.1919 - mae: 6.3097 - val_loss: 40.8570 - val_mae: 5.6469\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.0124 - mae: 6.3530 - val_loss: 27.7698 - val_mae: 4.3153\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.5965 - mae: 6.1591 - val_loss: 32.9371 - val_mae: 5.0513\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 76.9863 - mae: 6.2906 - val_loss: 31.6053 - val_mae: 4.9367\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.0251 - mae: 6.2000 - val_loss: 27.4938 - val_mae: 4.4206\n",
      "Epoch 00026: early stopping\n",
      "75.02506256103516 : loss history ----\n",
      "6.063873291015625 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 154914.0312 - mae: 138.2834 - val_loss: 65.5549 - val_mae: 6.9497\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 97.9995 - mae: 7.6239 - val_loss: 48.1013 - val_mae: 5.6397\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.9793 - mae: 7.3308 - val_loss: 46.0298 - val_mae: 5.4348\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.2432 - mae: 7.3019 - val_loss: 45.4542 - val_mae: 5.4699\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.4375 - mae: 7.2807 - val_loss: 45.0640 - val_mae: 5.4289\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 88.8231 - mae: 7.2057 - val_loss: 45.6048 - val_mae: 5.3426\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 88.8335 - mae: 7.1284 - val_loss: 44.2356 - val_mae: 5.2741\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 90.5811 - mae: 7.1519 - val_loss: 45.4883 - val_mae: 5.5277\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.4325 - mae: 7.2735 - val_loss: 44.4132 - val_mae: 5.2755\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 89.5263 - mae: 7.0822 - val_loss: 46.4486 - val_mae: 5.6504\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 90.4594 - mae: 7.2554 - val_loss: 44.1944 - val_mae: 5.2644\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 91.3802 - mae: 7.1675 - val_loss: 49.8594 - val_mae: 5.9470\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 92.3390 - mae: 7.1976 - val_loss: 49.8605 - val_mae: 5.9487\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 88.9709 - mae: 7.1825 - val_loss: 43.6250 - val_mae: 5.2291\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 88.5618 - mae: 7.1451 - val_loss: 43.3715 - val_mae: 5.2155\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 88.8441 - mae: 7.0938 - val_loss: 45.4261 - val_mae: 5.3146\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.7495 - mae: 6.9142 - val_loss: 101.1800 - val_mae: 8.6878\n",
      "Epoch 00017: early stopping\n",
      "87.74948120117188 : loss history ----\n",
      "6.914227485656738 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 55338.7500 - mae: 229.3864 - val_loss: 50966.4961 - val_mae: 221.2126\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 55063.1406 - mae: 228.8112 - val_loss: 50711.8281 - val_mae: 220.6561\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 54793.5977 - mae: 228.2399 - val_loss: 50456.3594 - val_mae: 220.0966\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 54520.2578 - mae: 227.6688 - val_loss: 50206.1133 - val_mae: 219.5469\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 54250.6836 - mae: 227.1019 - val_loss: 49958.6875 - val_mae: 219.0016\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 53984.6758 - mae: 226.5408 - val_loss: 49711.6484 - val_mae: 218.4560\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 53719.7734 - mae: 225.9768 - val_loss: 49462.9531 - val_mae: 217.9056\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 53454.0234 - mae: 225.4117 - val_loss: 49216.1523 - val_mae: 217.3579\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 53188.4453 - mae: 224.8490 - val_loss: 48969.9219 - val_mae: 216.8102\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 52921.1992 - mae: 224.2826 - val_loss: 48725.0117 - val_mae: 216.2641\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 52657.5547 - mae: 223.7185 - val_loss: 48480.5234 - val_mae: 215.7176\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 52391.5273 - mae: 223.1525 - val_loss: 48238.1914 - val_mae: 215.1746\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 52131.7812 - mae: 222.5888 - val_loss: 47990.5625 - val_mae: 214.6191\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 51866.3203 - mae: 222.0189 - val_loss: 47748.0078 - val_mae: 214.0734\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 51605.7188 - mae: 221.4591 - val_loss: 47508.8789 - val_mae: 213.5335\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 51349.9727 - mae: 220.9001 - val_loss: 47267.2188 - val_mae: 212.9868\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 51089.5273 - mae: 220.3373 - val_loss: 47029.8789 - val_mae: 212.4481\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 50833.1445 - mae: 219.7792 - val_loss: 46793.0508 - val_mae: 211.9094\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 50576.3320 - mae: 219.2220 - val_loss: 46555.9258 - val_mae: 211.3689\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 50322.0273 - mae: 218.6630 - val_loss: 46317.5352 - val_mae: 210.8244\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 50067.1289 - mae: 218.1015 - val_loss: 46080.1523 - val_mae: 210.2806\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 49812.1836 - mae: 217.5447 - val_loss: 45846.6758 - val_mae: 209.7444\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 49561.7188 - mae: 216.9909 - val_loss: 45611.6836 - val_mae: 209.2035\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 49307.5156 - mae: 216.4356 - val_loss: 45381.4062 - val_mae: 208.6717\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 49060.4844 - mae: 215.8839 - val_loss: 45148.1953 - val_mae: 208.1321\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 48810.7891 - mae: 215.3310 - val_loss: 44917.7812 - val_mae: 207.5974\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 48565.4102 - mae: 214.7810 - val_loss: 44689.0508 - val_mae: 207.0650\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 48318.4570 - mae: 214.2346 - val_loss: 44463.5312 - val_mae: 206.5387\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 48076.9961 - mae: 213.6903 - val_loss: 44236.8867 - val_mae: 206.0085\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 47833.4141 - mae: 213.1425 - val_loss: 44011.3555 - val_mae: 205.4795\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 47589.5039 - mae: 212.5961 - val_loss: 43784.1055 - val_mae: 204.9454\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 47345.3594 - mae: 212.0468 - val_loss: 43561.2734 - val_mae: 204.4200\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 47103.8672 - mae: 211.5020 - val_loss: 43337.4727 - val_mae: 203.8911\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 46862.2461 - mae: 210.9573 - val_loss: 43115.7930 - val_mae: 203.3656\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 46623.8828 - mae: 210.4140 - val_loss: 42893.5078 - val_mae: 202.8376\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 46387.5586 - mae: 209.8699 - val_loss: 42670.8125 - val_mae: 202.3073\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 46147.6133 - mae: 209.3259 - val_loss: 42452.6055 - val_mae: 201.7860\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 45911.8398 - mae: 208.7884 - val_loss: 42237.2656 - val_mae: 201.2700\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 45680.0078 - mae: 208.2541 - val_loss: 42020.5781 - val_mae: 200.7497\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 45446.8320 - mae: 207.7168 - val_loss: 41802.4102 - val_mae: 200.2247\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 45212.4766 - mae: 207.1746 - val_loss: 41584.6328 - val_mae: 199.6991\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 44977.0898 - mae: 206.6319 - val_loss: 41369.2344 - val_mae: 199.1778\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 44745.4336 - mae: 206.0927 - val_loss: 41151.8398 - val_mae: 198.6507\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 44511.5664 - mae: 205.5522 - val_loss: 40938.0898 - val_mae: 198.1306\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 44278.9102 - mae: 205.0109 - val_loss: 40722.3789 - val_mae: 197.6045\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 44048.2305 - mae: 204.4689 - val_loss: 40504.7188 - val_mae: 197.0726\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 43817.3203 - mae: 203.9251 - val_loss: 40288.9727 - val_mae: 196.5439\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 43586.3516 - mae: 203.3851 - val_loss: 40077.3594 - val_mae: 196.0236\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 43356.9375 - mae: 202.8474 - val_loss: 39864.3398 - val_mae: 195.4987\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 43131.6211 - mae: 202.3104 - val_loss: 39651.7148 - val_mae: 194.9732\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 42903.4180 - mae: 201.7735 - val_loss: 39442.8281 - val_mae: 194.4554\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 42679.1406 - mae: 201.2417 - val_loss: 39235.8906 - val_mae: 193.9408\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 42457.4805 - mae: 200.7130 - val_loss: 39027.8594 - val_mae: 193.4224\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 42233.9531 - mae: 200.1802 - val_loss: 38821.8086 - val_mae: 192.9074\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 42015.3398 - mae: 199.6547 - val_loss: 38616.2812 - val_mae: 192.3924\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 41792.9102 - mae: 199.1246 - val_loss: 38413.4453 - val_mae: 191.8825\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 41576.5781 - mae: 198.5971 - val_loss: 38206.8477 - val_mae: 191.3623\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 41355.6758 - mae: 198.0693 - val_loss: 38006.6172 - val_mae: 190.8562\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 41140.9375 - mae: 197.5488 - val_loss: 37805.2773 - val_mae: 190.3461\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 40924.3789 - mae: 197.0244 - val_loss: 37605.5547 - val_mae: 189.8386\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 40711.7578 - mae: 196.5013 - val_loss: 37404.6602 - val_mae: 189.3269\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 40494.2188 - mae: 195.9760 - val_loss: 37206.3828 - val_mae: 188.8204\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 40281.6250 - mae: 195.4547 - val_loss: 37006.6172 - val_mae: 188.3089\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 40068.4258 - mae: 194.9296 - val_loss: 36807.5625 - val_mae: 187.7979\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 39855.5469 - mae: 194.4070 - val_loss: 36611.9258 - val_mae: 187.2940\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 39644.7695 - mae: 193.8892 - val_loss: 36417.0000 - val_mae: 186.7906\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 39435.8477 - mae: 193.3721 - val_loss: 36221.9375 - val_mae: 186.2855\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 39226.5391 - mae: 192.8522 - val_loss: 36026.3789 - val_mae: 185.7778\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 39016.1836 - mae: 192.3340 - val_loss: 35834.0781 - val_mae: 185.2769\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 38810.3828 - mae: 191.8153 - val_loss: 35637.5430 - val_mae: 184.7642\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 38599.6953 - mae: 191.2925 - val_loss: 35444.4531 - val_mae: 184.2588\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 38390.4961 - mae: 190.7717 - val_loss: 35251.5430 - val_mae: 183.7523\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 38185.5195 - mae: 190.2518 - val_loss: 35056.6719 - val_mae: 183.2395\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 37979.5938 - mae: 189.7292 - val_loss: 34862.1602 - val_mae: 182.7265\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 37771.1250 - mae: 189.2094 - val_loss: 34672.5234 - val_mae: 182.2244\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 37569.7852 - mae: 188.6954 - val_loss: 34481.7852 - val_mae: 181.7182\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 37366.8086 - mae: 188.1824 - val_loss: 34295.9102 - val_mae: 181.2232\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 37167.6758 - mae: 187.6759 - val_loss: 34110.3711 - val_mae: 180.7276\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 36969.9570 - mae: 187.1662 - val_loss: 33921.9648 - val_mae: 180.2234\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 36768.4727 - mae: 186.6527 - val_loss: 33736.2461 - val_mae: 179.7249\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 36569.9961 - mae: 186.1427 - val_loss: 33550.7773 - val_mae: 179.2256\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 36372.0039 - mae: 185.6342 - val_loss: 33366.9453 - val_mae: 178.7292\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 36175.1445 - mae: 185.1243 - val_loss: 33180.9023 - val_mae: 178.2257\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 35975.5000 - mae: 184.6092 - val_loss: 32996.4492 - val_mae: 177.7249\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 35779.2383 - mae: 184.1006 - val_loss: 32814.4922 - val_mae: 177.2293\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 35584.3008 - mae: 183.5966 - val_loss: 32633.7559 - val_mae: 176.7357\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 35391.0234 - mae: 183.0879 - val_loss: 32449.7051 - val_mae: 176.2317\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 35194.3320 - mae: 182.5758 - val_loss: 32268.5547 - val_mae: 175.7342\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 35003.2500 - mae: 182.0682 - val_loss: 32086.0059 - val_mae: 175.2314\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 34808.8438 - mae: 181.5605 - val_loss: 31905.8379 - val_mae: 174.7338\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 34617.0742 - mae: 181.0509 - val_loss: 31726.4668 - val_mae: 174.2367\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 34426.5742 - mae: 180.5469 - val_loss: 31548.0449 - val_mae: 173.7409\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 34234.7695 - mae: 180.0410 - val_loss: 31371.5215 - val_mae: 173.2489\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 34046.5781 - mae: 179.5380 - val_loss: 31194.1289 - val_mae: 172.7531\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 33855.7773 - mae: 179.0305 - val_loss: 31016.8984 - val_mae: 172.2565\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 33669.9961 - mae: 178.5280 - val_loss: 30839.4609 - val_mae: 171.7579\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 33479.4883 - mae: 178.0224 - val_loss: 30666.6582 - val_mae: 171.2703\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 33296.6797 - mae: 177.5247 - val_loss: 30492.1348 - val_mae: 170.7768\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 33111.8906 - mae: 177.0238 - val_loss: 30317.8477 - val_mae: 170.2826\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 32926.7812 - mae: 176.5246 - val_loss: 30145.6016 - val_mae: 169.7925\n",
      "32926.78125 : loss history ----\n",
      "176.52464294433594 : mae history ----\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fourth experiment\n",
    "\n",
    "# we do a trail experiment\n",
    "\n",
    "ExpResults = []\n",
    "\n",
    "expOptimizers = { \"SGD\" : keras.optimizers.SGD(), \"Nesterov\" :  keras.optimizers.SGD(nesterov = True) ,  \"Momentum\" : keras.optimizers.SGD(momentum=0.5), \"Adam\" : keras.optimizers.Adam() }\n",
    "\n",
    "\n",
    "for optName,opt in expOptimizers.items():\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    run_logdir = get_run_logdir(\"opt\",opt)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    model = build_model(defaultNOL,defaultNeuronsOnLayer,opt,defaultLR)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100,validation_split = 0.1, callbacks=[early_stopping_cb,tensorboard_cb])\n",
    "    history.history['loss'].sort()\n",
    "    \n",
    "    lossHis = history.history[\"loss\"][0]\n",
    "    print(str(lossHis) + \" : loss history ----\")\n",
    "    \n",
    "    history.history['mae'].sort()\n",
    "    maeHis = history.history['mae'][0]\n",
    "    print(str(maeHis) + \" : mae history ----\")\n",
    "\n",
    "    ExpResults.append((optName,lossHis,maeHis))\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "pickle_out = open(\"opt.pickle\",\"wb\")\n",
    "pickle.dump(ExpResults, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "cleanSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "033d7a9a-6618-4285-b502-38817889bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 117097.6406 - mae: 123.6391 - val_loss: 104.6918 - val_mae: 8.3816\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 151.6685 - mae: 9.1904 - val_loss: 61.9500 - val_mae: 6.4389\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 105.8230 - mae: 7.6377 - val_loss: 47.2934 - val_mae: 5.3977\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 94.2339 - mae: 7.1140 - val_loss: 43.1925 - val_mae: 5.2576\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.9217 - mae: 6.9550 - val_loss: 42.3040 - val_mae: 5.2694\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.3587 - mae: 6.9521 - val_loss: 42.5274 - val_mae: 5.0696\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.2858 - mae: 6.7735 - val_loss: 40.8609 - val_mae: 5.0941\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.3857 - mae: 6.7388 - val_loss: 43.7928 - val_mae: 5.4038\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 87.1998 - mae: 6.9336 - val_loss: 40.8447 - val_mae: 5.0046\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.2868 - mae: 6.6882 - val_loss: 44.0320 - val_mae: 5.4223\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.0560 - mae: 6.8589 - val_loss: 40.6689 - val_mae: 5.1115\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.4484 - mae: 6.8023 - val_loss: 47.3865 - val_mae: 5.7075\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.3139 - mae: 6.7669 - val_loss: 43.9890 - val_mae: 5.4235\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.7410 - mae: 6.8169 - val_loss: 40.4068 - val_mae: 5.0640\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.4807 - mae: 6.7848 - val_loss: 40.1983 - val_mae: 4.9953\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.3348 - mae: 6.7452 - val_loss: 41.0769 - val_mae: 4.9800\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.1991 - mae: 6.5329 - val_loss: 76.1879 - val_mae: 7.4965\n",
      "Epoch 00017: early stopping\n",
      "84.1990966796875 : loss history ----\n",
      "6.532925128936768 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 2030.7059 - mae: 25.5772 - val_loss: 66.5712 - val_mae: 6.5983\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 88.0625 - mae: 6.7221 - val_loss: 28.8802 - val_mae: 4.3820\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 79.6298 - mae: 6.2815 - val_loss: 30.1814 - val_mae: 4.8472\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.6043 - mae: 6.3162 - val_loss: 30.1315 - val_mae: 4.7741\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.7741 - mae: 6.2822 - val_loss: 28.7603 - val_mae: 4.6498\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.3203 - mae: 6.3558 - val_loss: 29.3497 - val_mae: 4.3226\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.9259 - mae: 6.2547 - val_loss: 28.4897 - val_mae: 4.5632\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.5203 - mae: 6.2852 - val_loss: 31.9607 - val_mae: 4.9487\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.8725 - mae: 6.4029 - val_loss: 27.6004 - val_mae: 4.4061\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 76.0135 - mae: 6.1442 - val_loss: 32.8645 - val_mae: 5.0298\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.0141 - mae: 6.3539 - val_loss: 28.3898 - val_mae: 4.6246\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.3926 - mae: 6.2953 - val_loss: 41.6691 - val_mae: 5.6221\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 81.3385 - mae: 6.4175 - val_loss: 36.5671 - val_mae: 5.2743\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.2299 - mae: 6.2984 - val_loss: 27.2613 - val_mae: 4.5076\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 74.6067 - mae: 6.2011 - val_loss: 27.0880 - val_mae: 4.4307\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 75.6978 - mae: 6.2441 - val_loss: 26.9273 - val_mae: 4.2874\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 73.7855 - mae: 5.9524 - val_loss: 76.6960 - val_mae: 7.6965\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 78.3504 - mae: 6.6169 - val_loss: 26.5163 - val_mae: 4.1997\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.8798 - mae: 6.0460 - val_loss: 33.8721 - val_mae: 5.0852\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 73.8734 - mae: 6.1981 - val_loss: 28.6230 - val_mae: 4.6882\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.2406 - mae: 6.1647 - val_loss: 37.0588 - val_mae: 5.2885\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.9639 - mae: 6.2485 - val_loss: 41.6937 - val_mae: 5.6077\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 74.4075 - mae: 6.2804 - val_loss: 27.0863 - val_mae: 4.0980\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.6612 - mae: 5.9853 - val_loss: 32.9922 - val_mae: 5.0174\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 73.8773 - mae: 6.1428 - val_loss: 30.1690 - val_mae: 4.8087\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 71.1211 - mae: 6.0684 - val_loss: 25.4466 - val_mae: 4.2738\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 70.8943 - mae: 6.0007 - val_loss: 26.0594 - val_mae: 4.1062\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 73.4937 - mae: 5.9733 - val_loss: 30.5040 - val_mae: 4.8333\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 72.1747 - mae: 6.1864 - val_loss: 26.1290 - val_mae: 4.1083\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 71.2460 - mae: 5.9531 - val_loss: 26.7210 - val_mae: 4.4775\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 71.4157 - mae: 6.0007 - val_loss: 25.5247 - val_mae: 4.2814\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 70.3624 - mae: 5.8407 - val_loss: 26.7886 - val_mae: 4.4981\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 70.8453 - mae: 5.9521 - val_loss: 28.5408 - val_mae: 4.6669\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 70.5095 - mae: 5.9837 - val_loss: 31.7162 - val_mae: 4.9187\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 70.5624 - mae: 5.9215 - val_loss: 40.3058 - val_mae: 5.4806\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 72.4640 - mae: 6.2155 - val_loss: 25.8139 - val_mae: 4.0349\n",
      "Epoch 00036: early stopping\n",
      "70.36236572265625 : loss history ----\n",
      "5.840688705444336 : mae history ----\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 155039.4219 - mae: 142.2598 - val_loss: 346.6103 - val_mae: 17.0484\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 280.6159 - mae: 14.0073 - val_loss: 194.8663 - val_mae: 12.0992\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 279.8095 - mae: 13.8930 - val_loss: 228.4728 - val_mae: 14.2381\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 201.1451 - mae: 11.2458 - val_loss: 46.9245 - val_mae: 5.4228\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 127.3826 - mae: 9.0152 - val_loss: 48.5554 - val_mae: 5.4361\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 105.4237 - mae: 7.7433 - val_loss: 48.9293 - val_mae: 5.4606\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 86.3996 - mae: 7.1381 - val_loss: 41.8356 - val_mae: 5.1484\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 88.4440 - mae: 7.0554 - val_loss: 44.2527 - val_mae: 5.5205\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 88.5328 - mae: 7.0923 - val_loss: 46.7346 - val_mae: 5.3819\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 87.0299 - mae: 6.8163 - val_loss: 44.0351 - val_mae: 5.5421\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 86.1233 - mae: 7.0554 - val_loss: 41.9311 - val_mae: 5.1293\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.2069 - mae: 6.8625 - val_loss: 41.1572 - val_mae: 5.2725\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 86.7010 - mae: 7.0961 - val_loss: 49.6723 - val_mae: 5.9248\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.0718 - mae: 6.8358 - val_loss: 41.2529 - val_mae: 5.0794\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 82.5435 - mae: 6.7256 - val_loss: 39.5955 - val_mae: 4.9928\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 79.9832 - mae: 6.7975 - val_loss: 39.0726 - val_mae: 5.0142\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.4040 - mae: 6.6899 - val_loss: 50.8709 - val_mae: 5.9441\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 105.6541 - mae: 8.3130 - val_loss: 37.8684 - val_mae: 4.9653\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 102.4433 - mae: 7.6834 - val_loss: 45.0558 - val_mae: 5.2910\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.2176 - mae: 6.7514 - val_loss: 50.1326 - val_mae: 5.8534\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 83.2005 - mae: 6.9311 - val_loss: 51.5259 - val_mae: 5.9434\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 80.5506 - mae: 6.6400 - val_loss: 37.2373 - val_mae: 4.8587\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 77.5931 - mae: 6.5825 - val_loss: 45.2794 - val_mae: 5.3494\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 84.0138 - mae: 6.3822 - val_loss: 45.8552 - val_mae: 5.5120\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.0903 - mae: 6.3798 - val_loss: 40.8179 - val_mae: 5.1495\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 71.2921 - mae: 6.1477 - val_loss: 35.6005 - val_mae: 4.6416\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 72.4760 - mae: 6.3087 - val_loss: 49.1796 - val_mae: 5.6292\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 76.7666 - mae: 6.2825 - val_loss: 44.7005 - val_mae: 5.3443\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 70.6129 - mae: 6.3134 - val_loss: 51.5941 - val_mae: 5.7750\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 75.4448 - mae: 6.3812 - val_loss: 45.7636 - val_mae: 5.4490\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 72.8104 - mae: 6.3035 - val_loss: 44.7464 - val_mae: 5.3266\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 73.7659 - mae: 6.1320 - val_loss: 46.0848 - val_mae: 5.4717\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 81.5351 - mae: 6.7858 - val_loss: 40.5414 - val_mae: 4.9472\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 74.9728 - mae: 6.4254 - val_loss: 41.2369 - val_mae: 5.0360\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 68.3749 - mae: 5.9523 - val_loss: 38.5684 - val_mae: 4.8238\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 85.3796 - mae: 7.0343 - val_loss: 59.5289 - val_mae: 6.3554\n",
      "Epoch 00036: early stopping\n",
      "68.37486267089844 : loss history ----\n",
      "5.95234489440918 : mae history ----\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fourth experiment\n",
    "\n",
    "# we do a trail experiment\n",
    "\n",
    "ExpResults = []\n",
    "\n",
    "momentumValues = [0.1, 0.5, 0.9]\n",
    "\n",
    "for mom in momentumValues:\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    run_logdir = get_run_logdir(\"mom\",mom)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    optimizerMom = keras.optimizers.SGD(momentum = mom)\n",
    "\n",
    "    \n",
    "    model = build_model(defaultNOL,defaultNeuronsOnLayer,optimizerMom,defaultLR)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=100,validation_split = 0.1, callbacks=[early_stopping_cb,tensorboard_cb])\n",
    "    history.history['loss'].sort()\n",
    "    \n",
    "    lossHis = history.history[\"loss\"][0]\n",
    "    print(str(lossHis) + \" : loss history ----\")\n",
    "    \n",
    "    history.history['mae'].sort()\n",
    "    maeHis = history.history['mae'][0]\n",
    "    print(str(maeHis) + \" : mae history ----\")\n",
    "\n",
    "    ExpResults.append((mom,lossHis,maeHis))\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "pickle_out = open(\"mom.pickle\",\"wb\")\n",
    "pickle.dump(ExpResults, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "cleanSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60718ea4-f978-49ff-ba84-0a9ff2fb080d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f163179-9b69-4a43-a0c3-568cca8f138a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83bae0fb-f36f-416b-85d3-92c1aede7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = { \"model__n_hidden\": [0, 1, 2, 3], \"model__n_neurons\": [5,25,125],\"model__learning_rate\": [10**-6,10**-5,10**-4]}\n",
    "param_distribs[\"model__optimizer\"] =  [ keras.optimizers.SGD(),  keras.optimizers.SGD(nesterov = True) ,  keras.optimizers.SGD(momentum=0.5),  keras.optimizers.Adam() ]\n",
    "param_distribs[\"model__momentum\"] = [0.1, 0.5, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d4a3e54-0858-4893-beda-8c6fcfdbc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)\n",
    "keras_reg = KerasRegressor(build_model, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d781e99-4fa0-4cad-8294-6e3bd31de4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 4228.3408 - mae: 41.6215 - val_loss: 445.6256 - val_mae: 18.1985\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 473.1085 - mae: 18.5061 - val_loss: 291.4022 - val_mae: 14.4270\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 334.7518 - mae: 15.4703 - val_loss: 183.5076 - val_mae: 11.8259\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 256.1659 - mae: 13.3600 - val_loss: 164.1733 - val_mae: 10.9870\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 208.5113 - mae: 11.7953 - val_loss: 94.9027 - val_mae: 8.5727\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 186.9752 - mae: 10.9844 - val_loss: 76.2287 - val_mae: 7.6698\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.1158 - mae: 9.5075 - val_loss: 66.0481 - val_mae: 6.9853\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.5241 - mae: 9.3196 - val_loss: 64.6155 - val_mae: 6.7345\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.9867 - mae: 8.5633 - val_loss: 87.6886 - val_mae: 7.5769\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.5075 - mae: 8.7095 - val_loss: 68.5291 - val_mae: 6.8308\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.4821 - mae: 8.3198 - val_loss: 67.5508 - val_mae: 6.8344\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.0681 - mae: 8.2170 - val_loss: 56.6254 - val_mae: 6.2619\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.7205 - mae: 8.0509 - val_loss: 67.0562 - val_mae: 6.8365\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.2601 - mae: 8.0227 - val_loss: 57.0235 - val_mae: 6.1978\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.3431 - mae: 7.8402 - val_loss: 63.5287 - val_mae: 6.4385\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.1663 - mae: 7.8435 - val_loss: 58.8138 - val_mae: 6.4404\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.0512 - mae: 7.8222 - val_loss: 57.4937 - val_mae: 6.2710\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.7079 - mae: 7.8090 - val_loss: 57.2549 - val_mae: 6.2401\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.3197 - mae: 7.6289 - val_loss: 57.2388 - val_mae: 6.2732\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.2782 - mae: 7.7270 - val_loss: 59.0610 - val_mae: 6.4337\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.8112 - mae: 7.6229 - val_loss: 60.8484 - val_mae: 6.1512\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.8670 - mae: 7.6330 - val_loss: 57.5854 - val_mae: 5.9522\n",
      "Epoch 00022: early stopping\n",
      "5/5 [==============================] - 0s 661us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 18122.4727 - mae: 90.3599 - val_loss: 1125.1084 - val_mae: 28.6174\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1282.6973 - mae: 30.2133 - val_loss: 840.8419 - val_mae: 25.9599\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1073.8076 - mae: 28.5068 - val_loss: 726.3327 - val_mae: 24.7982\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 963.0779 - mae: 27.5245 - val_loss: 650.8882 - val_mae: 23.9486\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 893.3414 - mae: 26.8576 - val_loss: 597.0441 - val_mae: 23.2607\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 839.7682 - mae: 26.3108 - val_loss: 560.3456 - val_mae: 22.7278\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 797.9468 - mae: 25.8157 - val_loss: 530.4654 - val_mae: 22.2327\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 763.4448 - mae: 25.3944 - val_loss: 512.7982 - val_mae: 21.9266\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 737.8937 - mae: 25.0501 - val_loss: 502.2312 - val_mae: 21.7437\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 719.9279 - mae: 24.7996 - val_loss: 493.4612 - val_mae: 21.5786\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 706.3853 - mae: 24.5993 - val_loss: 487.2328 - val_mae: 21.4513\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 697.3486 - mae: 24.4616 - val_loss: 481.5905 - val_mae: 21.3287\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 689.4331 - mae: 24.3326 - val_loss: 476.7777 - val_mae: 21.2170\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 682.6288 - mae: 24.2160 - val_loss: 474.1064 - val_mae: 21.1465\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 677.3605 - mae: 24.1203 - val_loss: 472.3508 - val_mae: 21.0968\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 673.2750 - mae: 24.0463 - val_loss: 470.7127 - val_mae: 21.0492\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 669.7116 - mae: 23.9793 - val_loss: 469.1746 - val_mae: 21.0034\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 666.5239 - mae: 23.9143 - val_loss: 467.6660 - val_mae: 20.9574\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 664.1074 - mae: 23.8669 - val_loss: 466.3724 - val_mae: 20.9170\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 661.8463 - mae: 23.8201 - val_loss: 465.2358 - val_mae: 20.8808\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 660.1024 - mae: 23.7825 - val_loss: 464.1742 - val_mae: 20.8462\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 658.7513 - mae: 23.7518 - val_loss: 463.1519 - val_mae: 20.8123\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 657.5283 - mae: 23.7233 - val_loss: 462.1578 - val_mae: 20.7786\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 656.3333 - mae: 23.6950 - val_loss: 461.2798 - val_mae: 20.7483\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 655.2675 - mae: 23.6697 - val_loss: 460.3729 - val_mae: 20.7164\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 654.1544 - mae: 23.6425 - val_loss: 459.5630 - val_mae: 20.6873\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 653.1506 - mae: 23.6171 - val_loss: 458.7720 - val_mae: 20.6584\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 652.1714 - mae: 23.5940 - val_loss: 458.0406 - val_mae: 20.6312\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 651.2178 - mae: 23.5691 - val_loss: 457.3333 - val_mae: 20.6048\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 650.2391 - mae: 23.5444 - val_loss: 456.8414 - val_mae: 20.5873\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 649.4467 - mae: 23.5233 - val_loss: 456.3729 - val_mae: 20.5705\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 648.7579 - mae: 23.5041 - val_loss: 455.9253 - val_mae: 20.5542\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 648.0913 - mae: 23.4853 - val_loss: 455.4706 - val_mae: 20.5373\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 647.3913 - mae: 23.4655 - val_loss: 455.0304 - val_mae: 20.5207\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 646.7341 - mae: 23.4458 - val_loss: 454.6574 - val_mae: 20.5064\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 646.2040 - mae: 23.4303 - val_loss: 454.3146 - val_mae: 20.4931\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 645.7380 - mae: 23.4164 - val_loss: 453.9773 - val_mae: 20.4799\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 645.3181 - mae: 23.4040 - val_loss: 453.6910 - val_mae: 20.4686\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 644.9799 - mae: 23.3942 - val_loss: 453.4187 - val_mae: 20.4577\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 644.6609 - mae: 23.3848 - val_loss: 453.1495 - val_mae: 20.4469\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 644.3402 - mae: 23.3755 - val_loss: 452.9121 - val_mae: 20.4372\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 644.0577 - mae: 23.3673 - val_loss: 452.6815 - val_mae: 20.4277\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 643.7840 - mae: 23.3590 - val_loss: 452.4576 - val_mae: 20.4184\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 643.5183 - mae: 23.3514 - val_loss: 452.2314 - val_mae: 20.4090\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 643.2552 - mae: 23.3432 - val_loss: 452.0058 - val_mae: 20.3994\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 642.9799 - mae: 23.3347 - val_loss: 451.7891 - val_mae: 20.3902\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 642.7314 - mae: 23.3272 - val_loss: 451.5827 - val_mae: 20.3813\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 642.5010 - mae: 23.3203 - val_loss: 451.3894 - val_mae: 20.3729\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 642.2749 - mae: 23.3132 - val_loss: 451.1992 - val_mae: 20.3646\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 642.0626 - mae: 23.3069 - val_loss: 451.0039 - val_mae: 20.3559\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 641.8441 - mae: 23.2999 - val_loss: 450.8234 - val_mae: 20.3479\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 641.6331 - mae: 23.2933 - val_loss: 450.6911 - val_mae: 20.3434\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 641.4209 - mae: 23.2864 - val_loss: 450.5754 - val_mae: 20.3394\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 641.2291 - mae: 23.2803 - val_loss: 450.4510 - val_mae: 20.3352\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 641.0311 - mae: 23.2739 - val_loss: 450.3350 - val_mae: 20.3311\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 640.8496 - mae: 23.2684 - val_loss: 450.2260 - val_mae: 20.3274\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 640.6743 - mae: 23.2625 - val_loss: 450.1107 - val_mae: 20.3233\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 640.4871 - mae: 23.2565 - val_loss: 449.9949 - val_mae: 20.3193\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 640.3018 - mae: 23.2505 - val_loss: 449.8922 - val_mae: 20.3156\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 640.1440 - mae: 23.2451 - val_loss: 449.7913 - val_mae: 20.3121\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.9862 - mae: 23.2399 - val_loss: 449.6917 - val_mae: 20.3085\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.8322 - mae: 23.2350 - val_loss: 449.5865 - val_mae: 20.3047\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.6670 - mae: 23.2295 - val_loss: 449.4942 - val_mae: 20.3014\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.5312 - mae: 23.2252 - val_loss: 449.3992 - val_mae: 20.2980\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.3801 - mae: 23.2202 - val_loss: 449.2996 - val_mae: 20.2944\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.2274 - mae: 23.2152 - val_loss: 449.2083 - val_mae: 20.2910\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 639.0873 - mae: 23.2105 - val_loss: 449.1217 - val_mae: 20.2879\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 638.9541 - mae: 23.2060 - val_loss: 449.0363 - val_mae: 20.2847\n",
      "Epoch 00068: early stopping\n",
      "5/5 [==============================] - 0s 853us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2427.4695 - mae: 38.6775 - val_loss: 1885.5056 - val_mae: 35.0395\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1268.6818 - mae: 28.7027 - val_loss: 883.0964 - val_mae: 24.2903\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 963.0519 - mae: 24.6775 - val_loss: 704.5971 - val_mae: 21.5152\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 814.2288 - mae: 22.2731 - val_loss: 657.4215 - val_mae: 20.6709\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 703.8117 - mae: 20.9356 - val_loss: 625.7899 - val_mae: 20.2759\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 638.9024 - mae: 19.9417 - val_loss: 523.8993 - val_mae: 18.1838\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 649.6253 - mae: 20.3456 - val_loss: 587.9154 - val_mae: 20.0172\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 539.1755 - mae: 18.5904 - val_loss: 334.2670 - val_mae: 14.7483\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 472.3828 - mae: 16.9554 - val_loss: 506.7221 - val_mae: 18.7686\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 395.2950 - mae: 15.9375 - val_loss: 254.8433 - val_mae: 12.0891\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 354.0885 - mae: 14.4642 - val_loss: 243.1544 - val_mae: 12.2131\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 310.0096 - mae: 13.5339 - val_loss: 221.2421 - val_mae: 11.0194\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 278.3744 - mae: 12.6221 - val_loss: 201.2593 - val_mae: 10.4299\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 252.0695 - mae: 11.8396 - val_loss: 212.1632 - val_mae: 10.8654\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 233.4467 - mae: 11.4153 - val_loss: 198.0300 - val_mae: 10.2473\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 217.7817 - mae: 10.9592 - val_loss: 209.1129 - val_mae: 10.5004\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 192.2587 - mae: 10.4140 - val_loss: 166.4232 - val_mae: 9.6948\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 193.9856 - mae: 10.3260 - val_loss: 177.3284 - val_mae: 9.1515\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.4915 - mae: 10.0652 - val_loss: 190.2070 - val_mae: 9.6684\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 177.4950 - mae: 9.6394 - val_loss: 162.2034 - val_mae: 8.4952\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 172.9356 - mae: 9.5527 - val_loss: 137.9388 - val_mae: 8.6525\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.1156 - mae: 9.4209 - val_loss: 142.6147 - val_mae: 7.9629\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 158.7012 - mae: 9.2196 - val_loss: 146.1337 - val_mae: 7.9444\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 159.4115 - mae: 8.9703 - val_loss: 188.1259 - val_mae: 9.6239\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 163.6068 - mae: 9.1282 - val_loss: 152.6542 - val_mae: 8.1021\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 158.5231 - mae: 9.0507 - val_loss: 235.1551 - val_mae: 11.4641\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 168.5901 - mae: 9.1229 - val_loss: 122.5550 - val_mae: 7.6135\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 159.9725 - mae: 9.0074 - val_loss: 155.0032 - val_mae: 8.3182\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.1007 - mae: 8.8371 - val_loss: 129.9101 - val_mae: 7.6173\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.8041 - mae: 8.8609 - val_loss: 206.9062 - val_mae: 10.6160\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 160.2860 - mae: 8.8827 - val_loss: 135.6372 - val_mae: 7.6646\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.9083 - mae: 8.9004 - val_loss: 203.1518 - val_mae: 10.4271\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 169.8374 - mae: 9.3015 - val_loss: 132.4904 - val_mae: 7.5788\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.7220 - mae: 8.7917 - val_loss: 161.4088 - val_mae: 8.6961\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.2204 - mae: 8.8046 - val_loss: 136.4710 - val_mae: 7.6656\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.7553 - mae: 8.5664 - val_loss: 112.1151 - val_mae: 7.7691\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.8427 - mae: 8.6740 - val_loss: 110.6291 - val_mae: 7.5484\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.8708 - mae: 8.7542 - val_loss: 109.4771 - val_mae: 7.6713\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.6342 - mae: 8.9264 - val_loss: 139.7870 - val_mae: 7.8321\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.1042 - mae: 8.4728 - val_loss: 142.5866 - val_mae: 7.9466\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.0277 - mae: 8.6083 - val_loss: 116.9114 - val_mae: 7.2348\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.7664 - mae: 8.4974 - val_loss: 251.3447 - val_mae: 12.2378\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.7792 - mae: 8.8468 - val_loss: 121.3612 - val_mae: 7.2452\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.2607 - mae: 8.4203 - val_loss: 107.5663 - val_mae: 7.6985\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.5430 - mae: 8.6351 - val_loss: 147.6228 - val_mae: 8.2279\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.9611 - mae: 8.4345 - val_loss: 144.5778 - val_mae: 8.0889\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.6192 - mae: 8.2797 - val_loss: 106.8694 - val_mae: 7.7375\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.9871 - mae: 8.8403 - val_loss: 105.4684 - val_mae: 7.5682\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.6459 - mae: 8.6299 - val_loss: 133.5491 - val_mae: 7.6278\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.0080 - mae: 8.3752 - val_loss: 189.1603 - val_mae: 10.0680\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 137.1260 - mae: 8.3105 - val_loss: 103.1767 - val_mae: 7.3146\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.7056 - mae: 8.4922 - val_loss: 120.6031 - val_mae: 7.1854\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 137.5900 - mae: 8.2617 - val_loss: 151.6231 - val_mae: 8.4991\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.3935 - mae: 8.1784 - val_loss: 147.3602 - val_mae: 8.2873\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.1305 - mae: 8.8380 - val_loss: 123.4405 - val_mae: 7.2681\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.4342 - mae: 8.0155 - val_loss: 111.9315 - val_mae: 8.4217\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.4260 - mae: 8.4811 - val_loss: 107.3177 - val_mae: 6.8684\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.3866 - mae: 8.7489 - val_loss: 141.9877 - val_mae: 8.0926\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.3571 - mae: 8.1502 - val_loss: 103.4076 - val_mae: 6.8338\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.6319 - mae: 8.2567 - val_loss: 125.3783 - val_mae: 7.3570\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.2590 - mae: 8.0808 - val_loss: 147.1839 - val_mae: 8.3879\n",
      "Epoch 00061: early stopping\n",
      "5/5 [==============================] - 0s 701us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 96011329536.0000 - mae: 116588.8750 - val_loss: 23951124201472.0000 - val_mae: 4790636.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 818824992106366369792.0000 - mae: 11476788224.0000 - val_loss: 187289720522779039629312.0000 - val_mae: 423977877504.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6369209360141109009445629722624.0000 - mae: 973789546414080.0000 - val_loss: 1666535771124141866861339953070080.0000 - val_mae: 39869532868706304.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 97703843493291491328.0000 - val_loss: inf - val_mae: 3246756435462167986176.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 8139616599152302795784192.0000 - val_loss: inf - val_mae: 288939044718791443844431872.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 636909874342861427087614410752.0000 - val_loss: inf - val_mae: 24966583702028050272755933773824.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 64554646302927968109757348259561472.0000 - val_loss: inf - val_mae: inf\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 617us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 489663561728.0000 - mae: 280268.5625 - val_loss: 90535503069184.0000 - val_mae: 9349477.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1809780798933065269248.0000 - mae: 16791748608.0000 - val_loss: 405697188771664087744512.0000 - val_mae: 626022219776.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8229011774092607422094800584704.0000 - mae: 1139725540786176.0000 - val_loss: 1484618470698591396721966937276416.0000 - val_mae: 37888079771467776.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 72100892805829754880.0000 - val_loss: inf - val_mae: 2225416601168204791808.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 4131308591396394436132864.0000 - val_loss: inf - val_mae: 139200228361484662212657152.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 222918516466292480212123779072.0000 - val_loss: inf - val_mae: 8097544717986975343233028063232.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 14764706588450248450976496264151040.0000 - val_loss: inf - val_mae: 548779294784184161448807329762902016.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan   \n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 596us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 311732830208.0000 - mae: 228342.2500 - val_loss: 47703996235776.0000 - val_mae: 6760310.5000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 616128809665023705088.0000 - mae: 9543495680.0000 - val_loss: 131659501521643998543872.0000 - val_mae: 354875637760.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2090265383110823438054695370752.0000 - mae: 603735701061632.0000 - val_loss: 358926284652213090493171775832064.0000 - val_mae: 18612358888816640.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 31297154281946218496.0000 - val_loss: inf - val_mae: 1023981617687881056256.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 1904012250794119982481408.0000 - val_loss: inf - val_mae: 55965180240229823892422656.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 96247712989521025033060745216.0000 - val_loss: inf - val_mae: 2874244105724353540818252857344.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 4831265572902688431884259364438016.0000 - val_loss: inf - val_mae: 150102497669504329148182427355578368.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan   \n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 618us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 8810.3350 - mae: 75.5116 - val_loss: 10184.6055 - val_mae: 83.6167\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8555.1016 - mae: 74.0692 - val_loss: 9897.9922 - val_mae: 82.1001\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8300.4004 - mae: 72.6210 - val_loss: 9619.9082 - val_mae: 80.6021\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8056.0234 - mae: 71.1989 - val_loss: 9347.7275 - val_mae: 79.1120\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7821.9395 - mae: 69.7982 - val_loss: 9083.9863 - val_mae: 77.6424\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7586.2817 - mae: 68.4314 - val_loss: 8829.4795 - val_mae: 76.1988\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7365.0972 - mae: 67.1003 - val_loss: 8579.0713 - val_mae: 74.7544\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7147.4580 - mae: 65.7442 - val_loss: 8334.1797 - val_mae: 73.3196\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6933.4834 - mae: 64.4656 - val_loss: 8096.9346 - val_mae: 71.9019\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6727.1060 - mae: 63.1855 - val_loss: 7869.5757 - val_mae: 70.5192\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6534.3179 - mae: 61.9544 - val_loss: 7644.2227 - val_mae: 69.1265\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6341.0244 - mae: 60.7633 - val_loss: 7431.5444 - val_mae: 67.7872\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6156.0063 - mae: 59.6059 - val_loss: 7224.6807 - val_mae: 66.4760\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5984.3701 - mae: 58.4692 - val_loss: 7019.4238 - val_mae: 65.1844\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5804.9248 - mae: 57.3232 - val_loss: 6826.4312 - val_mae: 63.9872\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5638.2065 - mae: 56.2418 - val_loss: 6636.5127 - val_mae: 62.7916\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5481.9219 - mae: 55.2065 - val_loss: 6447.4092 - val_mae: 61.5823\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5322.4863 - mae: 54.1686 - val_loss: 6266.1807 - val_mae: 60.4017\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5167.5864 - mae: 53.1724 - val_loss: 6094.5811 - val_mae: 59.2635\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5021.6367 - mae: 52.1830 - val_loss: 5926.1650 - val_mae: 58.1369\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4878.8066 - mae: 51.2113 - val_loss: 5762.6685 - val_mae: 57.0475\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4741.1870 - mae: 50.2721 - val_loss: 5603.0415 - val_mae: 56.0475\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4610.0981 - mae: 49.3544 - val_loss: 5445.8252 - val_mae: 55.0643\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4477.2939 - mae: 48.4044 - val_loss: 5296.0630 - val_mae: 54.1203\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4352.2114 - mae: 47.5384 - val_loss: 5150.9165 - val_mae: 53.1907\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4229.3105 - mae: 46.6858 - val_loss: 5011.2700 - val_mae: 52.3115\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4114.5898 - mae: 45.8414 - val_loss: 4872.9009 - val_mae: 51.4341\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4000.5693 - mae: 45.0180 - val_loss: 4741.3320 - val_mae: 50.5852\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3893.0195 - mae: 44.2416 - val_loss: 4612.3013 - val_mae: 49.7523\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3784.2676 - mae: 43.4396 - val_loss: 4488.0410 - val_mae: 48.9601\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3683.4246 - mae: 42.7082 - val_loss: 4366.2056 - val_mae: 48.1858\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3582.8081 - mae: 41.9810 - val_loss: 4249.2529 - val_mae: 47.4563\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3489.2671 - mae: 41.2939 - val_loss: 4134.8633 - val_mae: 46.7351\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3396.1997 - mae: 40.6466 - val_loss: 4026.5142 - val_mae: 46.0412\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3308.7632 - mae: 40.0176 - val_loss: 3921.9968 - val_mae: 45.4106\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3218.5662 - mae: 39.4199 - val_loss: 3823.8740 - val_mae: 44.8273\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3141.0093 - mae: 38.8828 - val_loss: 3720.6396 - val_mae: 44.2032\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3059.9495 - mae: 38.3448 - val_loss: 3621.3748 - val_mae: 43.5930\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2980.4807 - mae: 37.8105 - val_loss: 3526.4873 - val_mae: 42.9990\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2906.5569 - mae: 37.3147 - val_loss: 3434.2139 - val_mae: 42.4218\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2832.2173 - mae: 36.8150 - val_loss: 3346.4849 - val_mae: 41.8636\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2761.9119 - mae: 36.3465 - val_loss: 3262.9436 - val_mae: 41.3234\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2694.8972 - mae: 35.8923 - val_loss: 3182.8140 - val_mae: 40.8071\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2632.6370 - mae: 35.4690 - val_loss: 3102.4771 - val_mae: 40.3060\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2566.6431 - mae: 35.0242 - val_loss: 3028.9248 - val_mae: 39.8413\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2508.2544 - mae: 34.6250 - val_loss: 2958.2927 - val_mae: 39.3972\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2450.7310 - mae: 34.2577 - val_loss: 2890.3425 - val_mae: 38.9661\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2396.3252 - mae: 33.8779 - val_loss: 2823.3145 - val_mae: 38.5341\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2343.2539 - mae: 33.5242 - val_loss: 2759.3755 - val_mae: 38.1146\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2292.6306 - mae: 33.1864 - val_loss: 2696.4722 - val_mae: 37.6960\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2243.3591 - mae: 32.8755 - val_loss: 2636.4780 - val_mae: 37.2899\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2196.1194 - mae: 32.5518 - val_loss: 2578.8813 - val_mae: 36.8968\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2151.7104 - mae: 32.2686 - val_loss: 2523.3535 - val_mae: 36.5150\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2108.8264 - mae: 31.9950 - val_loss: 2469.2161 - val_mae: 36.1367\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2067.9050 - mae: 31.7273 - val_loss: 2415.2881 - val_mae: 35.7536\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2025.4261 - mae: 31.4624 - val_loss: 2364.7288 - val_mae: 35.3888\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1986.1652 - mae: 31.2089 - val_loss: 2315.6494 - val_mae: 35.0286\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1950.1270 - mae: 30.9711 - val_loss: 2264.9924 - val_mae: 34.6511\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1910.1250 - mae: 30.7305 - val_loss: 2219.7004 - val_mae: 34.3072\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1876.1040 - mae: 30.5242 - val_loss: 2175.0725 - val_mae: 33.9632\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1842.6807 - mae: 30.3031 - val_loss: 2131.5186 - val_mae: 33.6268\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1809.4114 - mae: 30.0905 - val_loss: 2090.3086 - val_mae: 33.3197\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1778.6726 - mae: 29.8812 - val_loss: 2050.6938 - val_mae: 33.0410\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1749.4038 - mae: 29.6993 - val_loss: 2012.1255 - val_mae: 32.7862\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1719.4109 - mae: 29.5036 - val_loss: 1976.0704 - val_mae: 32.5543\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1692.6840 - mae: 29.3229 - val_loss: 1940.0803 - val_mae: 32.3220\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1664.5417 - mae: 29.1334 - val_loss: 1906.0054 - val_mae: 32.0985\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1639.4218 - mae: 28.9670 - val_loss: 1872.5338 - val_mae: 31.8754\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1612.9873 - mae: 28.8019 - val_loss: 1842.8264 - val_mae: 31.6741\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1591.2262 - mae: 28.6438 - val_loss: 1812.4401 - val_mae: 31.4648\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1567.9216 - mae: 28.4960 - val_loss: 1783.7792 - val_mae: 31.2650\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1547.3344 - mae: 28.3645 - val_loss: 1755.0331 - val_mae: 31.0699\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1524.3457 - mae: 28.2124 - val_loss: 1730.5474 - val_mae: 30.9007\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1505.5634 - mae: 28.0870 - val_loss: 1705.2456 - val_mae: 30.7233\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1485.4878 - mae: 27.9680 - val_loss: 1680.8640 - val_mae: 30.5568\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1466.3389 - mae: 27.8447 - val_loss: 1657.3733 - val_mae: 30.4083\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1447.9084 - mae: 27.7102 - val_loss: 1634.3146 - val_mae: 30.2600\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1429.1384 - mae: 27.5931 - val_loss: 1613.1791 - val_mae: 30.1223\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1413.4353 - mae: 27.4847 - val_loss: 1591.5990 - val_mae: 29.9796\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1396.1255 - mae: 27.3632 - val_loss: 1571.7623 - val_mae: 29.8461\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1381.4003 - mae: 27.2675 - val_loss: 1551.3323 - val_mae: 29.7067\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1365.9091 - mae: 27.1614 - val_loss: 1530.4524 - val_mae: 29.5622\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1350.5168 - mae: 27.0619 - val_loss: 1511.0563 - val_mae: 29.4259\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1336.4321 - mae: 26.9670 - val_loss: 1492.5303 - val_mae: 29.2934\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1323.0846 - mae: 26.8751 - val_loss: 1474.8077 - val_mae: 29.1647\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1310.1017 - mae: 26.8034 - val_loss: 1457.6757 - val_mae: 29.0385\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1297.5343 - mae: 26.7243 - val_loss: 1441.2654 - val_mae: 28.9126\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1286.2565 - mae: 26.6679 - val_loss: 1424.1395 - val_mae: 28.7804\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1274.5082 - mae: 26.5955 - val_loss: 1407.3635 - val_mae: 28.6484\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1261.9604 - mae: 26.5220 - val_loss: 1392.0453 - val_mae: 28.5261\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1251.6401 - mae: 26.4646 - val_loss: 1376.4557 - val_mae: 28.4003\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1240.5422 - mae: 26.3922 - val_loss: 1361.3751 - val_mae: 28.2766\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1230.1206 - mae: 26.3332 - val_loss: 1346.5194 - val_mae: 28.1537\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1219.9923 - mae: 26.2787 - val_loss: 1331.9950 - val_mae: 28.0320\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1209.2765 - mae: 26.2173 - val_loss: 1318.7705 - val_mae: 27.9196\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1200.5247 - mae: 26.1600 - val_loss: 1304.7500 - val_mae: 27.7989\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1191.4272 - mae: 26.1109 - val_loss: 1290.8335 - val_mae: 27.6871\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1180.9137 - mae: 26.0464 - val_loss: 1279.0907 - val_mae: 27.5926\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1173.1697 - mae: 25.9969 - val_loss: 1265.5704 - val_mae: 27.4835\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1164.4625 - mae: 25.9433 - val_loss: 1252.8033 - val_mae: 27.3791\n",
      "5/5 [==============================] - 0s 750us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1301.2925 - mae: 29.4042 - val_loss: 1484.2240 - val_mae: 32.5590\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1277.3976 - mae: 29.0888 - val_loss: 1457.4563 - val_mae: 32.1985\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1254.1436 - mae: 28.7738 - val_loss: 1431.0635 - val_mae: 31.8393\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1231.0680 - mae: 28.4629 - val_loss: 1405.3390 - val_mae: 31.4856\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1208.6322 - mae: 28.1643 - val_loss: 1380.2976 - val_mae: 31.1378\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1186.6025 - mae: 27.8692 - val_loss: 1355.8114 - val_mae: 30.7942\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1165.4657 - mae: 27.5775 - val_loss: 1331.5968 - val_mae: 30.4510\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1144.9893 - mae: 27.2920 - val_loss: 1307.3969 - val_mae: 30.1046\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1123.6111 - mae: 27.0010 - val_loss: 1284.3868 - val_mae: 29.7715\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1103.8577 - mae: 26.7202 - val_loss: 1261.7734 - val_mae: 29.4409\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1084.4645 - mae: 26.4532 - val_loss: 1239.4436 - val_mae: 29.1113\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1065.2809 - mae: 26.1834 - val_loss: 1217.8186 - val_mae: 28.7887\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1046.5975 - mae: 25.9143 - val_loss: 1196.5569 - val_mae: 28.4683\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1028.8676 - mae: 25.6484 - val_loss: 1175.3557 - val_mae: 28.1460\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1010.7564 - mae: 25.3888 - val_loss: 1154.7855 - val_mae: 27.8298\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 992.8292 - mae: 25.1302 - val_loss: 1135.1459 - val_mae: 27.5248\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 976.3068 - mae: 24.8856 - val_loss: 1115.4630 - val_mae: 27.2159\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 959.4926 - mae: 24.6349 - val_loss: 1096.2659 - val_mae: 26.9114\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 943.1835 - mae: 24.3885 - val_loss: 1077.4818 - val_mae: 26.6111\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 927.0323 - mae: 24.1428 - val_loss: 1059.1792 - val_mae: 26.3154\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 911.4615 - mae: 23.9031 - val_loss: 1041.0958 - val_mae: 26.0203\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 896.2616 - mae: 23.6717 - val_loss: 1023.1660 - val_mae: 25.7243\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 881.0144 - mae: 23.4376 - val_loss: 1005.7647 - val_mae: 25.4340\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 866.4936 - mae: 23.2027 - val_loss: 988.7910 - val_mae: 25.1483\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 852.3182 - mae: 22.9824 - val_loss: 972.3248 - val_mae: 24.8681\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 838.5752 - mae: 22.7571 - val_loss: 955.9827 - val_mae: 24.5876\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 824.6092 - mae: 22.5384 - val_loss: 940.2636 - val_mae: 24.3149\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 811.8214 - mae: 22.3206 - val_loss: 924.2710 - val_mae: 24.0352\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 798.4072 - mae: 22.1086 - val_loss: 908.9846 - val_mae: 23.7646\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 785.7789 - mae: 21.8949 - val_loss: 894.0508 - val_mae: 23.4968\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 773.1471 - mae: 21.6836 - val_loss: 879.8380 - val_mae: 23.2531\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 761.1649 - mae: 21.4873 - val_loss: 865.7347 - val_mae: 23.0094\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 749.3696 - mae: 21.2946 - val_loss: 852.0229 - val_mae: 22.7702\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 737.7440 - mae: 21.1024 - val_loss: 838.6288 - val_mae: 22.5345\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 726.9096 - mae: 20.9163 - val_loss: 825.0533 - val_mae: 22.2937\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 715.2036 - mae: 20.7277 - val_loss: 812.3918 - val_mae: 22.0663\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 704.7847 - mae: 20.5491 - val_loss: 799.5011 - val_mae: 21.8326\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 694.1240 - mae: 20.3745 - val_loss: 786.9050 - val_mae: 21.6020\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 683.7350 - mae: 20.1979 - val_loss: 774.5495 - val_mae: 21.3738\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 673.5911 - mae: 20.0281 - val_loss: 762.3801 - val_mae: 21.1467\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 663.3503 - mae: 19.8565 - val_loss: 750.7169 - val_mae: 20.9273\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 653.7953 - mae: 19.6925 - val_loss: 739.1586 - val_mae: 20.7076\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 644.3392 - mae: 19.5325 - val_loss: 727.8160 - val_mae: 20.4898\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.3170 - mae: 19.3720 - val_loss: 716.4673 - val_mae: 20.2696\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.7102 - mae: 19.2150 - val_loss: 705.8866 - val_mae: 20.0659\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 617.2863 - mae: 19.0715 - val_loss: 695.1420 - val_mae: 19.8650\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 608.4660 - mae: 18.9272 - val_loss: 684.6696 - val_mae: 19.6672\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 600.0251 - mae: 18.7849 - val_loss: 674.3379 - val_mae: 19.4707\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 591.8730 - mae: 18.6413 - val_loss: 664.1728 - val_mae: 19.2752\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 583.5629 - mae: 18.4999 - val_loss: 654.3802 - val_mae: 19.0852\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 575.5185 - mae: 18.3629 - val_loss: 644.6656 - val_mae: 18.8950\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 567.6116 - mae: 18.2267 - val_loss: 635.0869 - val_mae: 18.7057\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 560.0124 - mae: 18.0921 - val_loss: 625.6548 - val_mae: 18.5208\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 552.2669 - mae: 17.9649 - val_loss: 616.6027 - val_mae: 18.3497\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 545.1060 - mae: 17.8361 - val_loss: 607.5319 - val_mae: 18.1766\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 538.0881 - mae: 17.7093 - val_loss: 598.6669 - val_mae: 18.0058\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 531.0623 - mae: 17.5825 - val_loss: 590.1591 - val_mae: 17.8427\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 524.3251 - mae: 17.4631 - val_loss: 581.7519 - val_mae: 17.6888\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 517.5835 - mae: 17.3434 - val_loss: 573.5727 - val_mae: 17.5406\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 511.1367 - mae: 17.2234 - val_loss: 565.6161 - val_mae: 17.4103\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 504.7570 - mae: 17.1067 - val_loss: 557.9534 - val_mae: 17.2930\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 498.7792 - mae: 16.9945 - val_loss: 550.3040 - val_mae: 17.1778\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 492.6763 - mae: 16.8783 - val_loss: 542.8065 - val_mae: 17.0639\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 486.7633 - mae: 16.7720 - val_loss: 535.4333 - val_mae: 16.9507\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 481.1190 - mae: 16.6656 - val_loss: 527.9833 - val_mae: 16.8352\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 475.5409 - mae: 16.5599 - val_loss: 520.6185 - val_mae: 16.7200\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 469.6280 - mae: 16.4539 - val_loss: 513.7782 - val_mae: 16.6118\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 464.4773 - mae: 16.3563 - val_loss: 506.7866 - val_mae: 16.5002\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 459.0377 - mae: 16.2582 - val_loss: 500.1252 - val_mae: 16.3929\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 453.9385 - mae: 16.1631 - val_loss: 493.6060 - val_mae: 16.2868\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 448.8947 - mae: 16.0719 - val_loss: 487.2223 - val_mae: 16.1820\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 444.1070 - mae: 15.9868 - val_loss: 480.7843 - val_mae: 16.0752\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 439.0629 - mae: 15.8945 - val_loss: 474.7540 - val_mae: 15.9742\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 434.3940 - mae: 15.8067 - val_loss: 468.7626 - val_mae: 15.8730\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 429.8832 - mae: 15.7245 - val_loss: 462.7632 - val_mae: 15.7706\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 425.4626 - mae: 15.6392 - val_loss: 456.8342 - val_mae: 15.6685\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 421.0173 - mae: 15.5533 - val_loss: 451.0740 - val_mae: 15.5684\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 416.4348 - mae: 15.4742 - val_loss: 445.7608 - val_mae: 15.4752\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 412.5529 - mae: 15.3991 - val_loss: 440.0775 - val_mae: 15.3821\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 408.2565 - mae: 15.3173 - val_loss: 434.7008 - val_mae: 15.2946\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 404.3026 - mae: 15.2476 - val_loss: 429.4053 - val_mae: 15.2077\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 400.3393 - mae: 15.1684 - val_loss: 424.1826 - val_mae: 15.1286\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 396.4235 - mae: 15.0956 - val_loss: 419.0582 - val_mae: 15.0517\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 392.5346 - mae: 15.0155 - val_loss: 414.2385 - val_mae: 14.9787\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 388.9511 - mae: 14.9470 - val_loss: 409.3625 - val_mae: 14.9041\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 385.4380 - mae: 14.8791 - val_loss: 404.5262 - val_mae: 14.8294\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 381.8547 - mae: 14.8038 - val_loss: 399.8103 - val_mae: 14.7558\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 378.3851 - mae: 14.7372 - val_loss: 395.2441 - val_mae: 14.6839\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 375.0551 - mae: 14.6684 - val_loss: 390.8047 - val_mae: 14.6133\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 371.6997 - mae: 14.6067 - val_loss: 386.5059 - val_mae: 14.5444\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 368.6397 - mae: 14.5458 - val_loss: 382.0291 - val_mae: 14.4719\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 365.1974 - mae: 14.4794 - val_loss: 377.9332 - val_mae: 14.4050\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 362.3489 - mae: 14.4263 - val_loss: 373.6989 - val_mae: 14.3351\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 359.1991 - mae: 14.3656 - val_loss: 369.6746 - val_mae: 14.2682\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 356.1325 - mae: 14.3112 - val_loss: 365.7784 - val_mae: 14.2027\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 353.4485 - mae: 14.2664 - val_loss: 361.7273 - val_mae: 14.1340\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 350.4435 - mae: 14.2171 - val_loss: 357.8996 - val_mae: 14.0685\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 347.6646 - mae: 14.1719 - val_loss: 354.1288 - val_mae: 14.0034\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 344.9205 - mae: 14.1257 - val_loss: 350.3653 - val_mae: 13.9434\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 342.3580 - mae: 14.0842 - val_loss: 346.5942 - val_mae: 13.8845\n",
      "5/5 [==============================] - 0s 672us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1173.4443 - mae: 27.2662 - val_loss: 1007.4750 - val_mae: 23.9113\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1140.0760 - mae: 26.8260 - val_loss: 977.0959 - val_mae: 23.4827\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1106.0593 - mae: 26.3643 - val_loss: 946.4343 - val_mae: 23.0432\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1072.8444 - mae: 25.9081 - val_loss: 914.2955 - val_mae: 22.5837\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1040.0369 - mae: 25.4404 - val_loss: 882.0144 - val_mae: 22.1140\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1007.9203 - mae: 24.9839 - val_loss: 849.8707 - val_mae: 21.6564\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 974.8502 - mae: 24.5020 - val_loss: 818.3669 - val_mae: 21.2038\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 942.8495 - mae: 24.0362 - val_loss: 787.3820 - val_mae: 20.7440\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 910.6846 - mae: 23.5852 - val_loss: 757.9777 - val_mae: 20.2927\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 879.5830 - mae: 23.1163 - val_loss: 729.5372 - val_mae: 19.8877\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 848.2281 - mae: 22.6478 - val_loss: 702.7074 - val_mae: 19.4982\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 817.5672 - mae: 22.1918 - val_loss: 676.3724 - val_mae: 19.1157\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 786.4756 - mae: 21.7538 - val_loss: 648.0647 - val_mae: 18.7184\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 755.5453 - mae: 21.3207 - val_loss: 618.5695 - val_mae: 18.3291\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 722.6125 - mae: 20.8643 - val_loss: 589.8825 - val_mae: 17.9459\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 688.6926 - mae: 20.4005 - val_loss: 558.9514 - val_mae: 17.5047\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 655.3152 - mae: 19.9034 - val_loss: 527.5375 - val_mae: 17.0393\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.2814 - mae: 19.3935 - val_loss: 500.5956 - val_mae: 16.6827\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 587.5910 - mae: 18.9063 - val_loss: 476.5931 - val_mae: 16.3447\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 557.9442 - mae: 18.4081 - val_loss: 454.6134 - val_mae: 16.0140\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 529.6545 - mae: 17.9576 - val_loss: 434.7088 - val_mae: 15.6930\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 504.2477 - mae: 17.5272 - val_loss: 416.9265 - val_mae: 15.4128\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 480.9050 - mae: 17.1694 - val_loss: 401.4468 - val_mae: 15.1785\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.5194 - mae: 16.8485 - val_loss: 387.6975 - val_mae: 14.9578\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 441.7673 - mae: 16.5172 - val_loss: 376.0027 - val_mae: 14.8052\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 425.7167 - mae: 16.2336 - val_loss: 365.6725 - val_mae: 14.6977\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 411.2711 - mae: 15.9851 - val_loss: 356.9100 - val_mae: 14.6342\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 397.8818 - mae: 15.7660 - val_loss: 349.5649 - val_mae: 14.5693\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 387.0748 - mae: 15.5962 - val_loss: 343.6024 - val_mae: 14.5537\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 377.2173 - mae: 15.4425 - val_loss: 338.4843 - val_mae: 14.5393\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 367.9547 - mae: 15.3062 - val_loss: 334.1840 - val_mae: 14.5190\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 359.9843 - mae: 15.1705 - val_loss: 330.6568 - val_mae: 14.5012\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 352.9994 - mae: 15.0600 - val_loss: 327.9022 - val_mae: 14.4890\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 347.1441 - mae: 14.9730 - val_loss: 325.4508 - val_mae: 14.4722\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 341.7196 - mae: 14.8769 - val_loss: 323.5063 - val_mae: 14.4600\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 337.2362 - mae: 14.7901 - val_loss: 321.7288 - val_mae: 14.4422\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 333.1116 - mae: 14.7153 - val_loss: 319.9199 - val_mae: 14.4370\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 328.9472 - mae: 14.6340 - val_loss: 317.9722 - val_mae: 14.4157\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 325.4167 - mae: 14.5642 - val_loss: 315.8689 - val_mae: 14.3826\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 322.4155 - mae: 14.4958 - val_loss: 313.7744 - val_mae: 14.3515\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 319.2234 - mae: 14.4322 - val_loss: 311.8841 - val_mae: 14.3233\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 316.1785 - mae: 14.3674 - val_loss: 309.9903 - val_mae: 14.2922\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 313.1114 - mae: 14.3001 - val_loss: 308.1409 - val_mae: 14.2592\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 310.3514 - mae: 14.2401 - val_loss: 306.1913 - val_mae: 14.2195\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 307.5652 - mae: 14.1771 - val_loss: 303.9330 - val_mae: 14.1670\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 304.9160 - mae: 14.1121 - val_loss: 301.6260 - val_mae: 14.1113\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 302.3732 - mae: 14.0551 - val_loss: 299.5967 - val_mae: 14.0627\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 299.8146 - mae: 13.9949 - val_loss: 297.0898 - val_mae: 13.9976\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 297.2334 - mae: 13.9267 - val_loss: 294.2356 - val_mae: 13.9194\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 294.8688 - mae: 13.8654 - val_loss: 291.8795 - val_mae: 13.8679\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 292.2035 - mae: 13.7973 - val_loss: 289.4932 - val_mae: 13.8138\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 289.7404 - mae: 13.7350 - val_loss: 286.8841 - val_mae: 13.7502\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 287.2236 - mae: 13.6692 - val_loss: 284.3196 - val_mae: 13.6866\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 284.8787 - mae: 13.6080 - val_loss: 282.1810 - val_mae: 13.6435\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 282.2146 - mae: 13.5365 - val_loss: 279.8496 - val_mae: 13.5903\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 279.6570 - mae: 13.4677 - val_loss: 277.8970 - val_mae: 13.5486\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 277.1912 - mae: 13.3998 - val_loss: 275.0347 - val_mae: 13.4707\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 274.6906 - mae: 13.3315 - val_loss: 272.4042 - val_mae: 13.4006\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 272.3148 - mae: 13.2697 - val_loss: 270.2139 - val_mae: 13.3472\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 269.8985 - mae: 13.2008 - val_loss: 267.7264 - val_mae: 13.2811\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 267.5632 - mae: 13.1354 - val_loss: 265.3116 - val_mae: 13.2157\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 265.2053 - mae: 13.0700 - val_loss: 263.0489 - val_mae: 13.1568\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 262.9117 - mae: 13.0022 - val_loss: 261.0231 - val_mae: 13.1046\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 260.6873 - mae: 12.9379 - val_loss: 258.1043 - val_mae: 13.0157\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.4630 - mae: 12.8690 - val_loss: 255.8514 - val_mae: 12.9542\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 256.0934 - mae: 12.8030 - val_loss: 253.2260 - val_mae: 12.8757\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 253.8227 - mae: 12.7349 - val_loss: 250.8412 - val_mae: 12.8073\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 251.6041 - mae: 12.6694 - val_loss: 248.7785 - val_mae: 12.7506\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 249.3754 - mae: 12.6016 - val_loss: 245.8714 - val_mae: 12.6605\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 247.1145 - mae: 12.5334 - val_loss: 243.6064 - val_mae: 12.5940\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 244.9711 - mae: 12.4682 - val_loss: 241.3137 - val_mae: 12.5245\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 242.8392 - mae: 12.4040 - val_loss: 239.1474 - val_mae: 12.4590\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 240.7796 - mae: 12.3432 - val_loss: 237.0616 - val_mae: 12.3949\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 238.8100 - mae: 12.2778 - val_loss: 234.9893 - val_mae: 12.3309\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 236.7674 - mae: 12.2165 - val_loss: 232.9863 - val_mae: 12.2691\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 234.7589 - mae: 12.1534 - val_loss: 231.0195 - val_mae: 12.2075\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 232.8538 - mae: 12.0928 - val_loss: 229.0506 - val_mae: 12.1459\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 230.7693 - mae: 12.0296 - val_loss: 226.9368 - val_mae: 12.0767\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 228.9937 - mae: 11.9702 - val_loss: 225.2227 - val_mae: 12.0241\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 226.8490 - mae: 11.9042 - val_loss: 222.9153 - val_mae: 11.9459\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 225.0258 - mae: 11.8448 - val_loss: 220.5894 - val_mae: 11.8661\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 223.0865 - mae: 11.7822 - val_loss: 218.6225 - val_mae: 11.8000\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 221.2957 - mae: 11.7234 - val_loss: 216.7312 - val_mae: 11.7366\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 219.3470 - mae: 11.6633 - val_loss: 215.0042 - val_mae: 11.6786\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 217.5030 - mae: 11.6001 - val_loss: 213.3253 - val_mae: 11.6218\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 215.6835 - mae: 11.5410 - val_loss: 211.4135 - val_mae: 11.5552\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 213.8908 - mae: 11.4807 - val_loss: 209.3685 - val_mae: 11.4823\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 212.0221 - mae: 11.4161 - val_loss: 207.4990 - val_mae: 11.4163\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 210.2110 - mae: 11.3552 - val_loss: 205.5277 - val_mae: 11.3457\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 208.4667 - mae: 11.2969 - val_loss: 203.9053 - val_mae: 11.2874\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 206.6693 - mae: 11.2360 - val_loss: 201.9056 - val_mae: 11.2133\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 204.9529 - mae: 11.1739 - val_loss: 200.0996 - val_mae: 11.1466\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 203.2776 - mae: 11.1161 - val_loss: 198.4588 - val_mae: 11.0868\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 201.5811 - mae: 11.0611 - val_loss: 196.9374 - val_mae: 11.0308\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 199.7953 - mae: 11.0020 - val_loss: 195.0773 - val_mae: 10.9596\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 198.2157 - mae: 10.9450 - val_loss: 193.1795 - val_mae: 10.8873\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 196.4482 - mae: 10.8866 - val_loss: 191.2985 - val_mae: 10.8149\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 194.8593 - mae: 10.8312 - val_loss: 189.6228 - val_mae: 10.7502\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 193.2937 - mae: 10.7767 - val_loss: 187.8818 - val_mae: 10.6821\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 191.5952 - mae: 10.7185 - val_loss: 186.0557 - val_mae: 10.6105\n",
      "5/5 [==============================] - 0s 754us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1037.3821 - mae: 17.8971 - val_loss: 19.5113 - val_mae: 3.6188\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1824 - mae: 6.3498 - val_loss: 16.2085 - val_mae: 3.3552\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.3540 - mae: 5.9083 - val_loss: 14.9033 - val_mae: 3.2188\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 71.9322 - mae: 5.9997 - val_loss: 23.3065 - val_mae: 3.9911\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.0892 - mae: 5.9569 - val_loss: 15.2692 - val_mae: 3.2727\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.1437 - mae: 5.7790 - val_loss: 15.5888 - val_mae: 3.1238\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.3257 - mae: 5.7548 - val_loss: 16.1964 - val_mae: 3.3760\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4458 - mae: 5.8574 - val_loss: 34.2286 - val_mae: 4.8458\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.3941 - mae: 5.7421 - val_loss: 37.1319 - val_mae: 5.0775\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.6572 - mae: 5.8444 - val_loss: 21.4437 - val_mae: 3.8532\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.6862 - mae: 5.7924 - val_loss: 15.1404 - val_mae: 3.2777\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.1591 - mae: 5.7030 - val_loss: 15.5568 - val_mae: 3.3030\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.6989 - mae: 5.6486 - val_loss: 49.2269 - val_mae: 5.8747\n",
      "Epoch 00013: early stopping\n",
      "5/5 [==============================] - 0s 740us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1708.2102 - mae: 23.0118 - val_loss: 68.9517 - val_mae: 6.6211\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.7734 - mae: 8.0480 - val_loss: 31.1799 - val_mae: 4.2523\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.8028 - mae: 7.6601 - val_loss: 33.1047 - val_mae: 4.9750\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.4261 - mae: 7.2393 - val_loss: 30.9611 - val_mae: 4.7954\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.5259 - mae: 7.1816 - val_loss: 23.3680 - val_mae: 4.1190\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.7469 - mae: 7.4177 - val_loss: 21.4656 - val_mae: 3.6192\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.6713 - mae: 6.9625 - val_loss: 27.0133 - val_mae: 4.4507\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.6156 - mae: 7.1795 - val_loss: 29.7238 - val_mae: 4.7045\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.1043 - mae: 6.7520 - val_loss: 35.4563 - val_mae: 4.9860\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.6949 - mae: 6.8015 - val_loss: 45.9961 - val_mae: 5.5899\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.2886 - mae: 6.9532 - val_loss: 19.0846 - val_mae: 3.6162\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.2850 - mae: 6.5940 - val_loss: 37.0008 - val_mae: 5.0328\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.4479 - mae: 6.7653 - val_loss: 30.0096 - val_mae: 4.6264\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.2446 - mae: 6.9491 - val_loss: 20.9400 - val_mae: 3.8840\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.4463 - mae: 6.5515 - val_loss: 22.7954 - val_mae: 4.1028\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.7108 - mae: 6.5603 - val_loss: 22.0443 - val_mae: 4.0204\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.7574 - mae: 6.6034 - val_loss: 36.5321 - val_mae: 5.0583\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.9665 - mae: 6.7704 - val_loss: 21.0772 - val_mae: 3.9406\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.1393 - mae: 6.4702 - val_loss: 22.9702 - val_mae: 4.1187\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.4207 - mae: 6.5725 - val_loss: 28.4994 - val_mae: 4.5440\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.5112 - mae: 6.5981 - val_loss: 20.8000 - val_mae: 3.9108\n",
      "Epoch 00021: early stopping\n",
      "5/5 [==============================] - 0s 698us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1617.9922 - mae: 30.3527 - val_loss: 192.8992 - val_mae: 11.1049\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.4221 - mae: 9.0872 - val_loss: 134.6459 - val_mae: 8.8109\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 110.3641 - mae: 7.8092 - val_loss: 107.8550 - val_mae: 7.5786\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.3860 - mae: 6.8598 - val_loss: 88.2166 - val_mae: 6.9026\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1648 - mae: 6.3465 - val_loss: 73.3745 - val_mae: 6.2608\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.3569 - mae: 5.9978 - val_loss: 65.4333 - val_mae: 5.9390\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.6296 - mae: 5.9335 - val_loss: 64.2231 - val_mae: 5.9787\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.3575 - mae: 5.7568 - val_loss: 58.5213 - val_mae: 5.5363\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.4123 - mae: 5.3058 - val_loss: 52.7948 - val_mae: 5.3980\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.7173 - mae: 5.3735 - val_loss: 51.4342 - val_mae: 5.2988\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.0372 - mae: 5.2015 - val_loss: 48.6273 - val_mae: 5.1543\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.5972 - mae: 5.2249 - val_loss: 51.5003 - val_mae: 5.3455\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.2016 - mae: 5.1653 - val_loss: 45.1702 - val_mae: 5.1084\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.6722 - mae: 5.1248 - val_loss: 44.6110 - val_mae: 4.9947\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.9271 - mae: 5.0588 - val_loss: 45.9815 - val_mae: 5.0980\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.8359 - mae: 5.0226 - val_loss: 48.4275 - val_mae: 5.4843\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.3823 - mae: 5.2254 - val_loss: 45.9745 - val_mae: 5.1713\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.8414 - mae: 5.1125 - val_loss: 42.1220 - val_mae: 4.9525\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.0165 - mae: 5.0726 - val_loss: 44.0586 - val_mae: 5.0742\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.5350 - mae: 4.9967 - val_loss: 41.0010 - val_mae: 4.8968\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.1010 - mae: 5.0917 - val_loss: 42.2073 - val_mae: 5.0097\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.8716 - mae: 4.8812 - val_loss: 39.6438 - val_mae: 4.8912\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.2812 - mae: 5.0601 - val_loss: 42.7065 - val_mae: 5.0401\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.7691 - mae: 4.9196 - val_loss: 40.9612 - val_mae: 5.1109\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.1748 - mae: 5.0781 - val_loss: 39.3475 - val_mae: 4.8470\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.5507 - mae: 4.8799 - val_loss: 41.9261 - val_mae: 5.1544\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.4601 - mae: 5.1420 - val_loss: 39.7362 - val_mae: 4.8769\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.1857 - mae: 5.0140 - val_loss: 37.6463 - val_mae: 4.7635\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.8576 - mae: 4.8888 - val_loss: 38.6446 - val_mae: 4.8088\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.3658 - mae: 4.8946 - val_loss: 41.1464 - val_mae: 5.0896\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.6780 - mae: 4.9533 - val_loss: 37.7222 - val_mae: 4.7458\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.2759 - mae: 4.8630 - val_loss: 39.6571 - val_mae: 4.9834\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.9401 - mae: 5.0829 - val_loss: 44.1599 - val_mae: 5.1577\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.6154 - mae: 5.0443 - val_loss: 36.7295 - val_mae: 4.6971\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 49.9594 - mae: 4.8114 - val_loss: 37.0827 - val_mae: 4.7148\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.9685 - mae: 4.8179 - val_loss: 39.8050 - val_mae: 4.9614\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.3330 - mae: 5.1351 - val_loss: 42.3433 - val_mae: 5.0729\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.1223 - mae: 4.9662 - val_loss: 40.9001 - val_mae: 5.0273\n",
      "Epoch 00038: early stopping\n",
      "5/5 [==============================] - 0s 758us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1032.6952 - mae: 23.0589 - val_loss: 39.2350 - val_mae: 5.0594\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.9494 - mae: 6.7074 - val_loss: 34.0467 - val_mae: 4.7177\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.5258 - mae: 6.6621 - val_loss: 30.6540 - val_mae: 4.5395\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4524 - mae: 6.4893 - val_loss: 30.3720 - val_mae: 4.5603\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.6740 - mae: 6.5197 - val_loss: 27.2842 - val_mae: 4.3499\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.0057 - mae: 6.4180 - val_loss: 24.8226 - val_mae: 4.1607\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.5268 - mae: 6.2209 - val_loss: 24.3328 - val_mae: 4.1246\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.8162 - mae: 6.2437 - val_loss: 25.7593 - val_mae: 4.1821\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.0396 - mae: 6.1844 - val_loss: 28.1705 - val_mae: 4.3433\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.5803 - mae: 6.3366 - val_loss: 23.5857 - val_mae: 4.0091\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.4208 - mae: 6.1511 - val_loss: 22.3577 - val_mae: 3.9085\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.7350 - mae: 6.2013 - val_loss: 20.1996 - val_mae: 3.7798\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.2021 - mae: 6.0112 - val_loss: 24.5665 - val_mae: 4.0548\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.4779 - mae: 6.3218 - val_loss: 18.7397 - val_mae: 3.6640\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.1041 - mae: 6.0471 - val_loss: 18.0374 - val_mae: 3.6058\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.3895 - mae: 6.0948 - val_loss: 17.5092 - val_mae: 3.5494\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.3359 - mae: 5.9352 - val_loss: 18.9542 - val_mae: 3.6524\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.9443 - mae: 6.0306 - val_loss: 18.9735 - val_mae: 3.6520\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.8616 - mae: 5.9001 - val_loss: 21.8431 - val_mae: 3.8734\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.0499 - mae: 6.0859 - val_loss: 21.6808 - val_mae: 3.8645\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.1211 - mae: 6.1640 - val_loss: 17.6250 - val_mae: 3.5316\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.4662 - mae: 5.9327 - val_loss: 17.3581 - val_mae: 3.5039\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4448 - mae: 5.8764 - val_loss: 21.1661 - val_mae: 3.8325\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.0404 - mae: 6.0480 - val_loss: 18.4604 - val_mae: 3.5933\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.3047 - mae: 6.0103 - val_loss: 17.3378 - val_mae: 3.4829\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.3397 - mae: 5.8889 - val_loss: 19.5570 - val_mae: 3.6787\n",
      "Epoch 00026: early stopping\n",
      "5/5 [==============================] - 0s 724us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   1.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1346.9138 - mae: 19.2111 - val_loss: 21.8306 - val_mae: 3.9225\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.5658 - mae: 7.1108 - val_loss: 19.4772 - val_mae: 3.7201\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.3823 - mae: 6.9043 - val_loss: 24.9845 - val_mae: 4.1217\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.0430 - mae: 6.9640 - val_loss: 21.0449 - val_mae: 3.7018\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.7882 - mae: 6.8939 - val_loss: 19.6887 - val_mae: 3.5415\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.9727 - mae: 6.8542 - val_loss: 15.3861 - val_mae: 3.0563\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.6286 - mae: 6.7012 - val_loss: 16.4111 - val_mae: 3.1517\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.9380 - mae: 6.8186 - val_loss: 24.7748 - val_mae: 4.1490\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.0502 - mae: 6.5934 - val_loss: 25.7391 - val_mae: 4.2402\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.7809 - mae: 6.6863 - val_loss: 27.9376 - val_mae: 4.4206\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.9182 - mae: 6.6903 - val_loss: 15.8682 - val_mae: 3.0839\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.7267 - mae: 6.5085 - val_loss: 28.9899 - val_mae: 4.5311\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.0480 - mae: 6.5629 - val_loss: 25.1076 - val_mae: 4.1584\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6344 - mae: 6.6731 - val_loss: 15.4643 - val_mae: 3.0345\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.0944 - mae: 6.3683 - val_loss: 19.2752 - val_mae: 3.5583\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.4387 - mae: 6.4690 - val_loss: 14.2358 - val_mae: 2.8551\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4529 - mae: 6.3894 - val_loss: 25.1599 - val_mae: 4.2071\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.3624 - mae: 6.5491 - val_loss: 14.7407 - val_mae: 2.9213\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.1307 - mae: 6.2173 - val_loss: 18.1875 - val_mae: 3.4524\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.1393 - mae: 6.3431 - val_loss: 23.7576 - val_mae: 4.0848\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.5858 - mae: 6.4014 - val_loss: 12.9629 - val_mae: 2.6927\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.5315 - mae: 6.2675 - val_loss: 13.2097 - val_mae: 2.7251\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.6848 - mae: 6.0942 - val_loss: 18.5950 - val_mae: 3.5163\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.1240 - mae: 6.2610 - val_loss: 18.5187 - val_mae: 3.5183\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.0541 - mae: 6.2780 - val_loss: 15.8511 - val_mae: 3.1408\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.5910 - mae: 6.2490 - val_loss: 23.3569 - val_mae: 4.0726\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.4073 - mae: 6.2495 - val_loss: 12.4203 - val_mae: 2.6408\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.3638 - mae: 6.2827 - val_loss: 12.9006 - val_mae: 2.7061\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.6622 - mae: 6.1261 - val_loss: 14.1736 - val_mae: 2.9083\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.2429 - mae: 6.1684 - val_loss: 15.7355 - val_mae: 3.1365\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.3719 - mae: 6.2817 - val_loss: 12.5583 - val_mae: 2.7150\n",
      "Epoch 00031: early stopping\n",
      "5/5 [==============================] - 0s 881us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 629.6652 - mae: 16.0907 - val_loss: 91.8204 - val_mae: 7.7454\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.9398 - mae: 8.1620 - val_loss: 65.5522 - val_mae: 5.9170\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.5769 - mae: 7.1771 - val_loss: 54.2532 - val_mae: 5.2690\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.3570 - mae: 6.5182 - val_loss: 51.3073 - val_mae: 5.1210\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.0458 - mae: 6.0785 - val_loss: 46.7934 - val_mae: 4.9230\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.6806 - mae: 5.8246 - val_loss: 43.6169 - val_mae: 4.7478\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.0161 - mae: 5.9190 - val_loss: 49.5357 - val_mae: 5.2246\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.0590 - mae: 5.7362 - val_loss: 40.2691 - val_mae: 4.5541\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.3655 - mae: 5.3143 - val_loss: 38.9304 - val_mae: 4.5507\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.4665 - mae: 5.3529 - val_loss: 37.7039 - val_mae: 4.4798\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.6562 - mae: 5.2816 - val_loss: 37.8698 - val_mae: 4.5006\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.9626 - mae: 5.2366 - val_loss: 40.3568 - val_mae: 4.6888\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.4124 - mae: 5.2233 - val_loss: 35.9050 - val_mae: 4.4427\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.7441 - mae: 5.1913 - val_loss: 35.5165 - val_mae: 4.4363\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.6418 - mae: 5.1861 - val_loss: 35.3745 - val_mae: 4.4253\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.9123 - mae: 5.1234 - val_loss: 36.4878 - val_mae: 4.5372\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.7288 - mae: 5.2341 - val_loss: 35.4699 - val_mae: 4.4775\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.8141 - mae: 5.1948 - val_loss: 34.0719 - val_mae: 4.3902\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.2800 - mae: 5.1619 - val_loss: 34.4034 - val_mae: 4.4248\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.2447 - mae: 5.0586 - val_loss: 33.3980 - val_mae: 4.3561\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.0040 - mae: 5.2592 - val_loss: 33.7243 - val_mae: 4.3923\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.4301 - mae: 4.9821 - val_loss: 33.6698 - val_mae: 4.3849\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.7789 - mae: 5.1704 - val_loss: 33.4460 - val_mae: 4.3878\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.4282 - mae: 5.0298 - val_loss: 36.3702 - val_mae: 4.6293\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.0160 - mae: 5.1621 - val_loss: 32.7704 - val_mae: 4.3599\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.9310 - mae: 4.9699 - val_loss: 38.0910 - val_mae: 4.7744\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.6487 - mae: 5.2909 - val_loss: 32.2174 - val_mae: 4.3217\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.1453 - mae: 5.1406 - val_loss: 32.0914 - val_mae: 4.3142\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.9905 - mae: 5.0181 - val_loss: 32.4867 - val_mae: 4.3775\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.7836 - mae: 5.0106 - val_loss: 34.4163 - val_mae: 4.4707\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.4795 - mae: 5.0580 - val_loss: 32.3392 - val_mae: 4.3679\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6980 - mae: 5.0179 - val_loss: 34.8216 - val_mae: 4.5042\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.0379 - mae: 5.1157 - val_loss: 38.0171 - val_mae: 4.8074\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.8868 - mae: 5.0835 - val_loss: 31.9085 - val_mae: 4.3051\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.0779 - mae: 4.9813 - val_loss: 32.0659 - val_mae: 4.3179\n",
      "Epoch 00035: early stopping\n",
      "5/5 [==============================] - 0s 713us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 46ms/step - loss: 238.7700 - mae: 13.5416 - val_loss: 230.4940 - val_mae: 13.2582\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 229.2497 - mae: 13.2431 - val_loss: 219.7463 - val_mae: 12.8771\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 220.0440 - mae: 12.9556 - val_loss: 209.3460 - val_mae: 12.4987\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 211.3942 - mae: 12.6672 - val_loss: 199.2954 - val_mae: 12.1450\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 203.1649 - mae: 12.3924 - val_loss: 189.7637 - val_mae: 11.8097\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 195.0631 - mae: 12.1305 - val_loss: 180.7379 - val_mae: 11.4991\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 187.7904 - mae: 11.8960 - val_loss: 171.9186 - val_mae: 11.1865\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.6449 - mae: 11.6475 - val_loss: 163.6443 - val_mae: 10.8895\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 173.9558 - mae: 11.4184 - val_loss: 155.8391 - val_mae: 10.6161\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 167.5898 - mae: 11.1991 - val_loss: 148.5540 - val_mae: 10.3527\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.7793 - mae: 10.9859 - val_loss: 141.5145 - val_mae: 10.0905\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.0847 - mae: 10.7696 - val_loss: 134.9122 - val_mae: 9.8364\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.8227 - mae: 10.5679 - val_loss: 128.6225 - val_mae: 9.5868\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.7388 - mae: 10.3625 - val_loss: 122.8047 - val_mae: 9.3490\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.0805 - mae: 10.1628 - val_loss: 117.1916 - val_mae: 9.1120\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.6375 - mae: 9.9705 - val_loss: 111.8352 - val_mae: 8.8787\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.5690 - mae: 9.7930 - val_loss: 106.6439 - val_mae: 8.6444\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.6829 - mae: 9.6134 - val_loss: 101.8360 - val_mae: 8.4207\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.1700 - mae: 9.4486 - val_loss: 97.3025 - val_mae: 8.2019\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.6582 - mae: 9.2865 - val_loss: 93.2080 - val_mae: 7.9982\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.4826 - mae: 9.1357 - val_loss: 89.4138 - val_mae: 7.8030\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.6791 - mae: 9.0057 - val_loss: 85.6919 - val_mae: 7.6058\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.0784 - mae: 8.8646 - val_loss: 82.0713 - val_mae: 7.4086\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.4847 - mae: 8.7306 - val_loss: 78.7935 - val_mae: 7.2376\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.0572 - mae: 8.6070 - val_loss: 75.8655 - val_mae: 7.0798\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.0444 - mae: 8.4963 - val_loss: 73.0633 - val_mae: 6.9241\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.1337 - mae: 8.3875 - val_loss: 70.3865 - val_mae: 6.7705\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.2235 - mae: 8.2773 - val_loss: 67.9111 - val_mae: 6.6236\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.6344 - mae: 8.1803 - val_loss: 65.4997 - val_mae: 6.4755\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.9115 - mae: 8.0813 - val_loss: 63.4940 - val_mae: 6.3490\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.5581 - mae: 7.9935 - val_loss: 61.4458 - val_mae: 6.2154\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.2859 - mae: 7.9118 - val_loss: 59.4331 - val_mae: 6.0808\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.0181 - mae: 7.8260 - val_loss: 57.6712 - val_mae: 5.9723\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.9234 - mae: 7.7509 - val_loss: 56.0620 - val_mae: 5.8727\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.9527 - mae: 7.6775 - val_loss: 54.5667 - val_mae: 5.7769\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.9799 - mae: 7.6121 - val_loss: 53.2075 - val_mae: 5.6876\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.2143 - mae: 7.5506 - val_loss: 51.8186 - val_mae: 5.5933\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.3512 - mae: 7.4851 - val_loss: 50.5895 - val_mae: 5.5070\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 89.6857 - mae: 7.4323 - val_loss: 49.3503 - val_mae: 5.4288\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.0539 - mae: 7.3787 - val_loss: 48.2378 - val_mae: 5.3627\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.3603 - mae: 7.3323 - val_loss: 47.3675 - val_mae: 5.3091\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.8715 - mae: 7.2918 - val_loss: 46.5089 - val_mae: 5.2563\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.3911 - mae: 7.2541 - val_loss: 45.7159 - val_mae: 5.2105\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.9676 - mae: 7.2177 - val_loss: 44.9214 - val_mae: 5.1635\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.5475 - mae: 7.1828 - val_loss: 44.2144 - val_mae: 5.1202\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.2020 - mae: 7.1486 - val_loss: 43.5177 - val_mae: 5.0764\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.8959 - mae: 7.1191 - val_loss: 42.8332 - val_mae: 5.0325\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.5454 - mae: 7.0867 - val_loss: 42.2867 - val_mae: 4.9960\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.2998 - mae: 7.0584 - val_loss: 41.7492 - val_mae: 4.9592\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.0393 - mae: 7.0334 - val_loss: 41.3221 - val_mae: 4.9293\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.8294 - mae: 7.0109 - val_loss: 40.8811 - val_mae: 4.9015\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.6205 - mae: 6.9908 - val_loss: 40.4734 - val_mae: 4.8762\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4356 - mae: 6.9698 - val_loss: 40.0873 - val_mae: 4.8520\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.2299 - mae: 6.9524 - val_loss: 39.8227 - val_mae: 4.8347\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0869 - mae: 6.9370 - val_loss: 39.4384 - val_mae: 4.8097\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.9134 - mae: 6.9182 - val_loss: 39.0852 - val_mae: 4.7864\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.7832 - mae: 6.9022 - val_loss: 38.7299 - val_mae: 4.7623\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.6195 - mae: 6.8852 - val_loss: 38.4422 - val_mae: 4.7421\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.4639 - mae: 6.8717 - val_loss: 38.2265 - val_mae: 4.7268\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.3535 - mae: 6.8584 - val_loss: 37.9217 - val_mae: 4.7053\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.2324 - mae: 6.8458 - val_loss: 37.6516 - val_mae: 4.6856\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.0845 - mae: 6.8324 - val_loss: 37.4756 - val_mae: 4.6724\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.9847 - mae: 6.8223 - val_loss: 37.2536 - val_mae: 4.6557\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 82.8781 - mae: 6.8114 - val_loss: 37.0263 - val_mae: 4.6386\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.8008 - mae: 6.8000 - val_loss: 36.7938 - val_mae: 4.6208\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.6824 - mae: 6.7896 - val_loss: 36.6076 - val_mae: 4.6061\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.5834 - mae: 6.7794 - val_loss: 36.4230 - val_mae: 4.5949\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.4988 - mae: 6.7710 - val_loss: 36.2818 - val_mae: 4.5865\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.4177 - mae: 6.7646 - val_loss: 36.1907 - val_mae: 4.5807\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.3295 - mae: 6.7565 - val_loss: 36.0504 - val_mae: 4.5721\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 82.2419 - mae: 6.7498 - val_loss: 35.9507 - val_mae: 4.5658\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.1746 - mae: 6.7413 - val_loss: 35.7320 - val_mae: 4.5529\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.0749 - mae: 6.7317 - val_loss: 35.6231 - val_mae: 4.5461\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.0120 - mae: 6.7242 - val_loss: 35.4591 - val_mae: 4.5360\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.9285 - mae: 6.7181 - val_loss: 35.3909 - val_mae: 4.5313\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.8604 - mae: 6.7120 - val_loss: 35.2600 - val_mae: 4.5232\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.7823 - mae: 6.7067 - val_loss: 35.1703 - val_mae: 4.5172\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.6979 - mae: 6.7022 - val_loss: 35.1131 - val_mae: 4.5130\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.6293 - mae: 6.6969 - val_loss: 35.0145 - val_mae: 4.5065\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.5596 - mae: 6.6919 - val_loss: 34.9022 - val_mae: 4.4993\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.5015 - mae: 6.6886 - val_loss: 34.9005 - val_mae: 4.4981\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.4186 - mae: 6.6831 - val_loss: 34.7468 - val_mae: 4.4884\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.3409 - mae: 6.6773 - val_loss: 34.6925 - val_mae: 4.4842\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.2832 - mae: 6.6730 - val_loss: 34.6253 - val_mae: 4.4791\n",
      "Epoch 00084: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 11393.6035 - mae: 105.6209 - val_loss: 10632.5850 - val_mae: 102.3796\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11309.7969 - mae: 105.2279 - val_loss: 10550.8057 - val_mae: 101.9835\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11226.7617 - mae: 104.8366 - val_loss: 10469.3672 - val_mae: 101.5875\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11144.1445 - mae: 104.4454 - val_loss: 10388.5020 - val_mae: 101.1928\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11061.4619 - mae: 104.0547 - val_loss: 10308.4258 - val_mae: 100.8003\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10979.8389 - mae: 103.6652 - val_loss: 10228.8828 - val_mae: 100.4089\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10898.4932 - mae: 103.2780 - val_loss: 10149.7539 - val_mae: 100.0180\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10817.5811 - mae: 102.8924 - val_loss: 10071.0498 - val_mae: 99.6276\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10737.2871 - mae: 102.5044 - val_loss: 9992.7891 - val_mae: 99.2380\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10657.9062 - mae: 102.1194 - val_loss: 9914.6270 - val_mae: 98.8473\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10578.2266 - mae: 101.7335 - val_loss: 9837.1309 - val_mae: 98.4583\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10499.0918 - mae: 101.3492 - val_loss: 9760.6016 - val_mae: 98.0727\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10420.7363 - mae: 100.9665 - val_loss: 9684.2988 - val_mae: 97.6866\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10343.4521 - mae: 100.5863 - val_loss: 9608.0225 - val_mae: 97.2991\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10265.5605 - mae: 100.2034 - val_loss: 9532.7520 - val_mae: 96.9152\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10188.3750 - mae: 99.8216 - val_loss: 9458.0488 - val_mae: 96.5327\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10112.2090 - mae: 99.4440 - val_loss: 9383.5254 - val_mae: 96.1495\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10036.5723 - mae: 99.0650 - val_loss: 9309.1416 - val_mae: 95.7655\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9960.3447 - mae: 98.6867 - val_loss: 9235.9756 - val_mae: 95.3861\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9885.2852 - mae: 98.3100 - val_loss: 9163.0967 - val_mae: 95.0066\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9811.2393 - mae: 97.9345 - val_loss: 9089.9795 - val_mae: 94.6244\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9736.8535 - mae: 97.5580 - val_loss: 9017.4609 - val_mae: 94.2439\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9662.8740 - mae: 97.1824 - val_loss: 8945.5615 - val_mae: 93.8650\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9589.5342 - mae: 96.8086 - val_loss: 8874.2705 - val_mae: 93.4877\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9517.2461 - mae: 96.4367 - val_loss: 8803.1641 - val_mae: 93.1098\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9444.5781 - mae: 96.0639 - val_loss: 8732.6748 - val_mae: 92.7337\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9372.4580 - mae: 95.6923 - val_loss: 8662.7920 - val_mae: 92.3592\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9301.4609 - mae: 95.3250 - val_loss: 8593.2070 - val_mae: 91.9847\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9230.7461 - mae: 94.9553 - val_loss: 8524.1465 - val_mae: 91.6117\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9160.4883 - mae: 94.5878 - val_loss: 8455.5283 - val_mae: 91.2394\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9090.0312 - mae: 94.2197 - val_loss: 8387.8018 - val_mae: 90.8704\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9020.7939 - mae: 93.8549 - val_loss: 8320.3701 - val_mae: 90.5016\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8951.9854 - mae: 93.4902 - val_loss: 8253.1553 - val_mae: 90.1325\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8882.9033 - mae: 93.1249 - val_loss: 8186.7573 - val_mae: 89.7663\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8815.0137 - mae: 92.7642 - val_loss: 8120.4355 - val_mae: 89.3988\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8747.1250 - mae: 92.3992 - val_loss: 8054.6431 - val_mae: 89.0329\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8679.8770 - mae: 92.0380 - val_loss: 7988.8604 - val_mae: 88.6656\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8612.6836 - mae: 91.6758 - val_loss: 7923.8135 - val_mae: 88.3007\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8546.5967 - mae: 91.3161 - val_loss: 7858.9648 - val_mae: 87.9353\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8479.8184 - mae: 90.9558 - val_loss: 7794.8979 - val_mae: 87.5728\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8413.9609 - mae: 90.5966 - val_loss: 7731.2915 - val_mae: 87.2114\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8348.9463 - mae: 90.2397 - val_loss: 7667.7710 - val_mae: 86.8492\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8283.7158 - mae: 89.8823 - val_loss: 7604.7471 - val_mae: 86.4882\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8219.5127 - mae: 89.5272 - val_loss: 7541.7476 - val_mae: 86.1258\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8155.0063 - mae: 89.1678 - val_loss: 7479.4502 - val_mae: 85.7661\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8091.4595 - mae: 88.8144 - val_loss: 7417.4072 - val_mae: 85.4062\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8028.0991 - mae: 88.4594 - val_loss: 7355.8369 - val_mae: 85.0476\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7965.0195 - mae: 88.1064 - val_loss: 7294.8760 - val_mae: 84.6908\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7902.5747 - mae: 87.7547 - val_loss: 7234.2344 - val_mae: 84.3344\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7840.1958 - mae: 87.4021 - val_loss: 7173.9009 - val_mae: 83.9781\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7778.2290 - mae: 87.0495 - val_loss: 7113.8882 - val_mae: 83.6223\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7716.8325 - mae: 86.6981 - val_loss: 7053.9961 - val_mae: 83.2656\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7655.2085 - mae: 86.3464 - val_loss: 6994.8638 - val_mae: 82.9119\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7594.4502 - mae: 85.9962 - val_loss: 6935.9321 - val_mae: 82.5579\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7534.1685 - mae: 85.6475 - val_loss: 6877.3208 - val_mae: 82.2042\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7474.3462 - mae: 85.2998 - val_loss: 6818.9839 - val_mae: 81.8507\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7414.3384 - mae: 84.9513 - val_loss: 6761.2866 - val_mae: 81.4996\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7355.3750 - mae: 84.6051 - val_loss: 6703.6914 - val_mae: 81.1476\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7295.8691 - mae: 84.2576 - val_loss: 6646.6538 - val_mae: 80.7974\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7237.6709 - mae: 83.9118 - val_loss: 6589.6309 - val_mae: 80.4458\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7178.9502 - mae: 83.5644 - val_loss: 6533.3970 - val_mae: 80.0974\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7121.2056 - mae: 83.2213 - val_loss: 6477.6006 - val_mae: 79.7501\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7063.5513 - mae: 82.8778 - val_loss: 6422.2466 - val_mae: 79.4041\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7006.5166 - mae: 82.5349 - val_loss: 6366.9907 - val_mae: 79.0572\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6949.7251 - mae: 82.1932 - val_loss: 6311.8623 - val_mae: 78.7095\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6893.3188 - mae: 81.8522 - val_loss: 6257.1865 - val_mae: 78.3630\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6837.0151 - mae: 81.5085 - val_loss: 6202.9131 - val_mae: 78.0176\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6781.5073 - mae: 81.1697 - val_loss: 6148.7788 - val_mae: 77.6714\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6725.9487 - mae: 80.8280 - val_loss: 6095.0352 - val_mae: 77.3263\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6670.3940 - mae: 80.4883 - val_loss: 6041.9429 - val_mae: 76.9837\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6615.9834 - mae: 80.1499 - val_loss: 5989.0239 - val_mae: 76.6409\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6561.0986 - mae: 79.8120 - val_loss: 5936.7446 - val_mae: 76.3007\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6507.0547 - mae: 79.4743 - val_loss: 5884.7690 - val_mae: 75.9612\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6453.0601 - mae: 79.1374 - val_loss: 5833.1240 - val_mae: 75.6221\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6400.0376 - mae: 78.8018 - val_loss: 5781.2271 - val_mae: 75.2799\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6346.2515 - mae: 78.4652 - val_loss: 5730.1807 - val_mae: 74.9415\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6293.9399 - mae: 78.1315 - val_loss: 5679.3032 - val_mae: 74.6027\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6241.4390 - mae: 77.7943 - val_loss: 5628.6958 - val_mae: 74.2644\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6189.3374 - mae: 77.4621 - val_loss: 5578.3022 - val_mae: 73.9257\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6137.5962 - mae: 77.1278 - val_loss: 5528.1772 - val_mae: 73.5873\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6085.7446 - mae: 76.7946 - val_loss: 5478.7002 - val_mae: 73.2516\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6034.4048 - mae: 76.4633 - val_loss: 5429.8433 - val_mae: 72.9187\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5983.9014 - mae: 76.1334 - val_loss: 5381.0801 - val_mae: 72.5848\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5933.3877 - mae: 75.8025 - val_loss: 5332.8374 - val_mae: 72.2533\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5883.3979 - mae: 75.4737 - val_loss: 5284.6489 - val_mae: 71.9206\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5833.7583 - mae: 75.1457 - val_loss: 5236.8022 - val_mae: 71.5886\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5783.7378 - mae: 74.8161 - val_loss: 5189.4946 - val_mae: 71.2588\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5734.5293 - mae: 74.4884 - val_loss: 5142.2837 - val_mae: 70.9282\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5685.5840 - mae: 74.1611 - val_loss: 5095.3867 - val_mae: 70.5981\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5637.0825 - mae: 73.8331 - val_loss: 5048.7573 - val_mae: 70.2685\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5589.0405 - mae: 73.5084 - val_loss: 5002.2876 - val_mae: 69.9384\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5540.2998 - mae: 73.1793 - val_loss: 4956.8081 - val_mae: 69.6140\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5492.7236 - mae: 72.8582 - val_loss: 4911.4688 - val_mae: 69.2889\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5446.0229 - mae: 72.5356 - val_loss: 4865.8530 - val_mae: 68.9603\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5398.9058 - mae: 72.2102 - val_loss: 4820.8901 - val_mae: 68.6348\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5352.0547 - mae: 71.8894 - val_loss: 4776.4883 - val_mae: 68.3117\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5305.6816 - mae: 71.5676 - val_loss: 4732.3730 - val_mae: 67.9892\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5260.3013 - mae: 71.2476 - val_loss: 4687.9321 - val_mae: 67.6628\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5213.8770 - mae: 70.9249 - val_loss: 4644.3442 - val_mae: 67.3410\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5168.5659 - mae: 70.6074 - val_loss: 4601.1133 - val_mae: 67.0202\n",
      "5/5 [==============================] - 0s 891us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   3.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 708.7521 - mae: 23.9820 - val_loss: 736.0485 - val_mae: 23.9989\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 692.4984 - mae: 23.6818 - val_loss: 719.2354 - val_mae: 23.6895\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 676.8661 - mae: 23.3875 - val_loss: 702.5790 - val_mae: 23.3793\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 661.3938 - mae: 23.0923 - val_loss: 686.2374 - val_mae: 23.0706\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 646.0323 - mae: 22.7944 - val_loss: 670.2413 - val_mae: 22.7647\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.9171 - mae: 22.5066 - val_loss: 654.6382 - val_mae: 22.4618\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 616.3938 - mae: 22.2166 - val_loss: 639.2728 - val_mae: 22.1596\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.9102 - mae: 21.9302 - val_loss: 624.4149 - val_mae: 21.8635\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 588.0426 - mae: 21.6502 - val_loss: 609.7203 - val_mae: 21.5662\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 574.3414 - mae: 21.3677 - val_loss: 595.2710 - val_mae: 21.2702\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 560.8716 - mae: 21.0896 - val_loss: 581.1418 - val_mae: 20.9764\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 547.6607 - mae: 20.8070 - val_loss: 567.4313 - val_mae: 20.6873\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 535.0623 - mae: 20.5326 - val_loss: 553.7521 - val_mae: 20.4117\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 522.2654 - mae: 20.2583 - val_loss: 540.5841 - val_mae: 20.1437\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 509.8322 - mae: 19.9911 - val_loss: 527.8405 - val_mae: 19.8804\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 497.8278 - mae: 19.7295 - val_loss: 515.4191 - val_mae: 19.6199\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 485.9599 - mae: 19.4696 - val_loss: 503.3583 - val_mae: 19.3636\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 474.8009 - mae: 19.2137 - val_loss: 491.1865 - val_mae: 19.1012\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 463.6315 - mae: 18.9592 - val_loss: 479.2393 - val_mae: 18.8399\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 452.4037 - mae: 18.7056 - val_loss: 467.7419 - val_mae: 18.5848\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 441.6353 - mae: 18.4556 - val_loss: 456.5016 - val_mae: 18.3312\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 431.2533 - mae: 18.2101 - val_loss: 445.4633 - val_mae: 18.0786\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 420.9922 - mae: 17.9729 - val_loss: 434.7502 - val_mae: 17.8295\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 410.9903 - mae: 17.7358 - val_loss: 424.3518 - val_mae: 17.5841\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 401.1857 - mae: 17.5046 - val_loss: 414.3546 - val_mae: 17.3444\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 391.8062 - mae: 17.2779 - val_loss: 404.5441 - val_mae: 17.1057\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 382.6370 - mae: 17.0541 - val_loss: 394.8920 - val_mae: 16.8683\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 373.7646 - mae: 16.8365 - val_loss: 385.3415 - val_mae: 16.6521\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 364.8362 - mae: 16.6171 - val_loss: 376.2199 - val_mae: 16.4520\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 356.1934 - mae: 16.4044 - val_loss: 367.3353 - val_mae: 16.2543\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 347.9651 - mae: 16.1924 - val_loss: 358.5347 - val_mae: 16.0554\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 340.0030 - mae: 15.9820 - val_loss: 349.8541 - val_mae: 15.8561\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 331.7687 - mae: 15.7744 - val_loss: 341.6050 - val_mae: 15.6638\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 324.0392 - mae: 15.5656 - val_loss: 333.4164 - val_mae: 15.4702\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 316.5542 - mae: 15.3678 - val_loss: 325.3526 - val_mae: 15.2763\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 308.9716 - mae: 15.1673 - val_loss: 317.5947 - val_mae: 15.0867\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 301.5670 - mae: 14.9679 - val_loss: 310.0155 - val_mae: 14.8988\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 241.8670 - mae: 13.65 - 0s 3ms/step - loss: 294.6153 - mae: 14.7804 - val_loss: 302.3987 - val_mae: 14.7069\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 287.7581 - mae: 14.5887 - val_loss: 294.8787 - val_mae: 14.5144\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 280.9568 - mae: 14.3975 - val_loss: 287.6393 - val_mae: 14.3261\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 274.2087 - mae: 14.2123 - val_loss: 280.7653 - val_mae: 14.1438\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 267.8867 - mae: 14.0362 - val_loss: 273.9838 - val_mae: 13.9609\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 261.6634 - mae: 13.8569 - val_loss: 267.4802 - val_mae: 13.7827\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 255.5896 - mae: 13.6851 - val_loss: 261.1435 - val_mae: 13.6060\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 287.9381 - mae: 14.65 - 0s 3ms/step - loss: 249.7863 - mae: 13.5130 - val_loss: 254.8810 - val_mae: 13.4283\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 243.9208 - mae: 13.3417 - val_loss: 248.9270 - val_mae: 13.2562\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 238.3228 - mae: 13.1729 - val_loss: 243.1166 - val_mae: 13.0856\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 233.0584 - mae: 13.0103 - val_loss: 237.2640 - val_mae: 12.9108\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 227.7051 - mae: 12.8396 - val_loss: 231.6095 - val_mae: 12.7389\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 222.4030 - mae: 12.6755 - val_loss: 226.2830 - val_mae: 12.5739\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 217.4304 - mae: 12.5160 - val_loss: 221.0889 - val_mae: 12.4101\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 212.6008 - mae: 12.3612 - val_loss: 215.9714 - val_mae: 12.2460\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 208.0188 - mae: 12.2073 - val_loss: 210.9147 - val_mae: 12.0808\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 203.3509 - mae: 12.0542 - val_loss: 206.1631 - val_mae: 11.9230\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 198.9597 - mae: 11.9038 - val_loss: 201.5867 - val_mae: 11.7679\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 194.6537 - mae: 11.7573 - val_loss: 197.1169 - val_mae: 11.6138\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.5027 - mae: 11.6128 - val_loss: 192.7143 - val_mae: 11.4589\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 186.4115 - mae: 11.4727 - val_loss: 188.5081 - val_mae: 11.3092\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 182.5679 - mae: 11.3365 - val_loss: 184.4320 - val_mae: 11.1721\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.7110 - mae: 11.2034 - val_loss: 180.5378 - val_mae: 11.0390\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.1499 - mae: 11.0758 - val_loss: 176.6607 - val_mae: 10.9042\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.4185 - mae: 10.9474 - val_loss: 173.0679 - val_mae: 10.7765\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 168.0046 - mae: 10.8259 - val_loss: 169.5311 - val_mae: 10.6521\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.8179 - mae: 10.7066 - val_loss: 165.9047 - val_mae: 10.5296\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.3120 - mae: 10.5805 - val_loss: 162.5901 - val_mae: 10.4157\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 158.2719 - mae: 10.4663 - val_loss: 159.2753 - val_mae: 10.2994\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.2061 - mae: 10.3541 - val_loss: 156.1085 - val_mae: 10.1861\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.2179 - mae: 10.2385 - val_loss: 153.0852 - val_mae: 10.0758\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.4146 - mae: 10.1306 - val_loss: 150.0623 - val_mae: 9.9632\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.6484 - mae: 10.0193 - val_loss: 147.2036 - val_mae: 9.8549\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.0403 - mae: 9.9111 - val_loss: 144.3889 - val_mae: 9.7460\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.3605 - mae: 9.8051 - val_loss: 141.7743 - val_mae: 9.6427\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.9936 - mae: 9.7044 - val_loss: 139.1428 - val_mae: 9.5367\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.4665 - mae: 9.5995 - val_loss: 136.6672 - val_mae: 9.4356\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.1800 - mae: 9.5023 - val_loss: 134.2217 - val_mae: 9.3335\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.8707 - mae: 9.4056 - val_loss: 131.8995 - val_mae: 9.2344\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.6767 - mae: 9.3084 - val_loss: 129.6864 - val_mae: 9.1385\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 127.6122 - mae: 9.2153 - val_loss: 127.5227 - val_mae: 9.0429\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.5273 - mae: 9.1244 - val_loss: 125.4975 - val_mae: 8.9521\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.6802 - mae: 9.0374 - val_loss: 123.3771 - val_mae: 8.8548\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.7877 - mae: 8.9451 - val_loss: 121.3093 - val_mae: 8.7581\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.8357 - mae: 8.8555 - val_loss: 119.4678 - val_mae: 8.6702\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.1849 - mae: 8.7736 - val_loss: 117.6444 - val_mae: 8.5875\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.5155 - mae: 8.6888 - val_loss: 115.9207 - val_mae: 8.5089\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.8148 - mae: 8.6076 - val_loss: 114.3206 - val_mae: 8.4346\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.3123 - mae: 8.5305 - val_loss: 112.6826 - val_mae: 8.3570\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.8304 - mae: 8.4528 - val_loss: 111.0273 - val_mae: 8.2771\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.2952 - mae: 8.3734 - val_loss: 109.4795 - val_mae: 8.2007\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.8819 - mae: 8.2974 - val_loss: 107.9690 - val_mae: 8.1250\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.4853 - mae: 8.2230 - val_loss: 106.5253 - val_mae: 8.0526\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.1837 - mae: 8.1512 - val_loss: 105.1261 - val_mae: 7.9883\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.9058 - mae: 8.0801 - val_loss: 103.8028 - val_mae: 7.9264\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.6714 - mae: 8.0107 - val_loss: 102.5734 - val_mae: 7.8683\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.4663 - mae: 7.9472 - val_loss: 101.4048 - val_mae: 7.8119\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.3914 - mae: 7.8829 - val_loss: 100.1617 - val_mae: 7.7506\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.2344 - mae: 7.8165 - val_loss: 99.0041 - val_mae: 7.6927\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.1881 - mae: 7.7555 - val_loss: 97.8672 - val_mae: 7.6345\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.0940 - mae: 7.6927 - val_loss: 96.8169 - val_mae: 7.5800\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.1559 - mae: 7.6376 - val_loss: 95.7563 - val_mae: 7.5239\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.1357 - mae: 7.5805 - val_loss: 94.7971 - val_mae: 7.4721\n",
      "5/5 [==============================] - 0s 838us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   3.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 540.3402 - mae: 16.7292 - val_loss: 125.8078 - val_mae: 9.7388\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.4961 - mae: 10.1685 - val_loss: 84.7310 - val_mae: 7.7209\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.6849 - mae: 8.6918 - val_loss: 63.5891 - val_mae: 6.5790\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.1277 - mae: 7.9074 - val_loss: 50.9589 - val_mae: 5.8475\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.3191 - mae: 7.4178 - val_loss: 41.4451 - val_mae: 5.2618\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.0263 - mae: 6.9856 - val_loss: 36.0071 - val_mae: 4.8844\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.1330 - mae: 6.7703 - val_loss: 33.2131 - val_mae: 4.6966\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.7460 - mae: 6.5683 - val_loss: 41.5330 - val_mae: 5.4820\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.5328 - mae: 6.5004 - val_loss: 38.0761 - val_mae: 5.2148\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.6413 - mae: 6.4934 - val_loss: 31.7600 - val_mae: 4.7397\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1115 - mae: 6.3414 - val_loss: 28.1136 - val_mae: 4.4217\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.5365 - mae: 6.2157 - val_loss: 27.3006 - val_mae: 4.3479\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.8661 - mae: 6.0942 - val_loss: 35.3377 - val_mae: 4.9842\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.0174 - mae: 6.3372 - val_loss: 25.8689 - val_mae: 4.2115\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.8310 - mae: 6.1145 - val_loss: 25.4557 - val_mae: 4.1794\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.5156 - mae: 6.0792 - val_loss: 23.9498 - val_mae: 4.0084\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.8384 - mae: 5.9736 - val_loss: 25.8046 - val_mae: 4.2174\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.6084 - mae: 5.9979 - val_loss: 22.6390 - val_mae: 3.8894\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.9961 - mae: 5.9829 - val_loss: 25.4901 - val_mae: 4.2055\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.7048 - mae: 5.9191 - val_loss: 29.7517 - val_mae: 4.6275\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.7366 - mae: 6.1237 - val_loss: 23.2332 - val_mae: 3.9840\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.0233 - mae: 5.9424 - val_loss: 21.0923 - val_mae: 3.7628\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.5784 - mae: 5.8331 - val_loss: 25.8388 - val_mae: 4.2912\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.7657 - mae: 5.9154 - val_loss: 22.8219 - val_mae: 3.9802\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.7504 - mae: 5.8894 - val_loss: 20.2955 - val_mae: 3.6919\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1868 - mae: 5.8224 - val_loss: 25.9524 - val_mae: 4.3354\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.8053 - mae: 5.9089 - val_loss: 22.7114 - val_mae: 3.9859\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.8312 - mae: 5.9459 - val_loss: 20.6266 - val_mae: 3.7475\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.2275 - mae: 5.7778 - val_loss: 22.8756 - val_mae: 4.0318\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.0124 - mae: 5.8580 - val_loss: 18.6675 - val_mae: 3.5310\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.8054 - mae: 5.8026 - val_loss: 17.9156 - val_mae: 3.4666\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.1039 - mae: 5.7013 - val_loss: 22.7664 - val_mae: 4.0560\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.4705 - mae: 5.8937 - val_loss: 20.8323 - val_mae: 3.8242\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.9062 - mae: 5.7590 - val_loss: 23.1060 - val_mae: 4.1104\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.7982 - mae: 5.8319 - val_loss: 20.9290 - val_mae: 3.8579\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.2923 - mae: 5.8227 - val_loss: 20.0870 - val_mae: 3.7497\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.8962 - mae: 5.8237 - val_loss: 18.0435 - val_mae: 3.5212\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.2040 - mae: 5.7656 - val_loss: 17.1703 - val_mae: 3.4225\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.6457 - mae: 5.7541 - val_loss: 17.6407 - val_mae: 3.4900\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.6667 - mae: 5.7152 - val_loss: 22.5694 - val_mae: 4.0749\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.3546 - mae: 5.8685 - val_loss: 17.0804 - val_mae: 3.4305\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.3400 - mae: 5.7300 - val_loss: 17.7447 - val_mae: 3.5195\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.0475 - mae: 5.6916 - val_loss: 18.1565 - val_mae: 3.5670\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.6308 - mae: 5.7727 - val_loss: 16.6789 - val_mae: 3.3971\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.8346 - mae: 5.7921 - val_loss: 16.1367 - val_mae: 3.3296\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.5407 - mae: 5.6840 - val_loss: 15.9673 - val_mae: 3.3127\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.0331 - mae: 5.6923 - val_loss: 17.7128 - val_mae: 3.5358\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.3397 - mae: 5.7317 - val_loss: 19.6846 - val_mae: 3.7929\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.0836 - mae: 5.6810 - val_loss: 24.8421 - val_mae: 4.3008\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.3763 - mae: 5.8454 - val_loss: 17.3941 - val_mae: 3.4844\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.7887 - mae: 5.6133 - val_loss: 19.9567 - val_mae: 3.8125\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.0952 - mae: 5.8138 - val_loss: 16.2482 - val_mae: 3.3467\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.4690 - mae: 5.6900 - val_loss: 20.3661 - val_mae: 3.8628\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.9799 - mae: 5.8425 - val_loss: 14.9772 - val_mae: 3.1900\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.7117 - mae: 5.5761 - val_loss: 19.7113 - val_mae: 3.8036\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8225 - mae: 5.7178 - val_loss: 16.9413 - val_mae: 3.4552\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.1700 - mae: 5.6439 - val_loss: 16.5385 - val_mae: 3.4086\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8150 - mae: 5.6137 - val_loss: 21.6004 - val_mae: 3.9955\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8880 - mae: 5.8125 - val_loss: 14.4476 - val_mae: 3.1118\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.5590 - mae: 5.6419 - val_loss: 16.8842 - val_mae: 3.4618\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.9305 - mae: 5.6926 - val_loss: 16.7048 - val_mae: 3.4298\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.4815 - mae: 5.7155 - val_loss: 14.3686 - val_mae: 3.1096\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.7443 - mae: 5.6362 - val_loss: 15.5910 - val_mae: 3.2770\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.4635 - mae: 5.6359 - val_loss: 18.7866 - val_mae: 3.7096\n",
      "Epoch 00064: early stopping\n",
      "5/5 [==============================] - 0s 667us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1397.5208 - mae: 26.9878 - val_loss: 415.7070 - val_mae: 15.3049\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 398.1084 - mae: 15.1057 - val_loss: 133.1813 - val_mae: 8.7907\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 206.0617 - mae: 10.5365 - val_loss: 66.1295 - val_mae: 6.2607\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.6284 - mae: 8.4626 - val_loss: 43.2398 - val_mae: 5.5552\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.0997 - mae: 7.7359 - val_loss: 27.5877 - val_mae: 4.6218\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.2591 - mae: 7.3581 - val_loss: 21.7824 - val_mae: 3.9744\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.3893 - mae: 7.1236 - val_loss: 25.0811 - val_mae: 4.2269\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.8505 - mae: 7.3794 - val_loss: 29.7583 - val_mae: 4.6043\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.1101 - mae: 6.9668 - val_loss: 44.0478 - val_mae: 5.7051\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.6927 - mae: 7.0994 - val_loss: 47.9466 - val_mae: 5.9521\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.5417 - mae: 7.2402 - val_loss: 29.3969 - val_mae: 4.5378\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.3699 - mae: 7.0007 - val_loss: 47.0317 - val_mae: 5.8689\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.4862 - mae: 7.1740 - val_loss: 50.6848 - val_mae: 6.1286\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6874 - mae: 7.2785 - val_loss: 26.8007 - val_mae: 4.4744\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.8223 - mae: 7.0263 - val_loss: 28.8450 - val_mae: 4.5160\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.9473 - mae: 7.0946 - val_loss: 30.6895 - val_mae: 4.6222\n",
      "Epoch 00016: early stopping\n",
      "5/5 [==============================] - 0s 677us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 7234.5972 - mae: 39.8857 - val_loss: 167.2518 - val_mae: 10.3627\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 196.2828 - mae: 11.3883 - val_loss: 156.1476 - val_mae: 10.0873\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 185.4163 - mae: 11.0188 - val_loss: 156.4729 - val_mae: 10.1170\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.2065 - mae: 10.7964 - val_loss: 140.0798 - val_mae: 9.7256\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 169.2314 - mae: 10.6538 - val_loss: 131.9421 - val_mae: 9.4126\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.0134 - mae: 10.1190 - val_loss: 145.0697 - val_mae: 9.8787\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.0385 - mae: 9.9466 - val_loss: 136.0086 - val_mae: 9.5935\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 148.6430 - mae: 9.8063 - val_loss: 122.9995 - val_mae: 8.9270\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.9403 - mae: 9.3834 - val_loss: 129.5137 - val_mae: 9.3830\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.3486 - mae: 9.3847 - val_loss: 123.7955 - val_mae: 9.1550\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.7932 - mae: 9.1508 - val_loss: 112.6299 - val_mae: 8.5413\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.3880 - mae: 8.9364 - val_loss: 110.3573 - val_mae: 8.3876\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.7176 - mae: 8.8626 - val_loss: 110.3136 - val_mae: 8.5437\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.2524 - mae: 8.7991 - val_loss: 107.4565 - val_mae: 8.1430\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.5505 - mae: 8.6331 - val_loss: 105.8241 - val_mae: 8.3007\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.4626 - mae: 8.4930 - val_loss: 104.5854 - val_mae: 8.2480\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.2477 - mae: 8.4022 - val_loss: 101.4104 - val_mae: 8.0434\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.0644 - mae: 8.5277 - val_loss: 98.8004 - val_mae: 7.8537\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.8453 - mae: 8.2307 - val_loss: 100.8749 - val_mae: 7.7325\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.3223 - mae: 8.0470 - val_loss: 95.9528 - val_mae: 7.6087\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.9543 - mae: 8.2155 - val_loss: 93.9443 - val_mae: 7.6490\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.1569 - mae: 7.8465 - val_loss: 94.7682 - val_mae: 7.8157\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.6049 - mae: 8.0319 - val_loss: 99.4087 - val_mae: 7.5853\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.3398 - mae: 7.7700 - val_loss: 101.4061 - val_mae: 8.2415\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.4676 - mae: 7.7972 - val_loss: 87.3822 - val_mae: 7.3089\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.5661 - mae: 7.6314 - val_loss: 85.9439 - val_mae: 7.1593\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.8442 - mae: 7.7021 - val_loss: 87.4059 - val_mae: 7.1185\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.0753 - mae: 7.5979 - val_loss: 86.1520 - val_mae: 7.3884\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.2210 - mae: 7.4859 - val_loss: 81.7589 - val_mae: 6.9874\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.8982 - mae: 7.3808 - val_loss: 80.7505 - val_mae: 6.9338\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.6427 - mae: 7.3737 - val_loss: 79.3026 - val_mae: 6.8535\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.5704 - mae: 7.3441 - val_loss: 79.3053 - val_mae: 6.9094\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.4493 - mae: 7.4881 - val_loss: 82.4284 - val_mae: 6.8532\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.2672 - mae: 7.2953 - val_loss: 78.9087 - val_mae: 6.9952\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.1753 - mae: 7.2076 - val_loss: 76.2571 - val_mae: 6.6258\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.4506 - mae: 7.0259 - val_loss: 74.8466 - val_mae: 6.6591\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.8338 - mae: 7.1896 - val_loss: 78.6241 - val_mae: 6.6419\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.3571 - mae: 7.0313 - val_loss: 78.8919 - val_mae: 6.6290\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.7969 - mae: 7.0439 - val_loss: 73.1442 - val_mae: 6.6190\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.9702 - mae: 7.0507 - val_loss: 71.0641 - val_mae: 6.4088\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.4404 - mae: 6.8692 - val_loss: 69.8021 - val_mae: 6.3685\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.2359 - mae: 6.8123 - val_loss: 92.0561 - val_mae: 8.0651\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.4385 - mae: 7.2712 - val_loss: 70.5531 - val_mae: 6.3205\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.3786 - mae: 6.8308 - val_loss: 67.9303 - val_mae: 6.2573\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.5715 - mae: 6.7811 - val_loss: 67.1053 - val_mae: 6.2198\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4215 - mae: 6.7101 - val_loss: 70.2434 - val_mae: 6.5465\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.9961 - mae: 6.8867 - val_loss: 65.9498 - val_mae: 6.1531\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0050 - mae: 6.8004 - val_loss: 68.9907 - val_mae: 6.2191\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.8832 - mae: 6.6523 - val_loss: 65.5322 - val_mae: 6.0856\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.7189 - mae: 6.7060 - val_loss: 70.3821 - val_mae: 6.6673\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.8353 - mae: 6.7336 - val_loss: 64.3956 - val_mae: 6.0129\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.8273 - mae: 6.5880 - val_loss: 63.3061 - val_mae: 5.9701\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.8878 - mae: 6.5033 - val_loss: 62.8884 - val_mae: 6.0339\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.5055 - mae: 6.5124 - val_loss: 65.8893 - val_mae: 6.3447\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.6192 - mae: 6.6569 - val_loss: 61.2605 - val_mae: 5.8758\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.4820 - mae: 6.6405 - val_loss: 62.8491 - val_mae: 5.9233\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.5747 - mae: 6.4479 - val_loss: 62.3492 - val_mae: 5.8948\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.0196 - mae: 6.3398 - val_loss: 60.1445 - val_mae: 5.8694\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1301 - mae: 6.5617 - val_loss: 60.5869 - val_mae: 5.9305\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.0161 - mae: 6.4980 - val_loss: 67.5172 - val_mae: 6.1728\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.6276 - mae: 6.2152 - val_loss: 60.4848 - val_mae: 5.9684\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.6452 - mae: 6.4128 - val_loss: 58.3881 - val_mae: 5.6992\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.1151 - mae: 6.2862 - val_loss: 61.0210 - val_mae: 5.8447\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.7228 - mae: 6.2837 - val_loss: 56.9230 - val_mae: 5.6619\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.8668 - mae: 6.4044 - val_loss: 59.3212 - val_mae: 5.7428\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.6474 - mae: 6.2600 - val_loss: 56.2585 - val_mae: 5.6142\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.0908 - mae: 6.2044 - val_loss: 57.0117 - val_mae: 5.7292\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.3163 - mae: 6.2913 - val_loss: 58.6202 - val_mae: 5.7093\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.4374 - mae: 6.1660 - val_loss: 55.5912 - val_mae: 5.6113\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.2863 - mae: 6.2400 - val_loss: 62.7182 - val_mae: 5.9422\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.8325 - mae: 6.1031 - val_loss: 54.7781 - val_mae: 5.5430\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.5965 - mae: 6.1322 - val_loss: 56.8582 - val_mae: 5.6143\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.2048 - mae: 5.9741 - val_loss: 54.0131 - val_mae: 5.5113\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.5700 - mae: 6.0686 - val_loss: 54.9038 - val_mae: 5.6458\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1964 - mae: 6.1446 - val_loss: 55.9052 - val_mae: 5.5532\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.0952 - mae: 5.9393 - val_loss: 58.5106 - val_mae: 6.0117\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1950 - mae: 6.2411 - val_loss: 54.8981 - val_mae: 5.5172\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.0744 - mae: 6.0000 - val_loss: 55.3189 - val_mae: 5.7790\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.2454 - mae: 6.1254 - val_loss: 60.9002 - val_mae: 5.8522\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.3824 - mae: 5.9124 - val_loss: 52.9024 - val_mae: 5.3964\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4744 - mae: 6.0258 - val_loss: 51.2591 - val_mae: 5.3659\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.7144 - mae: 6.0358 - val_loss: 52.0116 - val_mae: 5.3392\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.7920 - mae: 6.0317 - val_loss: 53.0384 - val_mae: 5.6297\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.2142 - mae: 6.0833 - val_loss: 51.1716 - val_mae: 5.2841\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.6093 - mae: 5.9458 - val_loss: 52.6486 - val_mae: 5.3926\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.6075 - mae: 5.9586 - val_loss: 53.6553 - val_mae: 5.4565\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.2489 - mae: 6.0507 - val_loss: 49.6747 - val_mae: 5.2446\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.6896 - mae: 5.8653 - val_loss: 49.6072 - val_mae: 5.1885\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.6927 - mae: 5.7932 - val_loss: 49.8535 - val_mae: 5.3786\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.1418 - mae: 5.8971 - val_loss: 50.3169 - val_mae: 5.4467\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.6335 - mae: 5.9693 - val_loss: 48.6739 - val_mae: 5.2297\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.7246 - mae: 5.8039 - val_loss: 53.4807 - val_mae: 5.7873\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.2222 - mae: 5.9829 - val_loss: 48.1321 - val_mae: 5.1667\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.2252 - mae: 5.8182 - val_loss: 48.9026 - val_mae: 5.1540\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.5357 - mae: 5.7400 - val_loss: 48.5078 - val_mae: 5.3181\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.4256 - mae: 5.9318 - val_loss: 48.0264 - val_mae: 5.1060\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.4912 - mae: 5.7180 - val_loss: 50.4539 - val_mae: 5.2706\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.7321 - mae: 5.6667 - val_loss: 48.4241 - val_mae: 5.1448\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.6053 - mae: 5.7594 - val_loss: 47.3165 - val_mae: 5.0772\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.8961 - mae: 5.6492 - val_loss: 47.9550 - val_mae: 5.1070\n",
      "5/5 [==============================] - 0s 865us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 245.0107 - mae: 12.4712 - val_loss: 69.1099 - val_mae: 6.9892\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.6846 - mae: 8.2364 - val_loss: 65.9081 - val_mae: 7.0236\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.3156 - mae: 8.1410 - val_loss: 48.6255 - val_mae: 6.0213\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.0099 - mae: 7.8191 - val_loss: 47.0729 - val_mae: 5.5737\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.0674 - mae: 7.3758 - val_loss: 33.2447 - val_mae: 4.8295\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.1004 - mae: 7.1740 - val_loss: 34.7018 - val_mae: 5.0649\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.5670 - mae: 7.0318 - val_loss: 29.1014 - val_mae: 4.4669\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.4702 - mae: 6.7809 - val_loss: 47.6411 - val_mae: 5.5796\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.7643 - mae: 6.7601 - val_loss: 47.0585 - val_mae: 5.5948\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.1960 - mae: 6.5959 - val_loss: 29.5862 - val_mae: 4.5405\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.3339 - mae: 6.6583 - val_loss: 24.4300 - val_mae: 4.0788\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.2482 - mae: 6.5325 - val_loss: 25.3963 - val_mae: 4.2181\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.2883 - mae: 6.4646 - val_loss: 42.6529 - val_mae: 5.2977\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.3291 - mae: 6.9790 - val_loss: 22.8608 - val_mae: 3.9386\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.0363 - mae: 6.4654 - val_loss: 22.7462 - val_mae: 3.9486\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.0680 - mae: 6.3299 - val_loss: 31.4716 - val_mae: 4.5533\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.2495 - mae: 6.2712 - val_loss: 30.4976 - val_mae: 4.5162\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.7868 - mae: 6.4448 - val_loss: 23.1282 - val_mae: 4.0143\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.2197 - mae: 6.1719 - val_loss: 24.9386 - val_mae: 4.1871\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.2628 - mae: 6.3309 - val_loss: 28.4420 - val_mae: 4.4628\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.3498 - mae: 6.1245 - val_loss: 23.9892 - val_mae: 4.0373\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.4112 - mae: 6.2329 - val_loss: 19.9415 - val_mae: 3.7255\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.9305 - mae: 6.3483 - val_loss: 25.5550 - val_mae: 4.1559\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.9027 - mae: 6.0095 - val_loss: 37.8533 - val_mae: 5.0664\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4028 - mae: 6.1076 - val_loss: 22.5835 - val_mae: 3.9074\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.0270 - mae: 6.0190 - val_loss: 33.4227 - val_mae: 4.7665\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.8107 - mae: 6.2868 - val_loss: 19.6561 - val_mae: 3.7151\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.6374 - mae: 6.1099 - val_loss: 19.0881 - val_mae: 3.6326\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.1915 - mae: 6.0197 - val_loss: 27.9399 - val_mae: 4.3555\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.6218 - mae: 5.9473 - val_loss: 19.9709 - val_mae: 3.6819\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.6318 - mae: 5.8888 - val_loss: 23.5110 - val_mae: 3.9461\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.3789 - mae: 5.9823 - val_loss: 24.8729 - val_mae: 4.0849\n",
      "Epoch 00032: early stopping\n",
      "5/5 [==============================] - 0s 721us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 629.0927 - mae: 18.6219 - val_loss: 40.3282 - val_mae: 4.9687\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.8442 - mae: 7.6125 - val_loss: 27.5540 - val_mae: 3.9732\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.6239 - mae: 7.6598 - val_loss: 37.7216 - val_mae: 4.9445\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.3591 - mae: 7.2143 - val_loss: 33.2343 - val_mae: 4.6447\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.7744 - mae: 7.3697 - val_loss: 20.5906 - val_mae: 3.5354\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.9358 - mae: 7.0967 - val_loss: 19.3406 - val_mae: 3.3566\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.3707 - mae: 7.2202 - val_loss: 23.4763 - val_mae: 3.8843\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.6379 - mae: 7.1337 - val_loss: 59.2896 - val_mae: 6.5479\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6441 - mae: 6.9755 - val_loss: 37.2111 - val_mae: 5.0105\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.2655 - mae: 6.8604 - val_loss: 54.8733 - val_mae: 6.3006\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.4509 - mae: 7.3637 - val_loss: 18.5403 - val_mae: 3.3862\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.0299 - mae: 7.0560 - val_loss: 24.4557 - val_mae: 3.8877\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.5802 - mae: 6.8332 - val_loss: 21.0923 - val_mae: 3.6307\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.4648 - mae: 6.9895 - val_loss: 32.7786 - val_mae: 4.6674\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.5751 - mae: 6.9455 - val_loss: 16.0792 - val_mae: 3.1647\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.7438 - mae: 6.8491 - val_loss: 15.5578 - val_mae: 3.1090\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.6719 - mae: 6.7456 - val_loss: 38.2585 - val_mae: 5.0636\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.3464 - mae: 6.9022 - val_loss: 14.7862 - val_mae: 3.0693\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.4584 - mae: 6.4449 - val_loss: 16.8472 - val_mae: 3.3318\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.1776 - mae: 6.7722 - val_loss: 24.4029 - val_mae: 4.0261\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.2273 - mae: 6.4964 - val_loss: 17.7960 - val_mae: 3.3704\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.9035 - mae: 6.8268 - val_loss: 14.4924 - val_mae: 3.0491\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.7109 - mae: 6.4411 - val_loss: 17.4705 - val_mae: 3.3146\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.6190 - mae: 6.4981 - val_loss: 61.2650 - val_mae: 6.6673\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.7880 - mae: 6.5526 - val_loss: 20.4704 - val_mae: 3.5918\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.6200 - mae: 6.6538 - val_loss: 43.9200 - val_mae: 5.5018\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.1781 - mae: 6.7976 - val_loss: 17.5272 - val_mae: 3.4644\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0111 - mae: 6.9152 - val_loss: 16.0379 - val_mae: 3.2498\n",
      "Epoch 00028: early stopping\n",
      "5/5 [==============================] - 0s 684us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 594.4365 - mae: 18.3874 - val_loss: 116.5630 - val_mae: 8.0027\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.3730 - mae: 7.9369 - val_loss: 91.1313 - val_mae: 7.4636\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.0978 - mae: 7.7133 - val_loss: 87.5849 - val_mae: 7.4308\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.3147 - mae: 7.1597 - val_loss: 78.7439 - val_mae: 6.8850\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.3425 - mae: 7.1498 - val_loss: 77.8864 - val_mae: 6.5303\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.1458 - mae: 6.8671 - val_loss: 71.8241 - val_mae: 6.5178\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.6535 - mae: 7.1445 - val_loss: 70.2880 - val_mae: 6.2370\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.2889 - mae: 6.9066 - val_loss: 87.1883 - val_mae: 6.9072\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.7025 - mae: 6.6156 - val_loss: 62.7565 - val_mae: 6.1569\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1195 - mae: 6.5147 - val_loss: 70.8447 - val_mae: 6.2879\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.7526 - mae: 6.3835 - val_loss: 57.7360 - val_mae: 5.9599\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.0606 - mae: 6.4908 - val_loss: 61.5852 - val_mae: 5.9365\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.0230 - mae: 6.2970 - val_loss: 54.9048 - val_mae: 5.8005\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.7840 - mae: 6.1625 - val_loss: 53.5115 - val_mae: 5.6509\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.2229 - mae: 6.0652 - val_loss: 57.7977 - val_mae: 5.7736\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.2927 - mae: 6.1038 - val_loss: 54.3935 - val_mae: 5.8587\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.5345 - mae: 6.0449 - val_loss: 59.8229 - val_mae: 5.8197\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.5594 - mae: 6.1200 - val_loss: 48.9213 - val_mae: 5.4071\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.5436 - mae: 6.1578 - val_loss: 55.6031 - val_mae: 5.6200\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.9353 - mae: 5.9885 - val_loss: 48.2657 - val_mae: 5.3080\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.8127 - mae: 6.1838 - val_loss: 47.2536 - val_mae: 5.3096\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.4070 - mae: 5.6679 - val_loss: 52.2802 - val_mae: 5.7484\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.4867 - mae: 6.2820 - val_loss: 50.7728 - val_mae: 5.4194\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.1806 - mae: 5.7347 - val_loss: 50.7996 - val_mae: 5.6296\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.1950 - mae: 5.8193 - val_loss: 44.8642 - val_mae: 5.1840\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.2761 - mae: 5.7153 - val_loss: 46.1047 - val_mae: 5.2945\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.7985 - mae: 5.8529 - val_loss: 53.4573 - val_mae: 5.6605\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8362 - mae: 5.8149 - val_loss: 44.1878 - val_mae: 5.1173\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.9974 - mae: 5.6394 - val_loss: 45.8270 - val_mae: 5.2007\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.4444 - mae: 5.6725 - val_loss: 43.8774 - val_mae: 5.1394\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.2140 - mae: 5.5860 - val_loss: 43.0144 - val_mae: 5.0639\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.9226 - mae: 5.8893 - val_loss: 43.6468 - val_mae: 5.0165\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.7637 - mae: 5.6542 - val_loss: 48.5539 - val_mae: 5.4232\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.0470 - mae: 5.7402 - val_loss: 42.4445 - val_mae: 4.9863\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.8781 - mae: 5.5260 - val_loss: 43.0443 - val_mae: 4.9738\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.2318 - mae: 5.5154 - val_loss: 43.2346 - val_mae: 5.0175\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.8480 - mae: 5.7770 - val_loss: 48.3946 - val_mae: 5.3487\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.9630 - mae: 5.5981 - val_loss: 46.1876 - val_mae: 5.2448\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.3924 - mae: 6.0323 - val_loss: 48.9489 - val_mae: 5.4109\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.5017 - mae: 5.7286 - val_loss: 41.3941 - val_mae: 4.8439\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.0643 - mae: 5.6912 - val_loss: 43.5439 - val_mae: 5.0552\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.0576 - mae: 5.2325 - val_loss: 99.9288 - val_mae: 8.4106\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1068 - mae: 6.2204 - val_loss: 42.2122 - val_mae: 4.9710\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.1945 - mae: 5.5974 - val_loss: 44.7667 - val_mae: 5.1485\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.0795 - mae: 5.3704 - val_loss: 40.8906 - val_mae: 4.7768\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.8755 - mae: 5.3560 - val_loss: 47.6734 - val_mae: 5.2196\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.6663 - mae: 5.4113 - val_loss: 48.2199 - val_mae: 5.4431\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.5219 - mae: 5.5762 - val_loss: 46.6006 - val_mae: 5.3305\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.5266 - mae: 5.6186 - val_loss: 45.4615 - val_mae: 5.0794\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.4358 - mae: 5.6762 - val_loss: 43.9018 - val_mae: 4.9379\n",
      "Epoch 00050: early stopping\n",
      "5/5 [==============================] - 0s 731us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 991.3160 - mae: 22.2300 - val_loss: 78.4438 - val_mae: 7.5708\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.9691 - mae: 8.0583 - val_loss: 42.7997 - val_mae: 5.3648\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.0955 - mae: 7.0176 - val_loss: 31.4068 - val_mae: 4.6406\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.1956 - mae: 6.6210 - val_loss: 28.6357 - val_mae: 4.1169\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.4838 - mae: 6.5540 - val_loss: 22.3379 - val_mae: 3.7472\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.1013 - mae: 6.2611 - val_loss: 18.1013 - val_mae: 3.5663\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.6187 - mae: 6.1186 - val_loss: 19.2405 - val_mae: 3.5140\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.3799 - mae: 6.0354 - val_loss: 25.8899 - val_mae: 4.0498\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.5309 - mae: 6.0667 - val_loss: 27.2803 - val_mae: 4.1791\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.3209 - mae: 6.1063 - val_loss: 19.9781 - val_mae: 3.6566\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.6101 - mae: 5.9436 - val_loss: 19.4952 - val_mae: 3.6446\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.3392 - mae: 5.9223 - val_loss: 17.8025 - val_mae: 3.5404\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.7840 - mae: 5.7434 - val_loss: 26.5268 - val_mae: 4.2630\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.8707 - mae: 6.0537 - val_loss: 15.9595 - val_mae: 3.3989\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.2311 - mae: 5.8538 - val_loss: 15.5402 - val_mae: 3.3519\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.9331 - mae: 5.9358 - val_loss: 14.5192 - val_mae: 3.2958\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.0070 - mae: 5.6854 - val_loss: 17.5740 - val_mae: 3.6137\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.3613 - mae: 5.7683 - val_loss: 18.6949 - val_mae: 3.7211\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8013 - mae: 5.7114 - val_loss: 26.7955 - val_mae: 4.3634\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.1598 - mae: 5.8136 - val_loss: 28.0186 - val_mae: 4.4718\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.3315 - mae: 5.9208 - val_loss: 18.6541 - val_mae: 3.7112\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.8504 - mae: 5.7487 - val_loss: 17.5818 - val_mae: 3.6232\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.2299 - mae: 5.6545 - val_loss: 21.9759 - val_mae: 3.9544\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.1251 - mae: 5.6971 - val_loss: 20.8510 - val_mae: 3.8829\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.5109 - mae: 5.7417 - val_loss: 15.2009 - val_mae: 3.3804\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.3682 - mae: 5.6098 - val_loss: 20.2690 - val_mae: 3.8426\n",
      "Epoch 00026: early stopping\n",
      "5/5 [==============================] - 0s 843us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 16ms/step - loss: 231.6496 - mae: 10.9965 - val_loss: 14.6535 - val_mae: 3.1891\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.1232 - mae: 6.5751 - val_loss: 16.5078 - val_mae: 3.1787\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.2873 - mae: 6.7327 - val_loss: 28.7030 - val_mae: 4.6034\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.9868 - mae: 6.4784 - val_loss: 20.5511 - val_mae: 3.8383\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.1371 - mae: 6.5080 - val_loss: 15.8731 - val_mae: 3.3432\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.5024 - mae: 6.3887 - val_loss: 17.4588 - val_mae: 3.5128\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.8817 - mae: 6.5047 - val_loss: 15.7456 - val_mae: 3.3337\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.2048 - mae: 6.4644 - val_loss: 29.2431 - val_mae: 4.7640\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.7765 - mae: 6.2605 - val_loss: 36.8030 - val_mae: 5.3245\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.0324 - mae: 6.3661 - val_loss: 29.1511 - val_mae: 4.7585\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.9224 - mae: 6.4543 - val_loss: 18.5920 - val_mae: 3.6293\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 793us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 128.8379 - mae: 8.6914 - val_loss: 90.7970 - val_mae: 6.9259\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.8079 - mae: 7.1102 - val_loss: 67.6798 - val_mae: 5.9611\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.8805 - mae: 6.3577 - val_loss: 52.9389 - val_mae: 5.1174\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.5345 - mae: 5.8022 - val_loss: 46.8579 - val_mae: 4.9260\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.2763 - mae: 5.4839 - val_loss: 43.3683 - val_mae: 4.7145\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.5761 - mae: 5.3145 - val_loss: 41.9121 - val_mae: 4.7829\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.9764 - mae: 5.3988 - val_loss: 43.0874 - val_mae: 5.0256\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.4836 - mae: 5.4212 - val_loss: 42.3331 - val_mae: 4.5716\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.3941 - mae: 5.0892 - val_loss: 39.7462 - val_mae: 4.8403\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.6653 - mae: 5.3739 - val_loss: 37.0762 - val_mae: 4.4457\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.2853 - mae: 5.2730 - val_loss: 38.1364 - val_mae: 4.4485\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.2183 - mae: 5.0893 - val_loss: 35.8182 - val_mae: 4.4713\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.8266 - mae: 5.0486 - val_loss: 35.8300 - val_mae: 4.4636\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.7585 - mae: 4.9956 - val_loss: 35.5275 - val_mae: 4.3276\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.5385 - mae: 5.0762 - val_loss: 41.8302 - val_mae: 4.8564\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.0875 - mae: 5.0636 - val_loss: 38.9785 - val_mae: 4.8510\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.2523 - mae: 5.0999 - val_loss: 35.7523 - val_mae: 4.3611\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.1825 - mae: 5.0471 - val_loss: 34.7962 - val_mae: 4.3428\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.9900 - mae: 5.0820 - val_loss: 37.3055 - val_mae: 4.4875\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.9964 - mae: 4.8640 - val_loss: 35.1220 - val_mae: 4.5072\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.5170 - mae: 4.9927 - val_loss: 33.8364 - val_mae: 4.3527\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.8407 - mae: 4.8356 - val_loss: 42.0373 - val_mae: 5.1607\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.6207 - mae: 5.3269 - val_loss: 33.5913 - val_mae: 4.2652\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.3093 - mae: 4.7819 - val_loss: 35.0500 - val_mae: 4.5015\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.7525 - mae: 4.9012 - val_loss: 33.3239 - val_mae: 4.2893\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.4301 - mae: 4.8431 - val_loss: 35.6650 - val_mae: 4.5686\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.1927 - mae: 5.0471 - val_loss: 37.6849 - val_mae: 4.6349\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.9334 - mae: 4.9670 - val_loss: 33.0538 - val_mae: 4.2339\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.0437 - mae: 4.8555 - val_loss: 32.8093 - val_mae: 4.2431\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.1209 - mae: 4.7895 - val_loss: 32.8411 - val_mae: 4.2597\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 49.2911 - mae: 4.8376 - val_loss: 33.8643 - val_mae: 4.4095\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6568 - mae: 4.9489 - val_loss: 33.1850 - val_mae: 4.3064\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 49.1995 - mae: 4.8524 - val_loss: 35.1494 - val_mae: 4.4891\n",
      "Epoch 00033: early stopping\n",
      "5/5 [==============================] - 0s 805us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 741.1033 - mae: 17.5525 - val_loss: 108.0920 - val_mae: 8.2047\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.9830 - mae: 7.6136 - val_loss: 59.2490 - val_mae: 5.9279\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.0706 - mae: 6.9576 - val_loss: 43.4436 - val_mae: 5.0722\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0235 - mae: 6.8703 - val_loss: 46.0957 - val_mae: 5.3833\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.0369 - mae: 6.6599 - val_loss: 34.2726 - val_mae: 4.4653\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.0220 - mae: 6.4504 - val_loss: 32.4744 - val_mae: 4.2586\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.6006 - mae: 6.4993 - val_loss: 30.1842 - val_mae: 4.2230\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.2286 - mae: 6.5278 - val_loss: 41.4360 - val_mae: 5.4854\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.0994 - mae: 6.3867 - val_loss: 42.0310 - val_mae: 5.5990\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.6789 - mae: 6.4026 - val_loss: 28.9449 - val_mae: 4.1593\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.5041 - mae: 6.3723 - val_loss: 26.0232 - val_mae: 3.9012\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.0797 - mae: 6.2317 - val_loss: 25.6932 - val_mae: 3.8905\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1000 - mae: 6.1920 - val_loss: 47.7097 - val_mae: 6.1448\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.4368 - mae: 6.6362 - val_loss: 23.9164 - val_mae: 3.7405\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.9147 - mae: 6.1368 - val_loss: 26.6818 - val_mae: 4.1567\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.9834 - mae: 6.1847 - val_loss: 23.3004 - val_mae: 3.6402\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.7742 - mae: 5.9767 - val_loss: 27.4724 - val_mae: 4.2919\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.8220 - mae: 6.1092 - val_loss: 21.4255 - val_mae: 3.5289\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.9160 - mae: 5.8558 - val_loss: 25.0394 - val_mae: 4.0628\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.2218 - mae: 5.9139 - val_loss: 35.8771 - val_mae: 5.2115\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.4311 - mae: 6.0460 - val_loss: 21.8376 - val_mae: 3.6892\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.5337 - mae: 6.0413 - val_loss: 19.7951 - val_mae: 3.3476\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.3150 - mae: 5.8787 - val_loss: 22.3527 - val_mae: 3.8161\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.0321 - mae: 5.7838 - val_loss: 24.9040 - val_mae: 4.1402\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.7598 - mae: 5.9161 - val_loss: 22.5216 - val_mae: 3.8510\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.8186 - mae: 5.8055 - val_loss: 26.2459 - val_mae: 4.3197\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8837 - mae: 5.8930 - val_loss: 18.5342 - val_mae: 3.2774\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.7142 - mae: 5.8819 - val_loss: 18.9965 - val_mae: 3.3394\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.7300 - mae: 5.7632 - val_loss: 22.4315 - val_mae: 3.8938\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.1100 - mae: 5.7545 - val_loss: 18.2668 - val_mae: 3.1858\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.1667 - mae: 5.7191 - val_loss: 19.3913 - val_mae: 3.2519\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.0172 - mae: 5.6677 - val_loss: 25.7821 - val_mae: 4.3063\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.2198 - mae: 5.8174 - val_loss: 19.9852 - val_mae: 3.6149\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.1476 - mae: 5.6529 - val_loss: 21.7433 - val_mae: 3.8423\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.1870 - mae: 5.7676 - val_loss: 19.4161 - val_mae: 3.5370\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.7529 - mae: 5.7217 - val_loss: 17.8615 - val_mae: 3.2622\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.5139 - mae: 5.6981 - val_loss: 16.9459 - val_mae: 3.0813\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.9467 - mae: 5.6546 - val_loss: 17.8856 - val_mae: 3.0994\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 52.5730 - mae: 4.51 - 0s 3ms/step - loss: 64.1111 - mae: 5.7266 - val_loss: 17.6195 - val_mae: 3.2594\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.4580 - mae: 5.7842 - val_loss: 21.1784 - val_mae: 3.8038\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.7641 - mae: 5.7754 - val_loss: 17.0436 - val_mae: 3.1854\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.1740 - mae: 5.6469 - val_loss: 16.8379 - val_mae: 3.1487\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.9562 - mae: 5.6439 - val_loss: 16.2753 - val_mae: 3.0117\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.0960 - mae: 5.6082 - val_loss: 16.1426 - val_mae: 3.0055\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.0793 - mae: 5.6995 - val_loss: 17.5011 - val_mae: 3.2973\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.0458 - mae: 5.6037 - val_loss: 16.5161 - val_mae: 3.0078\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.6703 - mae: 5.6011 - val_loss: 16.6452 - val_mae: 3.1721\n",
      "Epoch 00047: early stopping\n",
      "5/5 [==============================] - 0s 796us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 455.4104 - mae: 15.4651 - val_loss: 89.5609 - val_mae: 7.3846\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.2342 - mae: 8.7624 - val_loss: 44.2274 - val_mae: 5.2596\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.7052 - mae: 7.9404 - val_loss: 44.7944 - val_mae: 5.4679\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.2504 - mae: 7.3835 - val_loss: 38.0748 - val_mae: 5.0316\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.8014 - mae: 7.1186 - val_loss: 26.9122 - val_mae: 4.1664\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.4982 - mae: 7.1708 - val_loss: 24.7569 - val_mae: 3.9250\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.2521 - mae: 6.9214 - val_loss: 37.6946 - val_mae: 5.1222\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.1473 - mae: 7.2538 - val_loss: 37.5730 - val_mae: 5.1306\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.6720 - mae: 6.6705 - val_loss: 46.5065 - val_mae: 5.7956\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.7804 - mae: 6.7132 - val_loss: 56.0380 - val_mae: 6.3885\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.7257 - mae: 6.9200 - val_loss: 22.5469 - val_mae: 3.8434\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0952 - mae: 6.6323 - val_loss: 48.4301 - val_mae: 5.9495\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.3953 - mae: 6.6377 - val_loss: 46.0718 - val_mae: 5.7970\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.2729 - mae: 7.0425 - val_loss: 26.9894 - val_mae: 4.3138\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.6536 - mae: 6.4703 - val_loss: 27.0710 - val_mae: 4.3241\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.2435 - mae: 6.4404 - val_loss: 25.6672 - val_mae: 4.1923\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.9976 - mae: 6.5197 - val_loss: 45.3314 - val_mae: 5.7147\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4331 - mae: 6.7242 - val_loss: 24.5273 - val_mae: 4.1273\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.0730 - mae: 6.3628 - val_loss: 28.8119 - val_mae: 4.4832\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.7701 - mae: 6.5146 - val_loss: 35.9877 - val_mae: 5.0594\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.5989 - mae: 6.4264 - val_loss: 19.0320 - val_mae: 3.6194\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.1960 - mae: 6.4647 - val_loss: 18.2550 - val_mae: 3.5434\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.2426 - mae: 6.2456 - val_loss: 27.9718 - val_mae: 4.4109\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.2777 - mae: 6.4287 - val_loss: 34.0246 - val_mae: 4.8843\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.5561 - mae: 6.3299 - val_loss: 28.2022 - val_mae: 4.4371\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.2828 - mae: 6.4928 - val_loss: 41.5326 - val_mae: 5.4656\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.9210 - mae: 6.4554 - val_loss: 18.6884 - val_mae: 3.4425\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.6057 - mae: 6.5337 - val_loss: 18.1636 - val_mae: 3.5077\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.0963 - mae: 6.4450 - val_loss: 17.5403 - val_mae: 3.4426\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.1611 - mae: 6.3473 - val_loss: 19.9737 - val_mae: 3.7051\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.9237 - mae: 6.4140 - val_loss: 17.6429 - val_mae: 3.3645\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0812 - mae: 6.4420 - val_loss: 49.5937 - val_mae: 5.9933\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.4639 - mae: 6.4526 - val_loss: 26.5553 - val_mae: 4.2946\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.6079 - mae: 6.2792 - val_loss: 29.8387 - val_mae: 4.5641\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.7387 - mae: 6.2875 - val_loss: 25.0713 - val_mae: 4.1736\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.9621 - mae: 6.3495 - val_loss: 29.2315 - val_mae: 4.5023\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.5258 - mae: 6.5720 - val_loss: 16.7872 - val_mae: 3.3784\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.0796 - mae: 6.2861 - val_loss: 16.4639 - val_mae: 3.3640\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.9411 - mae: 6.3091 - val_loss: 17.7486 - val_mae: 3.5046\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.0215 - mae: 6.1615 - val_loss: 28.3159 - val_mae: 4.4316\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.7995 - mae: 6.3553 - val_loss: 18.2273 - val_mae: 3.5662\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.1100 - mae: 6.2737 - val_loss: 16.9322 - val_mae: 3.4254\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.5184 - mae: 6.2701 - val_loss: 21.8223 - val_mae: 3.8770\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.3847 - mae: 6.1890 - val_loss: 19.4416 - val_mae: 3.6458\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.1514 - mae: 6.3658 - val_loss: 16.7970 - val_mae: 3.4145\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.4213 - mae: 6.1899 - val_loss: 16.0753 - val_mae: 3.2901\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.2563 - mae: 6.1915 - val_loss: 15.6287 - val_mae: 3.3143\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.5409 - mae: 6.1799 - val_loss: 25.9203 - val_mae: 4.2356\n",
      "Epoch 00048: early stopping\n",
      "5/5 [==============================] - 0s 790us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 481.8265 - mae: 16.5969 - val_loss: 176.5697 - val_mae: 10.5075\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.0548 - mae: 8.1382 - val_loss: 96.1867 - val_mae: 7.0487\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.9364 - mae: 6.5653 - val_loss: 73.5078 - val_mae: 6.0066\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.4972 - mae: 5.9322 - val_loss: 62.2378 - val_mae: 5.5422\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.9507 - mae: 5.7808 - val_loss: 58.6529 - val_mae: 5.3534\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.0617 - mae: 5.7891 - val_loss: 55.4376 - val_mae: 5.3842\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.2937 - mae: 5.9682 - val_loss: 60.9289 - val_mae: 5.9981\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.3571 - mae: 5.8130 - val_loss: 53.5285 - val_mae: 5.1839\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.8000 - mae: 5.5328 - val_loss: 50.7586 - val_mae: 5.3147\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.8742 - mae: 5.5418 - val_loss: 49.4387 - val_mae: 5.1794\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.8089 - mae: 5.5002 - val_loss: 49.3042 - val_mae: 5.1394\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.7737 - mae: 5.5511 - val_loss: 53.3080 - val_mae: 5.3000\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.9034 - mae: 5.5609 - val_loss: 47.9956 - val_mae: 5.1852\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.1512 - mae: 5.5089 - val_loss: 46.9095 - val_mae: 5.0612\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.8620 - mae: 5.4131 - val_loss: 47.4503 - val_mae: 5.0152\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.9467 - mae: 5.3510 - val_loss: 52.1736 - val_mae: 5.5307\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.2964 - mae: 5.5248 - val_loss: 48.2228 - val_mae: 5.0168\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.1727 - mae: 5.4611 - val_loss: 45.3026 - val_mae: 5.0014\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.7898 - mae: 5.5116 - val_loss: 46.0731 - val_mae: 4.9538\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.7220 - mae: 5.3233 - val_loss: 44.9146 - val_mae: 4.9531\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.8184 - mae: 5.4417 - val_loss: 45.1880 - val_mae: 4.9338\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.7710 - mae: 5.2749 - val_loss: 45.3327 - val_mae: 5.0825\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.6445 - mae: 5.4440 - val_loss: 44.9242 - val_mae: 4.8697\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.4618 - mae: 5.2825 - val_loss: 48.7257 - val_mae: 5.3156\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.5011 - mae: 5.4081 - val_loss: 43.5759 - val_mae: 4.9030\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.5631 - mae: 5.2666 - val_loss: 50.9663 - val_mae: 5.4678\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.8734 - mae: 5.5055 - val_loss: 43.9269 - val_mae: 4.8435\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.5663 - mae: 5.4132 - val_loss: 43.2506 - val_mae: 4.9105\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1779 - mae: 5.2408 - val_loss: 42.9603 - val_mae: 4.8448\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.9492 - mae: 5.2871 - val_loss: 47.3056 - val_mae: 5.2064\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1193 - mae: 5.2868 - val_loss: 42.6367 - val_mae: 4.8425\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.5093 - mae: 5.2878 - val_loss: 45.6475 - val_mae: 5.1015\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.4609 - mae: 5.4569 - val_loss: 47.0726 - val_mae: 5.1010\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.4690 - mae: 5.3399 - val_loss: 42.9096 - val_mae: 4.9139\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.1287 - mae: 5.2461 - val_loss: 42.4189 - val_mae: 4.8350\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.5801 - mae: 5.2188 - val_loss: 43.9638 - val_mae: 4.8607\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.4536 - mae: 5.4639 - val_loss: 46.4897 - val_mae: 5.0692\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.9395 - mae: 5.3204 - val_loss: 43.8306 - val_mae: 4.8703\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.0932 - mae: 5.2661 - val_loss: 41.9911 - val_mae: 4.8877\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.4269 - mae: 5.2688 - val_loss: 41.6046 - val_mae: 4.7788\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.3009 - mae: 5.2825 - val_loss: 41.4698 - val_mae: 4.7507\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.3616 - mae: 4.9580 - val_loss: 86.6224 - val_mae: 7.6293\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.3565 - mae: 5.8084 - val_loss: 41.4369 - val_mae: 4.7408\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.7764 - mae: 5.2670 - val_loss: 42.4249 - val_mae: 4.7732\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.9379 - mae: 5.1224 - val_loss: 43.3407 - val_mae: 4.9324\n",
      "Epoch 00045: early stopping\n",
      "5/5 [==============================] - 0s 778us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 668.7517 - mae: 23.7479 - val_loss: 626.9439 - val_mae: 23.8032\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 656.8172 - mae: 23.4905 - val_loss: 616.6831 - val_mae: 23.5643\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 645.4090 - mae: 23.2360 - val_loss: 606.5461 - val_mae: 23.3234\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.8249 - mae: 22.9810 - val_loss: 596.6116 - val_mae: 23.0855\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.7169 - mae: 22.7366 - val_loss: 586.8321 - val_mae: 22.8464\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 611.9053 - mae: 22.4837 - val_loss: 577.3173 - val_mae: 22.6094\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.2342 - mae: 22.2397 - val_loss: 568.0682 - val_mae: 22.3756\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 590.5705 - mae: 21.9971 - val_loss: 559.1025 - val_mae: 22.1622\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 580.7789 - mae: 21.7642 - val_loss: 550.2703 - val_mae: 21.9542\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 570.9294 - mae: 21.5269 - val_loss: 541.5749 - val_mae: 21.7461\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 560.9264 - mae: 21.2890 - val_loss: 533.2495 - val_mae: 21.5448\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 551.5991 - mae: 21.0607 - val_loss: 524.9733 - val_mae: 21.3417\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 542.3132 - mae: 20.8372 - val_loss: 516.9329 - val_mae: 21.1414\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 533.2202 - mae: 20.6113 - val_loss: 509.1425 - val_mae: 20.9428\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 524.4563 - mae: 20.3922 - val_loss: 501.4581 - val_mae: 20.7441\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 515.8341 - mae: 20.1740 - val_loss: 493.9439 - val_mae: 20.5462\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 507.1330 - mae: 19.9527 - val_loss: 486.7885 - val_mae: 20.3564\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 499.0906 - mae: 19.7412 - val_loss: 479.7245 - val_mae: 20.1655\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 491.1053 - mae: 19.5339 - val_loss: 472.7997 - val_mae: 19.9770\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 483.2988 - mae: 19.3276 - val_loss: 465.9271 - val_mae: 19.7898\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 475.6243 - mae: 19.1224 - val_loss: 459.2018 - val_mae: 19.6003\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 468.1182 - mae: 18.9203 - val_loss: 452.6357 - val_mae: 19.4112\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.5807 - mae: 18.7163 - val_loss: 446.2764 - val_mae: 19.2278\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 453.3173 - mae: 18.5222 - val_loss: 440.0426 - val_mae: 19.0482\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 446.4286 - mae: 18.3326 - val_loss: 433.9045 - val_mae: 18.8649\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 439.4129 - mae: 18.1409 - val_loss: 427.9823 - val_mae: 18.6860\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 432.4930 - mae: 17.9512 - val_loss: 422.2922 - val_mae: 18.5129\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 425.9684 - mae: 17.7657 - val_loss: 416.6073 - val_mae: 18.3378\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 419.5766 - mae: 17.5840 - val_loss: 411.1431 - val_mae: 18.1615\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 413.2294 - mae: 17.4040 - val_loss: 405.6937 - val_mae: 17.9884\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 406.9350 - mae: 17.2257 - val_loss: 400.3679 - val_mae: 17.8196\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 400.8596 - mae: 17.0559 - val_loss: 395.2268 - val_mae: 17.6512\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 395.0292 - mae: 16.8852 - val_loss: 390.1287 - val_mae: 17.4807\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 389.0565 - mae: 16.7224 - val_loss: 385.1137 - val_mae: 17.3160\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 383.4237 - mae: 16.5600 - val_loss: 380.1224 - val_mae: 17.1526\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 378.0849 - mae: 16.3999 - val_loss: 375.3551 - val_mae: 16.9854\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 372.4684 - mae: 16.2435 - val_loss: 370.8314 - val_mae: 16.8255\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 367.1596 - mae: 16.0883 - val_loss: 366.1967 - val_mae: 16.6812\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 362.0116 - mae: 15.9390 - val_loss: 361.7666 - val_mae: 16.5386\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 356.9225 - mae: 15.7905 - val_loss: 357.2668 - val_mae: 16.3966\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 352.0585 - mae: 15.6476 - val_loss: 352.8037 - val_mae: 16.2522\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 347.1287 - mae: 15.5004 - val_loss: 348.4719 - val_mae: 16.1098\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 342.3676 - mae: 15.3657 - val_loss: 344.1927 - val_mae: 15.9690\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 337.5764 - mae: 15.2212 - val_loss: 340.0833 - val_mae: 15.8333\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 333.1810 - mae: 15.0907 - val_loss: 335.9035 - val_mae: 15.6945\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 328.6050 - mae: 14.9529 - val_loss: 331.8958 - val_mae: 15.5564\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 324.1525 - mae: 14.8213 - val_loss: 328.0351 - val_mae: 15.4208\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 319.8661 - mae: 14.6894 - val_loss: 324.0096 - val_mae: 15.2861\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 315.5865 - mae: 14.5611 - val_loss: 320.1282 - val_mae: 15.1513\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 311.3834 - mae: 14.4322 - val_loss: 316.3301 - val_mae: 15.0275\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 307.2836 - mae: 14.3056 - val_loss: 312.6538 - val_mae: 14.9063\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 303.3136 - mae: 14.1839 - val_loss: 308.9964 - val_mae: 14.7849\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 299.3494 - mae: 14.0610 - val_loss: 305.4622 - val_mae: 14.6658\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 295.6334 - mae: 13.9419 - val_loss: 301.9205 - val_mae: 14.5450\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 291.7531 - mae: 13.8190 - val_loss: 298.4781 - val_mae: 14.4298\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 288.2092 - mae: 13.7047 - val_loss: 295.0000 - val_mae: 14.3123\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 284.6055 - mae: 13.5865 - val_loss: 291.6225 - val_mae: 14.1952\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 281.1001 - mae: 13.4723 - val_loss: 288.1564 - val_mae: 14.0813\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 277.7343 - mae: 13.3636 - val_loss: 284.9428 - val_mae: 13.9654\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 274.2978 - mae: 13.2465 - val_loss: 281.9158 - val_mae: 13.8527\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 270.9952 - mae: 13.1377 - val_loss: 278.7473 - val_mae: 13.7402\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 267.8730 - mae: 13.0337 - val_loss: 275.6910 - val_mae: 13.6263\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 264.6483 - mae: 12.9224 - val_loss: 272.6741 - val_mae: 13.5156\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 261.5362 - mae: 12.8190 - val_loss: 269.6535 - val_mae: 13.4059\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.4919 - mae: 12.7144 - val_loss: 266.7926 - val_mae: 13.2970\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 255.5627 - mae: 12.6102 - val_loss: 263.7809 - val_mae: 13.1885\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 252.5979 - mae: 12.5069 - val_loss: 260.9135 - val_mae: 13.0813\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 249.7646 - mae: 12.4037 - val_loss: 257.9586 - val_mae: 12.9724\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 246.9744 - mae: 12.3012 - val_loss: 255.3186 - val_mae: 12.8623\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 244.0319 - mae: 12.1967 - val_loss: 252.4830 - val_mae: 12.7561\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 241.3403 - mae: 12.0961 - val_loss: 249.7251 - val_mae: 12.6501\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 238.6069 - mae: 12.0011 - val_loss: 246.9379 - val_mae: 12.5476\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 236.0533 - mae: 11.9057 - val_loss: 244.3657 - val_mae: 12.4441\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 233.3990 - mae: 11.8084 - val_loss: 241.6737 - val_mae: 12.3445\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 230.8059 - mae: 11.7125 - val_loss: 239.1573 - val_mae: 12.2459\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 228.2504 - mae: 11.6203 - val_loss: 236.6015 - val_mae: 12.1485\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 225.8990 - mae: 11.5288 - val_loss: 233.9869 - val_mae: 12.0501\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 223.4315 - mae: 11.4383 - val_loss: 231.6928 - val_mae: 11.9550\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 221.0197 - mae: 11.3464 - val_loss: 229.2692 - val_mae: 11.8603\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 218.6755 - mae: 11.2579 - val_loss: 226.9553 - val_mae: 11.7671\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 216.4369 - mae: 11.1713 - val_loss: 224.5981 - val_mae: 11.6723\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 214.1222 - mae: 11.0845 - val_loss: 222.0942 - val_mae: 11.5792\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 211.8909 - mae: 11.0030 - val_loss: 219.8127 - val_mae: 11.4927\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 209.7510 - mae: 10.9207 - val_loss: 217.4622 - val_mae: 11.4076\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 207.5439 - mae: 10.8364 - val_loss: 215.2366 - val_mae: 11.3347\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 205.4889 - mae: 10.7574 - val_loss: 212.9892 - val_mae: 11.2616\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 203.4524 - mae: 10.6757 - val_loss: 210.8032 - val_mae: 11.1885\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 201.3755 - mae: 10.5939 - val_loss: 208.5949 - val_mae: 11.1156\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 199.4078 - mae: 10.5144 - val_loss: 206.3419 - val_mae: 11.0419\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 197.4349 - mae: 10.4348 - val_loss: 204.2493 - val_mae: 10.9703\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 195.3990 - mae: 10.3543 - val_loss: 202.1959 - val_mae: 10.9014\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 193.5685 - mae: 10.2817 - val_loss: 199.9507 - val_mae: 10.8282\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 191.6362 - mae: 10.2052 - val_loss: 197.8319 - val_mae: 10.7580\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 189.7753 - mae: 10.1334 - val_loss: 195.7882 - val_mae: 10.6876\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 187.9051 - mae: 10.0604 - val_loss: 193.9324 - val_mae: 10.6195\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 186.1582 - mae: 9.9883 - val_loss: 191.8040 - val_mae: 10.5476\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 184.3068 - mae: 9.9167 - val_loss: 189.8123 - val_mae: 10.4791\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 182.6816 - mae: 9.8520 - val_loss: 188.0474 - val_mae: 10.4103\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.8806 - mae: 9.7824 - val_loss: 186.0412 - val_mae: 10.3423\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 179.1634 - mae: 9.7204 - val_loss: 184.0947 - val_mae: 10.2752\n",
      "5/5 [==============================] - 0s 788us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1355.3384 - mae: 35.6931 - val_loss: 1077.0571 - val_mae: 32.5774\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1342.1941 - mae: 35.5091 - val_loss: 1065.7076 - val_mae: 32.4032\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1329.2268 - mae: 35.3279 - val_loss: 1054.4996 - val_mae: 32.2302\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1316.5704 - mae: 35.1497 - val_loss: 1043.2966 - val_mae: 32.0565\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1303.7856 - mae: 34.9693 - val_loss: 1032.3131 - val_mae: 31.8852\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1291.5935 - mae: 34.7944 - val_loss: 1021.4328 - val_mae: 31.7145\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1279.4174 - mae: 34.6206 - val_loss: 1010.8279 - val_mae: 31.5468\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1267.5834 - mae: 34.4525 - val_loss: 1000.2820 - val_mae: 31.3790\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1255.7286 - mae: 34.2799 - val_loss: 989.8999 - val_mae: 31.2130\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1244.1451 - mae: 34.1117 - val_loss: 979.5738 - val_mae: 31.0470\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1232.5909 - mae: 33.9432 - val_loss: 969.3874 - val_mae: 30.8825\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1221.0739 - mae: 33.7754 - val_loss: 959.4208 - val_mae: 30.7207\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1209.8298 - mae: 33.6100 - val_loss: 949.5348 - val_mae: 30.5593\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1198.8015 - mae: 33.4457 - val_loss: 939.6830 - val_mae: 30.3977\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1187.5513 - mae: 33.2796 - val_loss: 930.0512 - val_mae: 30.2390\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1176.5492 - mae: 33.1133 - val_loss: 920.5298 - val_mae: 30.0812\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1165.5455 - mae: 32.9493 - val_loss: 911.1039 - val_mae: 29.9241\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1154.5247 - mae: 32.7829 - val_loss: 901.7101 - val_mae: 29.7666\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1143.4382 - mae: 32.6165 - val_loss: 892.5079 - val_mae: 29.6119\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1132.5940 - mae: 32.4499 - val_loss: 883.3614 - val_mae: 29.4572\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1121.8635 - mae: 32.2827 - val_loss: 874.1879 - val_mae: 29.3012\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1110.8785 - mae: 32.1148 - val_loss: 865.1647 - val_mae: 29.1467\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1100.3129 - mae: 31.9508 - val_loss: 856.0479 - val_mae: 28.9896\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1089.8339 - mae: 31.7864 - val_loss: 846.8802 - val_mae: 28.8303\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1079.2778 - mae: 31.6205 - val_loss: 837.6347 - val_mae: 28.6689\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1068.7462 - mae: 31.4548 - val_loss: 828.5847 - val_mae: 28.5096\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1058.2467 - mae: 31.2886 - val_loss: 819.6591 - val_mae: 28.3513\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1047.8688 - mae: 31.1238 - val_loss: 810.7598 - val_mae: 28.1925\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1037.4435 - mae: 30.9566 - val_loss: 801.8881 - val_mae: 28.0330\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1027.1793 - mae: 30.7910 - val_loss: 793.0001 - val_mae: 27.8721\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1016.8446 - mae: 30.6228 - val_loss: 784.2079 - val_mae: 27.7119\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1006.6982 - mae: 30.4579 - val_loss: 775.5229 - val_mae: 27.5526\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 996.6042 - mae: 30.2923 - val_loss: 766.8961 - val_mae: 27.3936\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 986.5236 - mae: 30.1270 - val_loss: 758.2534 - val_mae: 27.2337\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 976.5193 - mae: 29.9605 - val_loss: 749.4163 - val_mae: 27.0695\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 966.3950 - mae: 29.7908 - val_loss: 740.5796 - val_mae: 26.9043\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 956.3879 - mae: 29.6230 - val_loss: 731.7590 - val_mae: 26.7382\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 946.2976 - mae: 29.4528 - val_loss: 723.1124 - val_mae: 26.5743\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 936.4744 - mae: 29.2850 - val_loss: 714.5125 - val_mae: 26.4103\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 926.5151 - mae: 29.1172 - val_loss: 706.0487 - val_mae: 26.2479\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 916.9410 - mae: 28.9494 - val_loss: 697.5738 - val_mae: 26.0841\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 907.1076 - mae: 28.7800 - val_loss: 689.2939 - val_mae: 25.9232\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 897.3911 - mae: 28.6115 - val_loss: 681.1933 - val_mae: 25.7648\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 887.8171 - mae: 28.4437 - val_loss: 673.0370 - val_mae: 25.6043\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 878.2221 - mae: 28.2713 - val_loss: 664.9074 - val_mae: 25.4434\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 868.3763 - mae: 28.0998 - val_loss: 656.8284 - val_mae: 25.2824\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 858.7471 - mae: 27.9269 - val_loss: 648.5519 - val_mae: 25.1171\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 849.0159 - mae: 27.7511 - val_loss: 640.3282 - val_mae: 24.9518\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 839.3311 - mae: 27.5779 - val_loss: 632.0895 - val_mae: 24.7850\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 829.5639 - mae: 27.3990 - val_loss: 623.5515 - val_mae: 24.6118\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 819.6818 - mae: 27.2171 - val_loss: 615.0245 - val_mae: 24.4378\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 809.7053 - mae: 27.0335 - val_loss: 606.4582 - val_mae: 24.2617\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 799.7470 - mae: 26.8488 - val_loss: 597.8200 - val_mae: 24.0832\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 789.5951 - mae: 26.6587 - val_loss: 589.0119 - val_mae: 23.9001\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 779.2307 - mae: 26.4655 - val_loss: 580.1163 - val_mae: 23.7141\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 768.6678 - mae: 26.2673 - val_loss: 571.1530 - val_mae: 23.5253\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 758.0397 - mae: 26.0670 - val_loss: 562.1373 - val_mae: 23.3344\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 747.3007 - mae: 25.8617 - val_loss: 552.8008 - val_mae: 23.1360\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 736.3756 - mae: 25.6533 - val_loss: 543.2072 - val_mae: 22.9309\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 725.5704 - mae: 25.4422 - val_loss: 533.4899 - val_mae: 22.7216\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 714.3193 - mae: 25.2217 - val_loss: 523.8914 - val_mae: 22.5130\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 703.2000 - mae: 25.0017 - val_loss: 514.3615 - val_mae: 22.3040\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 692.0505 - mae: 24.7782 - val_loss: 504.8739 - val_mae: 22.0941\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 680.9453 - mae: 24.5565 - val_loss: 495.5496 - val_mae: 21.8858\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 669.9988 - mae: 24.3363 - val_loss: 486.2539 - val_mae: 21.6762\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 659.1302 - mae: 24.1111 - val_loss: 476.9945 - val_mae: 21.4646\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 648.3884 - mae: 23.8871 - val_loss: 467.5414 - val_mae: 21.2461\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 637.3775 - mae: 23.6573 - val_loss: 458.2098 - val_mae: 21.0279\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.6755 - mae: 23.4296 - val_loss: 448.8705 - val_mae: 20.8072\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 615.8464 - mae: 23.2006 - val_loss: 439.7495 - val_mae: 20.5892\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 605.3743 - mae: 22.9713 - val_loss: 430.4434 - val_mae: 20.3637\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 594.6605 - mae: 22.7410 - val_loss: 421.3569 - val_mae: 20.1410\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 584.2650 - mae: 22.5121 - val_loss: 412.3469 - val_mae: 19.9176\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 573.9005 - mae: 22.2828 - val_loss: 403.4739 - val_mae: 19.6951\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 563.5546 - mae: 22.0499 - val_loss: 394.7395 - val_mae: 19.4735\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 553.2194 - mae: 21.8184 - val_loss: 386.0896 - val_mae: 19.2516\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 543.2269 - mae: 21.5846 - val_loss: 377.4693 - val_mae: 19.0280\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 533.3167 - mae: 21.3504 - val_loss: 368.9198 - val_mae: 18.8036\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 523.1860 - mae: 21.1154 - val_loss: 360.6809 - val_mae: 18.5847\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 513.3780 - mae: 20.8815 - val_loss: 352.4774 - val_mae: 18.3642\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 503.4365 - mae: 20.6438 - val_loss: 344.4216 - val_mae: 18.1449\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 493.6685 - mae: 20.4105 - val_loss: 336.5553 - val_mae: 17.9283\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 484.0876 - mae: 20.1775 - val_loss: 328.5463 - val_mae: 17.7050\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 474.6172 - mae: 19.9417 - val_loss: 320.3997 - val_mae: 17.4751\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 465.0420 - mae: 19.7041 - val_loss: 312.4252 - val_mae: 17.2467\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 455.7079 - mae: 19.4664 - val_loss: 304.5135 - val_mae: 17.0165\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 446.4390 - mae: 19.2277 - val_loss: 296.7596 - val_mae: 16.7879\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 437.1818 - mae: 18.9891 - val_loss: 288.8899 - val_mae: 16.5532\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 428.2665 - mae: 18.7494 - val_loss: 281.0188 - val_mae: 16.3155\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 419.3585 - mae: 18.5106 - val_loss: 273.3723 - val_mae: 16.0807\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 410.6698 - mae: 18.2766 - val_loss: 265.9840 - val_mae: 15.8505\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 402.1300 - mae: 18.0445 - val_loss: 258.7787 - val_mae: 15.6229\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 393.7357 - mae: 17.8120 - val_loss: 251.7928 - val_mae: 15.3988\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 385.5693 - mae: 17.5842 - val_loss: 244.9059 - val_mae: 15.1742\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 377.6787 - mae: 17.3542 - val_loss: 238.1377 - val_mae: 14.9503\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 369.7647 - mae: 17.1272 - val_loss: 231.6254 - val_mae: 14.7317\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 362.1114 - mae: 16.9042 - val_loss: 225.2910 - val_mae: 14.5158\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 354.7580 - mae: 16.6802 - val_loss: 218.9650 - val_mae: 14.2965\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 347.2943 - mae: 16.4569 - val_loss: 212.8757 - val_mae: 14.0823\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 340.0254 - mae: 16.2371 - val_loss: 207.0111 - val_mae: 13.8725\n",
      "5/5 [==============================] - 0s 759us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 19277.0938 - mae: 123.4359 - val_loss: 19230.9199 - val_mae: 124.7778\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19135.6562 - mae: 122.9195 - val_loss: 19087.1113 - val_mae: 124.2493\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19000.0312 - mae: 122.4098 - val_loss: 18942.3984 - val_mae: 123.7146\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18862.8398 - mae: 121.8944 - val_loss: 18799.3535 - val_mae: 123.1834\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18727.9297 - mae: 121.3845 - val_loss: 18656.7930 - val_mae: 122.6522\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18590.1914 - mae: 120.8735 - val_loss: 18517.0195 - val_mae: 122.1284\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18456.0176 - mae: 120.3687 - val_loss: 18378.2305 - val_mae: 121.6060\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18324.5996 - mae: 119.8651 - val_loss: 18239.7539 - val_mae: 121.0836\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18190.6562 - mae: 119.3608 - val_loss: 18103.6094 - val_mae: 120.5666\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18061.4668 - mae: 118.8611 - val_loss: 17967.1914 - val_mae: 120.0508\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17929.9199 - mae: 118.3598 - val_loss: 17833.2871 - val_mae: 119.5419\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17801.8281 - mae: 117.8633 - val_loss: 17700.8340 - val_mae: 119.0360\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17676.4395 - mae: 117.3710 - val_loss: 17567.4688 - val_mae: 118.5257\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17547.3203 - mae: 116.8753 - val_loss: 17437.0332 - val_mae: 118.0235\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17420.9277 - mae: 116.3891 - val_loss: 17307.8984 - val_mae: 117.5234\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17295.5879 - mae: 115.9032 - val_loss: 17180.7910 - val_mae: 117.0295\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17172.0859 - mae: 115.4234 - val_loss: 17054.4453 - val_mae: 116.5407\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17050.0820 - mae: 114.9424 - val_loss: 16927.1152 - val_mae: 116.0463\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16927.6270 - mae: 114.4601 - val_loss: 16799.9004 - val_mae: 115.5500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16805.5605 - mae: 113.9764 - val_loss: 16673.8379 - val_mae: 115.0570\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16682.1836 - mae: 113.4950 - val_loss: 16550.1426 - val_mae: 114.5704\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16563.6230 - mae: 113.0197 - val_loss: 16426.8965 - val_mae: 114.0841\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16442.3418 - mae: 112.5421 - val_loss: 16305.3818 - val_mae: 113.6021\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16324.0664 - mae: 112.0690 - val_loss: 16182.8516 - val_mae: 113.1144\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16204.6875 - mae: 111.5950 - val_loss: 16062.3975 - val_mae: 112.6329\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16086.3926 - mae: 111.1241 - val_loss: 15942.7383 - val_mae: 112.1533\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15972.0283 - mae: 110.6574 - val_loss: 15821.1680 - val_mae: 111.6653\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15856.4648 - mae: 110.1855 - val_loss: 15699.8193 - val_mae: 111.1756\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15739.2197 - mae: 109.7142 - val_loss: 15581.5127 - val_mae: 110.6961\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15625.4141 - mae: 109.2528 - val_loss: 15464.1504 - val_mae: 110.2194\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15513.6406 - mae: 108.7933 - val_loss: 15345.5615 - val_mae: 109.7357\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15402.1318 - mae: 108.3282 - val_loss: 15227.9619 - val_mae: 109.2543\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15288.6787 - mae: 107.8654 - val_loss: 15112.4648 - val_mae: 108.7788\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15177.7773 - mae: 107.4054 - val_loss: 14997.4814 - val_mae: 108.3046\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15067.8398 - mae: 106.9480 - val_loss: 14883.7324 - val_mae: 107.8334\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14957.1953 - mae: 106.4921 - val_loss: 14771.8369 - val_mae: 107.3671\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14847.9805 - mae: 106.0395 - val_loss: 14660.7510 - val_mae: 106.9036\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14741.5889 - mae: 105.5901 - val_loss: 14548.0635 - val_mae: 106.4318\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14635.9834 - mae: 105.1386 - val_loss: 14435.2822 - val_mae: 105.9585\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14528.8096 - mae: 104.6844 - val_loss: 14325.5547 - val_mae: 105.4993\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14422.2988 - mae: 104.2407 - val_loss: 14218.7295 - val_mae: 105.0487\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14318.4072 - mae: 103.8007 - val_loss: 14111.5732 - val_mae: 104.5965\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14217.7588 - mae: 103.3647 - val_loss: 14003.0674 - val_mae: 104.1381\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14114.6426 - mae: 102.9242 - val_loss: 13896.1924 - val_mae: 103.6839\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14013.8828 - mae: 102.4876 - val_loss: 13789.8711 - val_mae: 103.2304\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13911.2090 - mae: 102.0526 - val_loss: 13686.4102 - val_mae: 102.7867\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13813.8574 - mae: 101.6239 - val_loss: 13581.2461 - val_mae: 102.3347\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13714.7852 - mae: 101.1913 - val_loss: 13476.9551 - val_mae: 101.8845\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13617.6729 - mae: 100.7617 - val_loss: 13372.7578 - val_mae: 101.4324\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13516.2988 - mae: 100.3313 - val_loss: 13272.3838 - val_mae: 100.9940\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13420.5205 - mae: 99.9086 - val_loss: 13171.2578 - val_mae: 100.5511\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13323.3604 - mae: 99.4840 - val_loss: 13070.6445 - val_mae: 100.1091\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13229.5820 - mae: 99.0621 - val_loss: 12969.0137 - val_mae: 99.6607\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13133.3809 - mae: 98.6351 - val_loss: 12870.3066 - val_mae: 99.2235\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13038.5938 - mae: 98.2171 - val_loss: 12773.8916 - val_mae: 98.7937\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12947.6338 - mae: 97.8039 - val_loss: 12676.0225 - val_mae: 98.3561\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12853.9922 - mae: 97.3870 - val_loss: 12579.4688 - val_mae: 97.9225\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12762.2559 - mae: 96.9732 - val_loss: 12483.9854 - val_mae: 97.4916\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12671.1240 - mae: 96.5622 - val_loss: 12390.2695 - val_mae: 97.0659\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12581.1357 - mae: 96.1553 - val_loss: 12296.8486 - val_mae: 96.6400\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12493.9922 - mae: 95.7471 - val_loss: 12201.9668 - val_mae: 96.2068\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12401.6025 - mae: 95.3359 - val_loss: 12110.6260 - val_mae: 95.7858\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12313.6562 - mae: 94.9328 - val_loss: 12019.1826 - val_mae: 95.3642\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12228.3047 - mae: 94.5303 - val_loss: 11925.7002 - val_mae: 94.9356\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12136.7861 - mae: 94.1188 - val_loss: 11836.1016 - val_mae: 94.5220\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12051.3486 - mae: 93.7184 - val_loss: 11745.4775 - val_mae: 94.1022\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11965.0352 - mae: 93.3151 - val_loss: 11655.9883 - val_mae: 93.6857\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11879.8623 - mae: 92.9151 - val_loss: 11567.1260 - val_mae: 93.2725\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11794.2881 - mae: 92.5148 - val_loss: 11479.1709 - val_mae: 92.8627\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11710.9150 - mae: 92.1179 - val_loss: 11392.4727 - val_mae: 92.4568\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11628.1875 - mae: 91.7221 - val_loss: 11305.8398 - val_mae: 92.0496\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11544.0918 - mae: 91.3260 - val_loss: 11221.3857 - val_mae: 91.6497\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11463.1318 - mae: 90.9359 - val_loss: 11136.2236 - val_mae: 91.2446\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11379.7334 - mae: 90.5400 - val_loss: 11052.0645 - val_mae: 90.8431\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11299.5488 - mae: 90.1480 - val_loss: 10967.0889 - val_mae: 90.4366\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11217.2871 - mae: 89.7531 - val_loss: 10883.8242 - val_mae: 90.0352\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11136.6611 - mae: 89.3621 - val_loss: 10801.7959 - val_mae: 89.6389\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11057.3193 - mae: 88.9743 - val_loss: 10719.9639 - val_mae: 89.2414\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10978.0508 - mae: 88.5870 - val_loss: 10639.0391 - val_mae: 88.8469\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10900.6895 - mae: 88.1997 - val_loss: 10556.8994 - val_mae: 88.4450\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10823.0029 - mae: 87.8097 - val_loss: 10475.2617 - val_mae: 88.0436\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10743.2334 - mae: 87.4214 - val_loss: 10396.7041 - val_mae: 87.6533\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10667.3252 - mae: 87.0400 - val_loss: 10319.1064 - val_mae: 87.2681\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10592.1113 - mae: 86.6612 - val_loss: 10242.5127 - val_mae: 86.8877\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10515.4912 - mae: 86.2848 - val_loss: 10167.1436 - val_mae: 86.5109\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10442.3057 - mae: 85.9103 - val_loss: 10090.6709 - val_mae: 86.1278\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10369.0410 - mae: 85.5332 - val_loss: 10013.1211 - val_mae: 85.7383\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10293.9932 - mae: 85.1533 - val_loss: 9937.4980 - val_mae: 85.3566\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10220.6953 - mae: 84.7798 - val_loss: 9863.0518 - val_mae: 84.9792\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10147.6436 - mae: 84.4083 - val_loss: 9788.7363 - val_mae: 84.6009\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10076.1914 - mae: 84.0390 - val_loss: 9714.3242 - val_mae: 84.2208\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10004.7158 - mae: 83.6686 - val_loss: 9641.0820 - val_mae: 83.8446\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9934.0840 - mae: 83.3039 - val_loss: 9569.0029 - val_mae: 83.4722\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9862.2842 - mae: 82.9414 - val_loss: 9499.0957 - val_mae: 83.1098\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9795.6377 - mae: 82.5852 - val_loss: 9426.1943 - val_mae: 82.7335\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9724.3545 - mae: 82.2214 - val_loss: 9355.9395 - val_mae: 82.3680\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9658.1729 - mae: 81.8658 - val_loss: 9285.1641 - val_mae: 81.9996\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9587.3604 - mae: 81.5050 - val_loss: 9217.2725 - val_mae: 81.6429\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9521.9648 - mae: 81.1552 - val_loss: 9148.4648 - val_mae: 81.2804\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9454.1211 - mae: 80.8035 - val_loss: 9081.8271 - val_mae: 80.9267\n",
      "5/5 [==============================] - 0s 709us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 482.9195 - mae: 13.9686 - val_loss: 38.2165 - val_mae: 5.1372\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.7078 - mae: 7.2503 - val_loss: 35.8184 - val_mae: 5.0959\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.4174 - mae: 7.2036 - val_loss: 34.7824 - val_mae: 5.0679\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.3315 - mae: 7.1219 - val_loss: 45.7289 - val_mae: 5.2568\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.3220 - mae: 7.2155 - val_loss: 35.6933 - val_mae: 4.9832\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.9887 - mae: 7.1145 - val_loss: 33.1470 - val_mae: 4.9117\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.5550 - mae: 6.9827 - val_loss: 34.3002 - val_mae: 4.8978\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.9282 - mae: 7.0912 - val_loss: 37.0235 - val_mae: 4.9042\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.2655 - mae: 6.8626 - val_loss: 47.3335 - val_mae: 5.1503\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.9511 - mae: 7.0350 - val_loss: 33.6428 - val_mae: 4.7330\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.3775 - mae: 6.9274 - val_loss: 38.5500 - val_mae: 4.8555\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.4328 - mae: 6.9978 - val_loss: 31.5009 - val_mae: 4.5815\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.3554 - mae: 6.8310 - val_loss: 39.9744 - val_mae: 4.8036\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.3459 - mae: 7.0514 - val_loss: 29.9625 - val_mae: 4.4422\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.7753 - mae: 6.7907 - val_loss: 28.1850 - val_mae: 4.4311\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.5518 - mae: 6.9294 - val_loss: 27.9675 - val_mae: 4.5050\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.9394 - mae: 6.7817 - val_loss: 29.5087 - val_mae: 4.3435\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.0867 - mae: 6.7169 - val_loss: 32.3021 - val_mae: 4.3669\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.3740 - mae: 6.6492 - val_loss: 33.6513 - val_mae: 4.3941\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.8485 - mae: 6.6525 - val_loss: 32.7680 - val_mae: 4.3414\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.6909 - mae: 6.7188 - val_loss: 28.9644 - val_mae: 4.1660\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.0198 - mae: 6.6792 - val_loss: 24.5844 - val_mae: 4.0621\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.3294 - mae: 6.3779 - val_loss: 34.2209 - val_mae: 4.4082\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.6452 - mae: 6.6045 - val_loss: 26.4894 - val_mae: 4.0006\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.1840 - mae: 6.4765 - val_loss: 23.2634 - val_mae: 3.9350\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.7534 - mae: 6.2826 - val_loss: 27.8339 - val_mae: 4.1616\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4816 - mae: 6.3875 - val_loss: 24.0908 - val_mae: 3.9472\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.2438 - mae: 6.4523 - val_loss: 21.8234 - val_mae: 3.8311\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.5819 - mae: 6.1793 - val_loss: 27.3633 - val_mae: 4.2451\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.9682 - mae: 6.3559 - val_loss: 20.3992 - val_mae: 3.7438\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.6808 - mae: 6.2225 - val_loss: 20.0004 - val_mae: 3.7026\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.9459 - mae: 5.9658 - val_loss: 27.3090 - val_mae: 4.3198\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.4529 - mae: 6.3505 - val_loss: 24.5284 - val_mae: 4.1116\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.6527 - mae: 6.1199 - val_loss: 25.7662 - val_mae: 4.2248\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.0722 - mae: 6.1605 - val_loss: 23.9646 - val_mae: 4.0749\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.6660 - mae: 6.2166 - val_loss: 20.4773 - val_mae: 3.7220\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.3837 - mae: 6.1317 - val_loss: 20.2871 - val_mae: 3.6925\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.2380 - mae: 6.1592 - val_loss: 19.0900 - val_mae: 3.5116\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.2479 - mae: 6.0162 - val_loss: 24.5839 - val_mae: 4.1295\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.3674 - mae: 6.1285 - val_loss: 28.8201 - val_mae: 4.4571\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.0883 - mae: 6.3104 - val_loss: 20.0434 - val_mae: 3.6257\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.4461 - mae: 6.0606 - val_loss: 20.6512 - val_mae: 3.7055\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.8697 - mae: 6.1111 - val_loss: 19.7588 - val_mae: 3.5684\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.8470 - mae: 6.0754 - val_loss: 21.8719 - val_mae: 3.8395\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.5132 - mae: 6.1695 - val_loss: 19.3266 - val_mae: 3.5024\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.7400 - mae: 6.0860 - val_loss: 18.9447 - val_mae: 3.4513\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.8711 - mae: 6.0567 - val_loss: 21.2162 - val_mae: 3.7849\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.6446 - mae: 6.0615 - val_loss: 24.3127 - val_mae: 4.1079\n",
      "Epoch 00048: early stopping\n",
      "5/5 [==============================] - 0s 793us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=3, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 662.7577 - mae: 23.7008 - val_loss: 449.1480 - val_mae: 20.2740\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 637.0978 - mae: 23.1178 - val_loss: 448.9986 - val_mae: 20.2703\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 636.9273 - mae: 23.1141 - val_loss: 448.8477 - val_mae: 20.2666\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 636.7553 - mae: 23.1104 - val_loss: 448.6972 - val_mae: 20.2629\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 636.5838 - mae: 23.1067 - val_loss: 448.5475 - val_mae: 20.2592\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 636.4131 - mae: 23.1030 - val_loss: 448.3986 - val_mae: 20.2555\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 636.2433 - mae: 23.0993 - val_loss: 448.2498 - val_mae: 20.2518\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 636.0736 - mae: 23.0957 - val_loss: 448.1000 - val_mae: 20.2481\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.9026 - mae: 23.0920 - val_loss: 447.9493 - val_mae: 20.2444\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.7309 - mae: 23.0882 - val_loss: 447.7990 - val_mae: 20.2407\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.5595 - mae: 23.0845 - val_loss: 447.6502 - val_mae: 20.2370\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.3897 - mae: 23.0809 - val_loss: 447.5001 - val_mae: 20.2333\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.2186 - mae: 23.0772 - val_loss: 447.3503 - val_mae: 20.2296\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.0479 - mae: 23.0735 - val_loss: 447.2014 - val_mae: 20.2259\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.8779 - mae: 23.0698 - val_loss: 447.0514 - val_mae: 20.2222\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.7068 - mae: 23.0660 - val_loss: 446.9022 - val_mae: 20.2185\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.5366 - mae: 23.0624 - val_loss: 446.7518 - val_mae: 20.2148\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.3651 - mae: 23.0587 - val_loss: 446.6028 - val_mae: 20.2111\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.1951 - mae: 23.0550 - val_loss: 446.4529 - val_mae: 20.2074\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.0242 - mae: 23.0513 - val_loss: 446.3036 - val_mae: 20.2037\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.8539 - mae: 23.0476 - val_loss: 446.1546 - val_mae: 20.2000\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.6840 - mae: 23.0439 - val_loss: 446.0061 - val_mae: 20.1964\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.5145 - mae: 23.0402 - val_loss: 445.8568 - val_mae: 20.1927\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.3441 - mae: 23.0365 - val_loss: 445.7064 - val_mae: 20.1889\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.1726 - mae: 23.0328 - val_loss: 445.5577 - val_mae: 20.1853\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.0030 - mae: 23.0291 - val_loss: 445.4084 - val_mae: 20.1816\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.8329 - mae: 23.0254 - val_loss: 445.2615 - val_mae: 20.1779\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.6652 - mae: 23.0218 - val_loss: 445.1135 - val_mae: 20.1743\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.4963 - mae: 23.0181 - val_loss: 444.9653 - val_mae: 20.1706\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.3271 - mae: 23.0144 - val_loss: 444.8168 - val_mae: 20.1669\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.1580 - mae: 23.0107 - val_loss: 444.6699 - val_mae: 20.1633\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.9900 - mae: 23.0071 - val_loss: 444.5199 - val_mae: 20.1595\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.8190 - mae: 23.0034 - val_loss: 444.3713 - val_mae: 20.1559\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.6494 - mae: 22.9997 - val_loss: 444.2229 - val_mae: 20.1522\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.4800 - mae: 22.9961 - val_loss: 444.0736 - val_mae: 20.1485\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.3098 - mae: 22.9923 - val_loss: 443.9251 - val_mae: 20.1448\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 582.6096 - mae: 21.99 - 0s 3ms/step - loss: 631.1404 - mae: 22.9886 - val_loss: 443.7779 - val_mae: 20.1411\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.9724 - mae: 22.9850 - val_loss: 443.6306 - val_mae: 20.1375\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.8042 - mae: 22.9813 - val_loss: 443.4825 - val_mae: 20.1338\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.6352 - mae: 22.9777 - val_loss: 443.3341 - val_mae: 20.1301\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.4660 - mae: 22.9739 - val_loss: 443.1862 - val_mae: 20.1264\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.2971 - mae: 22.9703 - val_loss: 443.0385 - val_mae: 20.1228\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.1285 - mae: 22.9666 - val_loss: 442.8906 - val_mae: 20.1191\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.9597 - mae: 22.9630 - val_loss: 442.7424 - val_mae: 20.1154\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.7906 - mae: 22.9592 - val_loss: 442.5948 - val_mae: 20.1117\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.6223 - mae: 22.9556 - val_loss: 442.4481 - val_mae: 20.1081\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.4548 - mae: 22.9519 - val_loss: 442.3016 - val_mae: 20.1044\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.2875 - mae: 22.9483 - val_loss: 442.1534 - val_mae: 20.1008\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.1182 - mae: 22.9446 - val_loss: 442.0045 - val_mae: 20.0971\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.9485 - mae: 22.9409 - val_loss: 441.8580 - val_mae: 20.0934\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.7811 - mae: 22.9373 - val_loss: 441.7104 - val_mae: 20.0897\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.6128 - mae: 22.9336 - val_loss: 441.5629 - val_mae: 20.0861\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.4443 - mae: 22.9299 - val_loss: 441.4154 - val_mae: 20.0824\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.2761 - mae: 22.9262 - val_loss: 441.2693 - val_mae: 20.0788\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.1090 - mae: 22.9226 - val_loss: 441.1211 - val_mae: 20.0751\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.9399 - mae: 22.9189 - val_loss: 440.9734 - val_mae: 20.0714\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.7714 - mae: 22.9153 - val_loss: 440.8264 - val_mae: 20.0677\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.6036 - mae: 22.9116 - val_loss: 440.6796 - val_mae: 20.0641\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.4359 - mae: 22.9080 - val_loss: 440.5314 - val_mae: 20.0604\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.2667 - mae: 22.9042 - val_loss: 440.3835 - val_mae: 20.0567\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.0981 - mae: 22.9005 - val_loss: 440.2372 - val_mae: 20.0530\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.9309 - mae: 22.8969 - val_loss: 440.0909 - val_mae: 20.0494\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.7639 - mae: 22.8932 - val_loss: 439.9444 - val_mae: 20.0457\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.5964 - mae: 22.8896 - val_loss: 439.7977 - val_mae: 20.0421\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.4293 - mae: 22.8860 - val_loss: 439.6519 - val_mae: 20.0384\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.2625 - mae: 22.8823 - val_loss: 439.5047 - val_mae: 20.0348\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.0946 - mae: 22.8786 - val_loss: 439.3593 - val_mae: 20.0311\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.9283 - mae: 22.8750 - val_loss: 439.2117 - val_mae: 20.0275\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.7599 - mae: 22.8713 - val_loss: 439.0652 - val_mae: 20.0238\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.5925 - mae: 22.8677 - val_loss: 438.9187 - val_mae: 20.0201\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.4254 - mae: 22.8639 - val_loss: 438.7735 - val_mae: 20.0165\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.2594 - mae: 22.8604 - val_loss: 438.6267 - val_mae: 20.0128\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.0919 - mae: 22.8567 - val_loss: 438.4800 - val_mae: 20.0092\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.9243 - mae: 22.8530 - val_loss: 438.3329 - val_mae: 20.0055\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.7562 - mae: 22.8493 - val_loss: 438.1861 - val_mae: 20.0018\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.5889 - mae: 22.8458 - val_loss: 438.0408 - val_mae: 19.9982\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.4226 - mae: 22.8421 - val_loss: 437.8931 - val_mae: 19.9945\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.2542 - mae: 22.8383 - val_loss: 437.7482 - val_mae: 19.9909\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.0886 - mae: 22.8347 - val_loss: 437.6022 - val_mae: 19.9872\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.9218 - mae: 22.8310 - val_loss: 437.4558 - val_mae: 19.9836\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.7546 - mae: 22.8274 - val_loss: 437.3110 - val_mae: 19.9799\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.5892 - mae: 22.8238 - val_loss: 437.1652 - val_mae: 19.9763\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.4226 - mae: 22.8202 - val_loss: 437.0184 - val_mae: 19.9726\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.2549 - mae: 22.8165 - val_loss: 436.8732 - val_mae: 19.9690\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.0891 - mae: 22.8129 - val_loss: 436.7275 - val_mae: 19.9653\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.9227 - mae: 22.8092 - val_loss: 436.5824 - val_mae: 19.9617\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.7568 - mae: 22.8056 - val_loss: 436.4360 - val_mae: 19.9580\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.5894 - mae: 22.8019 - val_loss: 436.2891 - val_mae: 19.9544\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.4218 - mae: 22.7982 - val_loss: 436.1431 - val_mae: 19.9507\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.2551 - mae: 22.7945 - val_loss: 435.9986 - val_mae: 19.9471\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.0899 - mae: 22.7909 - val_loss: 435.8527 - val_mae: 19.9434\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.9233 - mae: 22.7873 - val_loss: 435.7076 - val_mae: 19.9398\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.7573 - mae: 22.7837 - val_loss: 435.5610 - val_mae: 19.9361\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.5899 - mae: 22.7800 - val_loss: 435.4160 - val_mae: 19.9325\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.4243 - mae: 22.7763 - val_loss: 435.2711 - val_mae: 19.9288\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.2587 - mae: 22.7727 - val_loss: 435.1262 - val_mae: 19.9252\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.0930 - mae: 22.7691 - val_loss: 434.9797 - val_mae: 19.9215\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.9258 - mae: 22.7653 - val_loss: 434.8350 - val_mae: 19.9179\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.7604 - mae: 22.7617 - val_loss: 434.6897 - val_mae: 19.9142\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.5944 - mae: 22.7581 - val_loss: 434.5457 - val_mae: 19.9106\n",
      "5/5 [==============================] - 0s 762us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=3, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1394.7983 - mae: 28.4315 - val_loss: 397.3397 - val_mae: 18.3456\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 260.7632 - mae: 13.6028 - val_loss: 118.2157 - val_mae: 8.5417\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.7934 - mae: 8.3156 - val_loss: 91.9855 - val_mae: 7.8758\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.2393 - mae: 7.7428 - val_loss: 86.2046 - val_mae: 7.6923\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.4014 - mae: 7.7673 - val_loss: 88.0335 - val_mae: 7.5123\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.9588 - mae: 7.5568 - val_loss: 83.8034 - val_mae: 7.3570\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.3632 - mae: 7.4770 - val_loss: 79.4089 - val_mae: 7.3446\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.9114 - mae: 7.6031 - val_loss: 84.9246 - val_mae: 7.1451\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.8630 - mae: 7.3315 - val_loss: 78.8101 - val_mae: 7.1451\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.6512 - mae: 7.3678 - val_loss: 78.9780 - val_mae: 7.0898\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.5910 - mae: 7.2281 - val_loss: 76.4379 - val_mae: 7.1150\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.4777 - mae: 7.3899 - val_loss: 77.9753 - val_mae: 7.0166\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.2148 - mae: 7.2173 - val_loss: 76.0939 - val_mae: 7.0114\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.0267 - mae: 7.2512 - val_loss: 75.3070 - val_mae: 6.9925\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.2297 - mae: 7.3028 - val_loss: 77.9499 - val_mae: 6.8923\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.8153 - mae: 7.2318 - val_loss: 74.7829 - val_mae: 6.9334\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.1111 - mae: 7.3542 - val_loss: 80.6103 - val_mae: 6.8413\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.3866 - mae: 7.1936 - val_loss: 75.7711 - val_mae: 6.8476\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.2503 - mae: 7.2264 - val_loss: 75.2648 - val_mae: 6.8300\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.1645 - mae: 7.1688 - val_loss: 74.6387 - val_mae: 6.8200\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.7250 - mae: 7.2270 - val_loss: 75.2111 - val_mae: 6.7865\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.4025 - mae: 7.1064 - val_loss: 73.3843 - val_mae: 6.8303\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.9859 - mae: 7.1938 - val_loss: 73.2067 - val_mae: 6.8120\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.1360 - mae: 7.1139 - val_loss: 73.6111 - val_mae: 6.9032\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.7039 - mae: 7.2743 - val_loss: 73.1001 - val_mae: 6.7490\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.1976 - mae: 7.0747 - val_loss: 73.1708 - val_mae: 6.8543\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.5816 - mae: 7.2887 - val_loss: 73.3738 - val_mae: 6.6996\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.9738 - mae: 7.2089 - val_loss: 73.3793 - val_mae: 6.6827\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.0545 - mae: 7.1250 - val_loss: 74.2356 - val_mae: 6.6517\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.1884 - mae: 7.0874 - val_loss: 72.4206 - val_mae: 6.6781\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6949 - mae: 7.1084 - val_loss: 72.5448 - val_mae: 6.6526\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6430 - mae: 7.0167 - val_loss: 72.6729 - val_mae: 6.7880\n",
      "Epoch 00032: early stopping\n",
      "5/5 [==============================] - 0s 712us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=3, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 21275.4746 - mae: 72.6838 - val_loss: 451.0757 - val_mae: 20.3215\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 600.0198 - mae: 22.7322 - val_loss: 449.6021 - val_mae: 20.2852\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 598.3728 - mae: 22.6956 - val_loss: 448.1399 - val_mae: 20.2491\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 596.7363 - mae: 22.6596 - val_loss: 446.6591 - val_mae: 20.2125\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 595.0814 - mae: 22.6231 - val_loss: 445.1998 - val_mae: 20.1764\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 593.4499 - mae: 22.5866 - val_loss: 443.7520 - val_mae: 20.1405\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 591.8292 - mae: 22.5510 - val_loss: 442.3053 - val_mae: 20.1045\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 590.2100 - mae: 22.5154 - val_loss: 440.8561 - val_mae: 20.0685\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 588.5869 - mae: 22.4791 - val_loss: 439.4026 - val_mae: 20.0322\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 586.9609 - mae: 22.4428 - val_loss: 437.9715 - val_mae: 19.9965\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 585.3579 - mae: 22.4073 - val_loss: 436.5365 - val_mae: 19.9606\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 583.7509 - mae: 22.3714 - val_loss: 435.1082 - val_mae: 19.9247\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 582.1503 - mae: 22.3357 - val_loss: 433.6758 - val_mae: 19.8888\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 580.5472 - mae: 22.2995 - val_loss: 432.2599 - val_mae: 19.8531\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 578.9596 - mae: 22.2638 - val_loss: 430.8463 - val_mae: 19.8175\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 577.3769 - mae: 22.2281 - val_loss: 429.4494 - val_mae: 19.7822\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 575.8094 - mae: 22.1933 - val_loss: 428.0453 - val_mae: 19.7467\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 574.2348 - mae: 22.1578 - val_loss: 426.6447 - val_mae: 19.7112\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 572.6645 - mae: 22.1228 - val_loss: 425.2483 - val_mae: 19.6758\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 571.0976 - mae: 22.0870 - val_loss: 423.8529 - val_mae: 19.6403\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 569.5320 - mae: 22.0512 - val_loss: 422.4621 - val_mae: 19.6048\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 567.9728 - mae: 22.0159 - val_loss: 421.0895 - val_mae: 19.5698\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 566.4310 - mae: 21.9812 - val_loss: 419.7098 - val_mae: 19.5345\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 564.8818 - mae: 21.9457 - val_loss: 418.3338 - val_mae: 19.4993\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 563.3371 - mae: 21.9102 - val_loss: 416.9633 - val_mae: 19.4641\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 561.7977 - mae: 21.8752 - val_loss: 415.5975 - val_mae: 19.4290\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 560.2645 - mae: 21.8404 - val_loss: 414.2396 - val_mae: 19.3940\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 558.7391 - mae: 21.8052 - val_loss: 412.8872 - val_mae: 19.3591\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 557.2183 - mae: 21.7706 - val_loss: 411.5321 - val_mae: 19.3241\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 555.6967 - mae: 21.7350 - val_loss: 410.1956 - val_mae: 19.2894\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 554.1944 - mae: 21.7006 - val_loss: 408.8673 - val_mae: 19.2550\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 552.6993 - mae: 21.6663 - val_loss: 407.5221 - val_mae: 19.2200\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 551.1867 - mae: 21.6313 - val_loss: 406.1864 - val_mae: 19.1852\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 549.6840 - mae: 21.5968 - val_loss: 404.8538 - val_mae: 19.1505\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 548.1854 - mae: 21.5620 - val_loss: 403.5291 - val_mae: 19.1159\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 546.6954 - mae: 21.5268 - val_loss: 402.2186 - val_mae: 19.0816\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 545.2202 - mae: 21.4929 - val_loss: 400.9117 - val_mae: 19.0473\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 543.7499 - mae: 21.4588 - val_loss: 399.6172 - val_mae: 19.0133\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 542.2905 - mae: 21.4249 - val_loss: 398.3077 - val_mae: 18.9788\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 540.8165 - mae: 21.3908 - val_loss: 397.0039 - val_mae: 18.9444\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 539.3477 - mae: 21.3557 - val_loss: 395.7103 - val_mae: 18.9102\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 537.8904 - mae: 21.3217 - val_loss: 394.4232 - val_mae: 18.8762\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 536.4406 - mae: 21.2875 - val_loss: 393.1450 - val_mae: 18.8423\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 534.9989 - mae: 21.2539 - val_loss: 391.8625 - val_mae: 18.8082\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 533.5536 - mae: 21.2195 - val_loss: 390.5850 - val_mae: 18.7742\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 532.1130 - mae: 21.1856 - val_loss: 389.3189 - val_mae: 18.7405\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 530.6861 - mae: 21.1520 - val_loss: 388.0579 - val_mae: 18.7068\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 529.2617 - mae: 21.1183 - val_loss: 386.7865 - val_mae: 18.6728\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 527.8275 - mae: 21.0845 - val_loss: 385.5260 - val_mae: 18.6390\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 526.4054 - mae: 21.0508 - val_loss: 384.2735 - val_mae: 18.6054\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 524.9910 - mae: 21.0173 - val_loss: 383.0181 - val_mae: 18.5716\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 523.5745 - mae: 20.9833 - val_loss: 381.7704 - val_mae: 18.5380\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 522.1650 - mae: 20.9498 - val_loss: 380.5230 - val_mae: 18.5043\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 520.7581 - mae: 20.9157 - val_loss: 379.2986 - val_mae: 18.4712\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 519.3732 - mae: 20.8831 - val_loss: 378.0646 - val_mae: 18.4378\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 517.9785 - mae: 20.8496 - val_loss: 376.8327 - val_mae: 18.4043\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 516.5872 - mae: 20.8162 - val_loss: 375.6150 - val_mae: 18.3712\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 515.2097 - mae: 20.7833 - val_loss: 374.3877 - val_mae: 18.3378\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 513.8230 - mae: 20.7495 - val_loss: 373.1750 - val_mae: 18.3047\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 512.4512 - mae: 20.7165 - val_loss: 371.9634 - val_mae: 18.2716\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 511.0817 - mae: 20.6836 - val_loss: 370.7614 - val_mae: 18.2387\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 509.7208 - mae: 20.6502 - val_loss: 369.5600 - val_mae: 18.2057\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 508.3615 - mae: 20.6174 - val_loss: 368.3637 - val_mae: 18.1728\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 507.0066 - mae: 20.5848 - val_loss: 367.1608 - val_mae: 18.1397\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 505.6468 - mae: 20.5519 - val_loss: 365.9818 - val_mae: 18.1072\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 504.3107 - mae: 20.5192 - val_loss: 364.7975 - val_mae: 18.0744\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 502.9691 - mae: 20.4865 - val_loss: 363.6138 - val_mae: 18.0416\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 501.6276 - mae: 20.4537 - val_loss: 362.4258 - val_mae: 18.0087\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 500.2828 - mae: 20.4202 - val_loss: 361.2483 - val_mae: 17.9760\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 498.9476 - mae: 20.3880 - val_loss: 360.0801 - val_mae: 17.9434\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 497.6247 - mae: 20.3551 - val_loss: 358.9260 - val_mae: 17.9113\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.3145 - mae: 20.3236 - val_loss: 357.7579 - val_mae: 17.8786\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 494.9910 - mae: 20.2904 - val_loss: 356.6082 - val_mae: 17.8464\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 493.6854 - mae: 20.2587 - val_loss: 355.4442 - val_mae: 17.8138\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 492.3656 - mae: 20.2259 - val_loss: 354.2956 - val_mae: 17.7815\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 491.0637 - mae: 20.1941 - val_loss: 353.1595 - val_mae: 17.7496\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 489.7723 - mae: 20.1615 - val_loss: 352.0107 - val_mae: 17.7172\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 488.4681 - mae: 20.1290 - val_loss: 350.8704 - val_mae: 17.6850\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 487.1732 - mae: 20.0972 - val_loss: 349.7355 - val_mae: 17.6528\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 485.8844 - mae: 20.0649 - val_loss: 348.5961 - val_mae: 17.6205\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 484.5921 - mae: 20.0324 - val_loss: 347.4832 - val_mae: 17.5889\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 454.0435 - mae: 19.21 - 0s 3ms/step - loss: 483.3251 - mae: 20.0014 - val_loss: 346.3576 - val_mae: 17.5569\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 482.0460 - mae: 19.9691 - val_loss: 345.2352 - val_mae: 17.5249\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 480.7701 - mae: 19.9373 - val_loss: 344.1178 - val_mae: 17.4930\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 479.5005 - mae: 19.9057 - val_loss: 343.0092 - val_mae: 17.4613\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 478.2382 - mae: 19.8739 - val_loss: 341.8932 - val_mae: 17.4293\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 476.9696 - mae: 19.8415 - val_loss: 340.7929 - val_mae: 17.3977\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 475.7168 - mae: 19.8104 - val_loss: 339.6814 - val_mae: 17.3657\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 474.4535 - mae: 19.7784 - val_loss: 338.5910 - val_mae: 17.3343\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 473.2115 - mae: 19.7464 - val_loss: 337.4988 - val_mae: 17.3028\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 471.9678 - mae: 19.7154 - val_loss: 336.4073 - val_mae: 17.2712\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 470.7260 - mae: 19.6836 - val_loss: 335.3270 - val_mae: 17.2399\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 469.4950 - mae: 19.6527 - val_loss: 334.2477 - val_mae: 17.2086\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 468.2646 - mae: 19.6214 - val_loss: 333.1673 - val_mae: 17.1772\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 467.0333 - mae: 19.5896 - val_loss: 332.0901 - val_mae: 17.1458\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 465.8062 - mae: 19.5585 - val_loss: 331.0244 - val_mae: 17.1147\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 464.5899 - mae: 19.5277 - val_loss: 329.9446 - val_mae: 17.0831\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 463.3596 - mae: 19.4953 - val_loss: 328.8822 - val_mae: 17.0520\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 462.1469 - mae: 19.4646 - val_loss: 327.8134 - val_mae: 17.0206\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.9282 - mae: 19.4339 - val_loss: 326.7517 - val_mae: 16.9894\n",
      "5/5 [==============================] - 0s 779us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 728111.3750 - mae: 328.2178 - val_loss: 430.2574 - val_mae: 19.8026\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 614.8799 - mae: 22.6310 - val_loss: 428.8297 - val_mae: 19.7666\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 612.7510 - mae: 22.5765 - val_loss: 426.6252 - val_mae: 19.6945\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 607.8621 - mae: 22.3730 - val_loss: 419.6180 - val_mae: 19.2582\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 600.4487 - mae: 21.9509 - val_loss: 412.8048 - val_mae: 18.7471\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 593.0385 - mae: 21.6832 - val_loss: 378.5915 - val_mae: 17.7030\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1111.6655 - mae: 24.9245 - val_loss: 422.8795 - val_mae: 19.6155\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 606.4888 - mae: 22.4466 - val_loss: 421.4713 - val_mae: 19.5795\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 604.8763 - mae: 22.4100 - val_loss: 420.0582 - val_mae: 19.5434\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 603.2600 - mae: 22.3738 - val_loss: 418.6523 - val_mae: 19.5074\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.6528 - mae: 22.3379 - val_loss: 417.2662 - val_mae: 19.4719\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 600.0652 - mae: 22.3027 - val_loss: 415.8719 - val_mae: 19.4360\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 598.4692 - mae: 22.2669 - val_loss: 414.4846 - val_mae: 19.4003\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 596.8819 - mae: 22.2311 - val_loss: 413.1101 - val_mae: 19.3648\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 595.3065 - mae: 22.1959 - val_loss: 411.7294 - val_mae: 19.3292\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 593.7250 - mae: 22.1597 - val_loss: 410.3599 - val_mae: 19.2937\n",
      "Epoch 00016: early stopping\n",
      "5/5 [==============================] - 0s 744us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 128500.7656 - mae: 164.3980 - val_loss: 554.1591 - val_mae: 21.9628\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 562.1508 - mae: 22.0069 - val_loss: 552.6127 - val_mae: 21.9276\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 560.6027 - mae: 21.9715 - val_loss: 551.0690 - val_mae: 21.8924\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 559.0567 - mae: 21.9367 - val_loss: 549.5358 - val_mae: 21.8573\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 557.5215 - mae: 21.9013 - val_loss: 548.0103 - val_mae: 21.8224\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 555.9940 - mae: 21.8663 - val_loss: 546.4895 - val_mae: 21.7875\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 554.4705 - mae: 21.8317 - val_loss: 544.9611 - val_mae: 21.7524\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 552.9410 - mae: 21.7961 - val_loss: 543.4465 - val_mae: 21.7176\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 551.4243 - mae: 21.7617 - val_loss: 541.9409 - val_mae: 21.6829\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 549.9156 - mae: 21.7271 - val_loss: 540.4332 - val_mae: 21.6481\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 548.4061 - mae: 21.6923 - val_loss: 538.9322 - val_mae: 21.6134\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 546.9043 - mae: 21.6577 - val_loss: 537.4472 - val_mae: 21.5790\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 545.4163 - mae: 21.6233 - val_loss: 535.9570 - val_mae: 21.5445\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 543.9240 - mae: 21.5888 - val_loss: 534.4703 - val_mae: 21.5099\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 542.4354 - mae: 21.5539 - val_loss: 532.9927 - val_mae: 21.4756\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 540.9548 - mae: 21.5196 - val_loss: 531.5065 - val_mae: 21.4409\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 539.4676 - mae: 21.4847 - val_loss: 530.0380 - val_mae: 21.4067\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 537.9969 - mae: 21.4506 - val_loss: 528.5739 - val_mae: 21.3724\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 536.5313 - mae: 21.4168 - val_loss: 527.1197 - val_mae: 21.3384\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 535.0742 - mae: 21.3827 - val_loss: 525.6611 - val_mae: 21.3042\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 533.6143 - mae: 21.3484 - val_loss: 524.2143 - val_mae: 21.2702\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 532.1647 - mae: 21.3147 - val_loss: 522.7668 - val_mae: 21.2362\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 530.7152 - mae: 21.2810 - val_loss: 521.3196 - val_mae: 21.2020\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 529.2651 - mae: 21.2466 - val_loss: 519.8698 - val_mae: 21.1678\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 527.8137 - mae: 21.2122 - val_loss: 518.4285 - val_mae: 21.1338\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 526.3702 - mae: 21.1783 - val_loss: 516.9944 - val_mae: 21.0998\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 524.9343 - mae: 21.1442 - val_loss: 515.5686 - val_mae: 21.0660\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 523.5063 - mae: 21.1104 - val_loss: 514.1395 - val_mae: 21.0320\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 522.0755 - mae: 21.0766 - val_loss: 512.7243 - val_mae: 20.9984\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 520.6578 - mae: 21.0427 - val_loss: 511.3064 - val_mae: 20.9646\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 519.2382 - mae: 21.0088 - val_loss: 509.8942 - val_mae: 20.9309\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 517.8232 - mae: 20.9757 - val_loss: 508.4839 - val_mae: 20.8972\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 516.4124 - mae: 20.9415 - val_loss: 507.0939 - val_mae: 20.8639\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 515.0199 - mae: 20.9083 - val_loss: 505.7002 - val_mae: 20.8304\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 513.6232 - mae: 20.8752 - val_loss: 504.3051 - val_mae: 20.7969\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 512.2278 - mae: 20.8415 - val_loss: 502.9336 - val_mae: 20.7639\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 510.8546 - mae: 20.8079 - val_loss: 501.5630 - val_mae: 20.7309\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 509.4819 - mae: 20.7752 - val_loss: 500.1967 - val_mae: 20.6979\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 508.1129 - mae: 20.7423 - val_loss: 498.8257 - val_mae: 20.6648\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 506.7390 - mae: 20.7095 - val_loss: 497.4588 - val_mae: 20.6317\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 505.3708 - mae: 20.6764 - val_loss: 496.1005 - val_mae: 20.5987\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 504.0085 - mae: 20.6438 - val_loss: 494.7208 - val_mae: 20.5652\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 502.6282 - mae: 20.6102 - val_loss: 493.3664 - val_mae: 20.5323\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 501.2721 - mae: 20.5770 - val_loss: 492.0209 - val_mae: 20.4995\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 499.9232 - mae: 20.5444 - val_loss: 490.6672 - val_mae: 20.4664\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 498.5684 - mae: 20.5113 - val_loss: 489.3297 - val_mae: 20.4337\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 497.2293 - mae: 20.4781 - val_loss: 488.0004 - val_mae: 20.4012\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 495.8975 - mae: 20.4462 - val_loss: 486.6699 - val_mae: 20.3685\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 494.5648 - mae: 20.4134 - val_loss: 485.3406 - val_mae: 20.3359\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 493.2337 - mae: 20.3806 - val_loss: 484.0174 - val_mae: 20.3033\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 491.9086 - mae: 20.3479 - val_loss: 482.7028 - val_mae: 20.2709\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 490.5920 - mae: 20.3157 - val_loss: 481.3867 - val_mae: 20.2384\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 489.2728 - mae: 20.2836 - val_loss: 480.0692 - val_mae: 20.2059\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 487.9533 - mae: 20.2507 - val_loss: 478.7570 - val_mae: 20.1734\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 486.6397 - mae: 20.2186 - val_loss: 477.4549 - val_mae: 20.1411\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 485.3361 - mae: 20.1861 - val_loss: 476.1602 - val_mae: 20.1089\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 484.0383 - mae: 20.1542 - val_loss: 474.8596 - val_mae: 20.0765\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 482.7356 - mae: 20.1216 - val_loss: 473.5649 - val_mae: 20.0443\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 481.4388 - mae: 20.0897 - val_loss: 472.2680 - val_mae: 20.0119\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 480.1411 - mae: 20.0571 - val_loss: 470.9924 - val_mae: 19.9800\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 478.8619 - mae: 20.0253 - val_loss: 469.7037 - val_mae: 19.9477\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 477.5720 - mae: 19.9927 - val_loss: 468.4294 - val_mae: 19.9157\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 476.2962 - mae: 19.9608 - val_loss: 467.1658 - val_mae: 19.8840\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 475.0302 - mae: 19.9295 - val_loss: 465.8997 - val_mae: 19.8521\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 473.7625 - mae: 19.8971 - val_loss: 464.6434 - val_mae: 19.8205\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 472.5031 - mae: 19.8657 - val_loss: 463.3749 - val_mae: 19.7884\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 471.2331 - mae: 19.8338 - val_loss: 462.1184 - val_mae: 19.7567\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 469.9754 - mae: 19.8019 - val_loss: 460.8767 - val_mae: 19.7252\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 379.3229 - mae: 18.41 - 0s 3ms/step - loss: 468.7299 - mae: 19.7709 - val_loss: 459.6165 - val_mae: 19.6932\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 467.4692 - mae: 19.7387 - val_loss: 458.3809 - val_mae: 19.6618\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 466.2305 - mae: 19.7075 - val_loss: 457.1404 - val_mae: 19.6303\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 464.9882 - mae: 19.6758 - val_loss: 455.9080 - val_mae: 19.5989\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 463.7546 - mae: 19.6448 - val_loss: 454.6821 - val_mae: 19.5676\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 462.5259 - mae: 19.6131 - val_loss: 453.4558 - val_mae: 19.5362\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 461.2976 - mae: 19.5819 - val_loss: 452.2337 - val_mae: 19.5049\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.0732 - mae: 19.5505 - val_loss: 451.0122 - val_mae: 19.4736\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 458.8501 - mae: 19.5189 - val_loss: 449.7976 - val_mae: 19.4423\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 457.6328 - mae: 19.4878 - val_loss: 448.5785 - val_mae: 19.4110\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 456.4135 - mae: 19.4557 - val_loss: 447.3861 - val_mae: 19.3802\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 455.2181 - mae: 19.4256 - val_loss: 446.1894 - val_mae: 19.3493\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 454.0181 - mae: 19.3950 - val_loss: 444.9739 - val_mae: 19.3179\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 452.8011 - mae: 19.3632 - val_loss: 443.7717 - val_mae: 19.2868\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 451.5962 - mae: 19.3326 - val_loss: 442.5660 - val_mae: 19.2555\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 450.3893 - mae: 19.3013 - val_loss: 441.3763 - val_mae: 19.2246\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 449.1982 - mae: 19.2698 - val_loss: 440.1975 - val_mae: 19.1939\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 448.0173 - mae: 19.2393 - val_loss: 439.0200 - val_mae: 19.1632\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 446.8378 - mae: 19.2088 - val_loss: 437.8492 - val_mae: 19.1326\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 445.6643 - mae: 19.1782 - val_loss: 436.6781 - val_mae: 19.1020\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 444.4914 - mae: 19.1477 - val_loss: 435.5112 - val_mae: 19.0714\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 443.3233 - mae: 19.1172 - val_loss: 434.3510 - val_mae: 19.0410\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 442.1596 - mae: 19.0870 - val_loss: 433.1825 - val_mae: 19.0103\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 440.9884 - mae: 19.0563 - val_loss: 432.0082 - val_mae: 18.9793\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 439.8131 - mae: 19.0250 - val_loss: 430.8528 - val_mae: 18.9489\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 438.6567 - mae: 18.9942 - val_loss: 429.7098 - val_mae: 18.9187\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 437.5105 - mae: 18.9645 - val_loss: 428.5550 - val_mae: 18.8882\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 436.3549 - mae: 18.9336 - val_loss: 427.4214 - val_mae: 18.8581\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 435.2182 - mae: 18.9039 - val_loss: 426.2796 - val_mae: 18.8278\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 434.0746 - mae: 18.8733 - val_loss: 425.1411 - val_mae: 18.7976\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 432.9341 - mae: 18.8431 - val_loss: 424.0060 - val_mae: 18.7673\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 431.7976 - mae: 18.8126 - val_loss: 422.8831 - val_mae: 18.7374\n",
      "5/5 [==============================] - 0s 690us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 5625644215430260906413522944.0000 - mae: 20478152409088.0000 - val_loss: 308479106168173241915158336372736.0000 - val_mae: 17191453784539136.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 4437983507235898923457445888.0000 - val_loss: inf - val_mae: 3663258004376578574094461566976.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 589us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 1285340940431136928408010752.0000 - mae: 9823343083520.0000 - val_loss: 62462774412145381740202002743296.0000 - val_mae: 7758594816606208.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 1670932051212266968425955328.0000 - val_loss: inf - val_mae: 1378082195956839480889941426176.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 634us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 2153525882213832749484081152.0000 - mae: 13186273116160.0000 - val_loss: 94050763302933078236482783674368.0000 - val_mae: 9482530801582080.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 1879592913310866625397784576.0000 - val_loss: inf - val_mae: 1482316535702650067476342112256.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 713us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 1191.5084 - mae: 18.7073 - val_loss: 56.3960 - val_mae: 6.0267\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.8415 - mae: 7.3738 - val_loss: 50.8880 - val_mae: 5.5979\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.4856 - mae: 7.1865 - val_loss: 47.7094 - val_mae: 5.3228\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.4612 - mae: 7.1838 - val_loss: 49.9319 - val_mae: 5.5972\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.8931 - mae: 7.0442 - val_loss: 46.0012 - val_mae: 5.2368\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.2881 - mae: 6.9630 - val_loss: 42.0321 - val_mae: 4.8841\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.4272 - mae: 6.9591 - val_loss: 42.6792 - val_mae: 4.9999\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.0411 - mae: 6.8640 - val_loss: 58.0932 - val_mae: 6.5749\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.3426 - mae: 6.8985 - val_loss: 54.2341 - val_mae: 6.2977\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.6266 - mae: 6.9142 - val_loss: 43.0127 - val_mae: 5.2090\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.8024 - mae: 6.8148 - val_loss: 39.4183 - val_mae: 4.8985\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.1728 - mae: 6.7906 - val_loss: 37.4977 - val_mae: 4.7112\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.6698 - mae: 6.7460 - val_loss: 55.6157 - val_mae: 6.4952\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.4502 - mae: 7.0187 - val_loss: 36.5104 - val_mae: 4.7129\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.5534 - mae: 6.7749 - val_loss: 35.6290 - val_mae: 4.6603\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.2724 - mae: 6.7534 - val_loss: 32.8285 - val_mae: 4.2518\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.6468 - mae: 6.5426 - val_loss: 39.4298 - val_mae: 5.1631\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.8310 - mae: 6.6940 - val_loss: 34.3785 - val_mae: 4.6009\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.9784 - mae: 6.5804 - val_loss: 41.3758 - val_mae: 5.3892\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.7913 - mae: 6.6642 - val_loss: 46.1464 - val_mae: 5.8436\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.5258 - mae: 6.7314 - val_loss: 37.8477 - val_mae: 5.1211\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.8471 - mae: 6.6356 - val_loss: 31.9285 - val_mae: 4.4726\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.5965 - mae: 6.5288 - val_loss: 38.9173 - val_mae: 5.2566\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.8723 - mae: 6.5115 - val_loss: 38.4747 - val_mae: 5.2303\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.8096 - mae: 6.6088 - val_loss: 33.8949 - val_mae: 4.7746\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.5673 - mae: 6.4781 - val_loss: 38.1446 - val_mae: 5.2159\n",
      "Epoch 00026: early stopping\n",
      "5/5 [==============================] - 0s 703us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 263.1295 - mae: 12.3985 - val_loss: 47.2904 - val_mae: 5.5720\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.1578 - mae: 7.8345 - val_loss: 46.6880 - val_mae: 5.4698\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.6096 - mae: 7.7874 - val_loss: 50.4382 - val_mae: 5.5877\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.0015 - mae: 7.8409 - val_loss: 48.1092 - val_mae: 5.4614\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.5975 - mae: 7.8222 - val_loss: 45.4996 - val_mae: 5.3193\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.8425 - mae: 7.7876 - val_loss: 42.4127 - val_mae: 5.1365\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.9244 - mae: 7.6417 - val_loss: 45.2597 - val_mae: 5.2956\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.0214 - mae: 7.7007 - val_loss: 47.7511 - val_mae: 5.4315\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.5078 - mae: 7.6763 - val_loss: 47.7060 - val_mae: 5.4137\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.8748 - mae: 7.6424 - val_loss: 47.3589 - val_mae: 5.3883\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.1683 - mae: 7.6619 - val_loss: 42.0076 - val_mae: 5.0706\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.3850 - mae: 7.4917 - val_loss: 46.2192 - val_mae: 5.3126\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.8577 - mae: 7.5549 - val_loss: 45.6517 - val_mae: 5.2821\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.9505 - mae: 7.5786 - val_loss: 43.1885 - val_mae: 5.1311\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.7703 - mae: 7.4776 - val_loss: 44.2006 - val_mae: 5.1924\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.1537 - mae: 7.5241 - val_loss: 40.7188 - val_mae: 4.9609\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.0692 - mae: 7.4075 - val_loss: 44.0167 - val_mae: 5.1640\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.9775 - mae: 7.4920 - val_loss: 39.9100 - val_mae: 4.8897\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.5830 - mae: 7.3016 - val_loss: 42.5678 - val_mae: 5.0645\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.6605 - mae: 7.4182 - val_loss: 40.9493 - val_mae: 4.9642\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 93.4648 - mae: 7.3832 - val_loss: 37.3094 - val_mae: 4.7144\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.8459 - mae: 7.3197 - val_loss: 37.3736 - val_mae: 4.7075\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.4735 - mae: 7.2433 - val_loss: 39.0201 - val_mae: 4.8125\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.2282 - mae: 7.3457 - val_loss: 39.8129 - val_mae: 4.8620\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.5286 - mae: 7.3664 - val_loss: 35.1427 - val_mae: 4.5976\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.6303 - mae: 7.2309 - val_loss: 37.4408 - val_mae: 4.7270\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.6264 - mae: 7.2345 - val_loss: 33.6851 - val_mae: 4.5107\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.3938 - mae: 7.1552 - val_loss: 33.6579 - val_mae: 4.5112\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.7829 - mae: 7.1416 - val_loss: 32.8339 - val_mae: 4.4703\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.9084 - mae: 7.0958 - val_loss: 33.6480 - val_mae: 4.4890\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.2327 - mae: 7.1242 - val_loss: 31.8278 - val_mae: 4.4273\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.0534 - mae: 7.0067 - val_loss: 38.9367 - val_mae: 4.7703\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.7151 - mae: 7.1964 - val_loss: 33.2523 - val_mae: 4.4506\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.3866 - mae: 7.0472 - val_loss: 35.0225 - val_mae: 4.5419\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.2306 - mae: 7.0657 - val_loss: 34.8152 - val_mae: 4.5252\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.2820 - mae: 7.0990 - val_loss: 34.6201 - val_mae: 4.5204\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.6215 - mae: 7.0645 - val_loss: 32.0461 - val_mae: 4.3699\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.1165 - mae: 7.0261 - val_loss: 30.7232 - val_mae: 4.2901\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.0509 - mae: 7.0316 - val_loss: 30.4492 - val_mae: 4.2667\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.6714 - mae: 6.9291 - val_loss: 33.8398 - val_mae: 4.4585\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.2704 - mae: 7.0942 - val_loss: 29.7334 - val_mae: 4.2259\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.3528 - mae: 6.9971 - val_loss: 29.6975 - val_mae: 4.2235\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.2626 - mae: 6.9102 - val_loss: 33.0293 - val_mae: 4.4004\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.7843 - mae: 7.0063 - val_loss: 29.9756 - val_mae: 4.2159\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.1967 - mae: 7.1098 - val_loss: 27.1218 - val_mae: 4.1464\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.2025 - mae: 6.8172 - val_loss: 29.3490 - val_mae: 4.1795\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.3941 - mae: 6.9012 - val_loss: 29.0928 - val_mae: 4.1579\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.6022 - mae: 6.8995 - val_loss: 30.4066 - val_mae: 4.2199\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.7309 - mae: 6.8164 - val_loss: 39.9693 - val_mae: 4.7738\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.5188 - mae: 7.0676 - val_loss: 29.0847 - val_mae: 4.1588\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.2267 - mae: 6.8628 - val_loss: 29.3531 - val_mae: 4.1575\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4668 - mae: 6.8927 - val_loss: 31.6512 - val_mae: 4.2871\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.5648 - mae: 6.8645 - val_loss: 30.0927 - val_mae: 4.1935\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.8357 - mae: 6.8917 - val_loss: 26.8982 - val_mae: 4.0639\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.5220 - mae: 6.7618 - val_loss: 31.1198 - val_mae: 4.2308\n",
      "Epoch 00055: early stopping\n",
      "5/5 [==============================] - 0s 742us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 360.9576 - mae: 16.6290 - val_loss: 272.9319 - val_mae: 13.6857\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 250.4578 - mae: 13.3456 - val_loss: 224.5465 - val_mae: 12.0445\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 206.2625 - mae: 11.8097 - val_loss: 192.6197 - val_mae: 10.8587\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 173.4077 - mae: 10.5543 - val_loss: 167.7486 - val_mae: 9.9546\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.9922 - mae: 9.5823 - val_loss: 149.4988 - val_mae: 9.2601\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.5412 - mae: 8.8239 - val_loss: 134.8464 - val_mae: 8.6576\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.5645 - mae: 8.2087 - val_loss: 123.8854 - val_mae: 8.0999\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.7528 - mae: 7.7010 - val_loss: 113.2880 - val_mae: 7.6156\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.5538 - mae: 7.2172 - val_loss: 104.2362 - val_mae: 7.0955\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.5510 - mae: 6.7485 - val_loss: 94.7946 - val_mae: 6.6326\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.6447 - mae: 6.3799 - val_loss: 87.3520 - val_mae: 6.2697\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.6738 - mae: 6.1122 - val_loss: 80.2079 - val_mae: 5.9412\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.3902 - mae: 5.8403 - val_loss: 76.1751 - val_mae: 5.7405\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.5770 - mae: 5.7501 - val_loss: 71.9321 - val_mae: 5.5547\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4439 - mae: 5.6381 - val_loss: 68.4845 - val_mae: 5.4610\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.2472 - mae: 5.5274 - val_loss: 65.7205 - val_mae: 5.3774\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.2840 - mae: 5.5369 - val_loss: 63.3987 - val_mae: 5.4407\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.2083 - mae: 5.3857 - val_loss: 61.0887 - val_mae: 5.3584\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.4230 - mae: 5.3094 - val_loss: 59.5308 - val_mae: 5.3519\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.2264 - mae: 5.3434 - val_loss: 58.0101 - val_mae: 5.3466\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.7159 - mae: 5.3292 - val_loss: 56.6869 - val_mae: 5.3394\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.5093 - mae: 5.2450 - val_loss: 55.7162 - val_mae: 5.3291\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.1983 - mae: 5.2966 - val_loss: 54.8082 - val_mae: 5.3210\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.4790 - mae: 5.2955 - val_loss: 54.1687 - val_mae: 5.3397\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.0771 - mae: 5.3552 - val_loss: 53.1193 - val_mae: 5.3227\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.4966 - mae: 5.3004 - val_loss: 52.6502 - val_mae: 5.3289\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.3924 - mae: 5.3741 - val_loss: 51.9659 - val_mae: 5.2983\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.9608 - mae: 5.2882 - val_loss: 51.4633 - val_mae: 5.3056\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.5399 - mae: 5.2484 - val_loss: 50.9812 - val_mae: 5.3052\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.5574 - mae: 5.3072 - val_loss: 50.6668 - val_mae: 5.3027\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1648 - mae: 5.2855 - val_loss: 50.2910 - val_mae: 5.2884\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.0650 - mae: 5.2108 - val_loss: 50.0504 - val_mae: 5.3173\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1902 - mae: 5.3424 - val_loss: 49.7119 - val_mae: 5.2721\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.9935 - mae: 5.2719 - val_loss: 49.3479 - val_mae: 5.2785\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.4845 - mae: 5.2063 - val_loss: 49.0382 - val_mae: 5.2885\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.6903 - mae: 5.2668 - val_loss: 48.7559 - val_mae: 5.2622\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.5819 - mae: 5.3467 - val_loss: 48.8960 - val_mae: 5.2640\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.6596 - mae: 5.2131 - val_loss: 48.5476 - val_mae: 5.2621\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1200 - mae: 5.2237 - val_loss: 47.8696 - val_mae: 5.2396\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.0732 - mae: 5.2525 - val_loss: 47.6974 - val_mae: 5.2351\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.2021 - mae: 5.2182 - val_loss: 47.4685 - val_mae: 5.2247\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.0958 - mae: 5.1827 - val_loss: 48.5568 - val_mae: 5.3246\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.5831 - mae: 5.4400 - val_loss: 47.1345 - val_mae: 5.2207\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.0152 - mae: 5.2251 - val_loss: 47.0207 - val_mae: 5.2194\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.7738 - mae: 5.2035 - val_loss: 46.8109 - val_mae: 5.2017\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.5366 - mae: 5.2041 - val_loss: 46.8584 - val_mae: 5.2126\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.5053 - mae: 5.3881 - val_loss: 46.9420 - val_mae: 5.2181\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.7861 - mae: 5.1591 - val_loss: 46.5064 - val_mae: 5.1996\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.6977 - mae: 5.2782 - val_loss: 46.3946 - val_mae: 5.1951\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.6478 - mae: 5.1682 - val_loss: 46.1872 - val_mae: 5.1699\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.3985 - mae: 5.2826 - val_loss: 46.0664 - val_mae: 5.1814\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.7572 - mae: 5.1823 - val_loss: 45.9195 - val_mae: 5.1734\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.2733 - mae: 5.1954 - val_loss: 45.8291 - val_mae: 5.1709\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.1893 - mae: 5.2604 - val_loss: 45.6824 - val_mae: 5.1711\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.5947 - mae: 5.2119 - val_loss: 45.5569 - val_mae: 5.1617\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.4971 - mae: 5.2978 - val_loss: 45.5369 - val_mae: 5.1772\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.3050 - mae: 5.1780 - val_loss: 45.2599 - val_mae: 5.1595\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.2929 - mae: 5.2099 - val_loss: 45.1697 - val_mae: 5.1513\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.0932 - mae: 5.1719 - val_loss: 45.0085 - val_mae: 5.1362\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.0104 - mae: 5.2828 - val_loss: 44.9790 - val_mae: 5.1501\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.7572 - mae: 5.1568 - val_loss: 44.7163 - val_mae: 5.1314\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.7066 - mae: 5.2662 - val_loss: 44.7073 - val_mae: 5.1369\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.7691 - mae: 5.2210 - val_loss: 44.8359 - val_mae: 5.1393\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.6280 - mae: 5.1190 - val_loss: 44.3065 - val_mae: 5.1188\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.5715 - mae: 5.2462 - val_loss: 44.3547 - val_mae: 5.1145\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.8480 - mae: 5.1668 - val_loss: 44.0476 - val_mae: 5.0949\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.4748 - mae: 5.1997 - val_loss: 43.9848 - val_mae: 5.0812\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.6013 - mae: 5.3434 - val_loss: 44.4597 - val_mae: 5.1051\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.4458 - mae: 5.0870 - val_loss: 43.7319 - val_mae: 5.0753\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.1207 - mae: 5.2471 - val_loss: 43.8517 - val_mae: 5.0853\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.9874 - mae: 5.1188 - val_loss: 43.6420 - val_mae: 5.0626\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.9426 - mae: 5.2222 - val_loss: 43.6578 - val_mae: 5.0745\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.9208 - mae: 5.0806 - val_loss: 43.5532 - val_mae: 5.0570\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.0020 - mae: 5.2113 - val_loss: 43.5675 - val_mae: 5.0557\n",
      "Epoch 00074: early stopping\n",
      "5/5 [==============================] - 0s 801us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 124.7979 - mae: 8.6629 - val_loss: 53.7385 - val_mae: 6.4384\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.1569 - mae: 7.1135 - val_loss: 34.8721 - val_mae: 4.7798\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 77.7964 - mae: 6.4707 - val_loss: 19.6756 - val_mae: 3.5638\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.8614 - mae: 6.1871 - val_loss: 21.0057 - val_mae: 3.7261\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.5756 - mae: 6.0879 - val_loss: 17.4978 - val_mae: 3.2657\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.3606 - mae: 5.8572 - val_loss: 16.5986 - val_mae: 3.0492\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.8521 - mae: 5.7853 - val_loss: 16.4394 - val_mae: 3.0266\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.8446 - mae: 5.8546 - val_loss: 43.3800 - val_mae: 5.8142\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.1983 - mae: 5.6839 - val_loss: 49.7325 - val_mae: 6.2546\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.7874 - mae: 5.7729 - val_loss: 29.1099 - val_mae: 4.6925\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.4656 - mae: 5.7948 - val_loss: 16.2625 - val_mae: 3.1729\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.4521 - mae: 5.7786 - val_loss: 17.1786 - val_mae: 3.1177\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.9410 - mae: 5.5648 - val_loss: 48.9772 - val_mae: 6.2778\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4750 - mae: 6.1533 - val_loss: 16.4471 - val_mae: 3.2430\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.2230 - mae: 5.6622 - val_loss: 16.4957 - val_mae: 3.2477\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.0369 - mae: 5.6257 - val_loss: 16.3775 - val_mae: 3.0772\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.1113 - mae: 5.4593 - val_loss: 21.4965 - val_mae: 3.9584\n",
      "Epoch 00017: early stopping\n",
      "5/5 [==============================] - 0s 850us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 535.7099 - mae: 16.5545 - val_loss: 52.3299 - val_mae: 5.9591\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.1643 - mae: 6.8928 - val_loss: 15.9718 - val_mae: 3.2254\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.2974 - mae: 6.9780 - val_loss: 29.5302 - val_mae: 4.7747\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.0437 - mae: 6.5281 - val_loss: 25.4064 - val_mae: 4.4374\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.8804 - mae: 6.6587 - val_loss: 17.4348 - val_mae: 3.5547\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.0439 - mae: 6.5690 - val_loss: 15.9636 - val_mae: 3.3639\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.9022 - mae: 6.5795 - val_loss: 19.2829 - val_mae: 3.8088\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.8580 - mae: 6.7400 - val_loss: 53.5455 - val_mae: 6.3901\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.5380 - mae: 6.4423 - val_loss: 39.3718 - val_mae: 5.5212\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.7415 - mae: 6.3624 - val_loss: 53.6680 - val_mae: 6.3800\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.7211 - mae: 6.6596 - val_loss: 21.2495 - val_mae: 3.9922\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.7167 - mae: 6.5260 - val_loss: 32.4014 - val_mae: 4.9758\n",
      "Epoch 00012: early stopping\n",
      "5/5 [==============================] - 0s 681us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1379.9489 - mae: 25.0684 - val_loss: 205.6301 - val_mae: 11.8009\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.7582 - mae: 8.7053 - val_loss: 90.5794 - val_mae: 7.1587\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 92.7128 - mae: 7.1230 - val_loss: 81.1015 - val_mae: 6.8330\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.9699 - mae: 6.6914 - val_loss: 75.3277 - val_mae: 6.6240\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.8545 - mae: 6.4488 - val_loss: 72.4251 - val_mae: 6.5119\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.9056 - mae: 6.4210 - val_loss: 67.5798 - val_mae: 6.2950\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.3738 - mae: 6.5483 - val_loss: 64.7769 - val_mae: 6.1384\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.8432 - mae: 6.3359 - val_loss: 87.6198 - val_mae: 7.1137\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.8463 - mae: 6.2969 - val_loss: 59.7501 - val_mae: 6.0870\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.5458 - mae: 6.1049 - val_loss: 72.3879 - val_mae: 6.4913\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.6521 - mae: 5.8713 - val_loss: 58.9581 - val_mae: 5.9943\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.9632 - mae: 6.0211 - val_loss: 61.5812 - val_mae: 6.0448\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.4253 - mae: 5.8313 - val_loss: 53.1013 - val_mae: 5.7889\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.4444 - mae: 5.5917 - val_loss: 51.6506 - val_mae: 5.7484\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.4443 - mae: 5.6919 - val_loss: 57.6661 - val_mae: 5.8578\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.9322 - mae: 5.7368 - val_loss: 51.9611 - val_mae: 5.7245\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.4650 - mae: 5.6727 - val_loss: 64.3780 - val_mae: 6.0991\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.5614 - mae: 5.7248 - val_loss: 48.2741 - val_mae: 5.5039\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.4805 - mae: 5.6159 - val_loss: 49.8710 - val_mae: 5.4986\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.7340 - mae: 5.6034 - val_loss: 45.7200 - val_mae: 5.4499\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.0538 - mae: 5.6986 - val_loss: 48.3367 - val_mae: 5.5182\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.8616 - mae: 5.4130 - val_loss: 48.3607 - val_mae: 5.5228\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1736 - mae: 5.7492 - val_loss: 45.9195 - val_mae: 5.3834\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.8604 - mae: 5.2953 - val_loss: 46.1017 - val_mae: 5.3703\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.0547 - mae: 5.4328 - val_loss: 42.9808 - val_mae: 5.3081\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.6854 - mae: 5.3287 - val_loss: 43.2805 - val_mae: 5.2722\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.1823 - mae: 5.5118 - val_loss: 53.3041 - val_mae: 5.6710\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.6115 - mae: 5.4265 - val_loss: 41.7984 - val_mae: 5.1844\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.1218 - mae: 5.2549 - val_loss: 42.6408 - val_mae: 5.2395\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.7246 - mae: 5.2960 - val_loss: 41.0279 - val_mae: 5.1690\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.2116 - mae: 5.1904 - val_loss: 40.6866 - val_mae: 5.1612\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.2817 - mae: 5.2924 - val_loss: 42.3251 - val_mae: 5.1226\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.7870 - mae: 5.3076 - val_loss: 43.3639 - val_mae: 5.1982\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.4573 - mae: 5.3359 - val_loss: 39.6340 - val_mae: 5.0736\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.0643 - mae: 5.1181 - val_loss: 39.6470 - val_mae: 5.0275\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.2846 - mae: 5.1218 - val_loss: 42.4552 - val_mae: 5.1560\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.0216 - mae: 5.3143 - val_loss: 42.3770 - val_mae: 5.0890\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.4784 - mae: 5.1611 - val_loss: 42.7457 - val_mae: 5.1329\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.1343 - mae: 5.5230 - val_loss: 43.7325 - val_mae: 5.1456\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.8702 - mae: 5.3085 - val_loss: 38.4935 - val_mae: 4.9858\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.7795 - mae: 5.2496 - val_loss: 41.8604 - val_mae: 5.0856\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.4838 - mae: 4.8540 - val_loss: 77.5135 - val_mae: 6.9532\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.9607 - mae: 5.6885 - val_loss: 39.0609 - val_mae: 5.0021\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.3523 - mae: 5.2143 - val_loss: 42.2708 - val_mae: 5.0655\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.9705 - mae: 4.9593 - val_loss: 38.8201 - val_mae: 4.9244\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 49.4498 - mae: 4.9965 - val_loss: 45.2901 - val_mae: 5.1072\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 48.7123 - mae: 4.9937 - val_loss: 42.6366 - val_mae: 5.0678\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.1394 - mae: 5.2253 - val_loss: 45.5214 - val_mae: 5.2912\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.3999 - mae: 5.2991 - val_loss: 40.2879 - val_mae: 4.9053\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.1961 - mae: 5.1076 - val_loss: 42.4182 - val_mae: 4.9658\n",
      "Epoch 00050: early stopping\n",
      "5/5 [==============================] - 0s 704us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   1.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2484861.5000 - mae: 581.5770 - val_loss: 42.5710 - val_mae: 5.4313\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.3957 - mae: 6.4340 - val_loss: 41.1838 - val_mae: 5.3700\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.5400 - mae: 6.5194 - val_loss: 38.4111 - val_mae: 5.2360\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.3180 - mae: 6.4752 - val_loss: 65.8841 - val_mae: 6.5041\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6743 - mae: 6.7436 - val_loss: 41.8583 - val_mae: 5.3987\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.1907 - mae: 6.6745 - val_loss: 38.2003 - val_mae: 5.2082\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.3997 - mae: 6.4940 - val_loss: 39.7948 - val_mae: 5.3194\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.2817 - mae: 6.6188 - val_loss: 48.0145 - val_mae: 5.6545\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.1038 - mae: 6.4271 - val_loss: 61.6052 - val_mae: 6.2433\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.3372 - mae: 6.6031 - val_loss: 39.4020 - val_mae: 5.3015\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.5898 - mae: 6.5127 - val_loss: 47.6132 - val_mae: 5.6378\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.3086 - mae: 6.5049 - val_loss: 44.7937 - val_mae: 5.5190\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.8091 - mae: 6.5391 - val_loss: 56.4682 - val_mae: 5.9734\n",
      "Epoch 00013: early stopping\n",
      "5/5 [==============================] - 0s 684us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 589762240.0000 - mae: 12003.2979 - val_loss: 299.1587 - val_mae: 16.1569\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 463.2744 - mae: 18.9894 - val_loss: 298.1819 - val_mae: 16.1267\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 462.1262 - mae: 18.9595 - val_loss: 297.1971 - val_mae: 16.0961\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.9690 - mae: 18.9287 - val_loss: 296.2178 - val_mae: 16.0656\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 459.8186 - mae: 18.8986 - val_loss: 295.2480 - val_mae: 16.0354\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 458.6799 - mae: 18.8677 - val_loss: 294.2874 - val_mae: 16.0055\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 457.5492 - mae: 18.8381 - val_loss: 293.3307 - val_mae: 15.9755\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 456.4241 - mae: 18.8090 - val_loss: 292.3696 - val_mae: 15.9454\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 455.2913 - mae: 18.7783 - val_loss: 291.4040 - val_mae: 15.9151\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 454.1550 - mae: 18.7479 - val_loss: 290.4436 - val_mae: 15.8849\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 453.0254 - mae: 18.7178 - val_loss: 289.4987 - val_mae: 15.8552\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 451.9117 - mae: 18.6884 - val_loss: 288.5466 - val_mae: 15.8251\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 450.7902 - mae: 18.6583 - val_loss: 287.5997 - val_mae: 15.7952\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 449.6753 - mae: 18.6283 - val_loss: 286.6626 - val_mae: 15.7655\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 448.5694 - mae: 18.5989 - val_loss: 285.7198 - val_mae: 15.7355\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 447.4580 - mae: 18.5684 - val_loss: 284.7856 - val_mae: 15.7058\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 446.3557 - mae: 18.5393 - val_loss: 283.8454 - val_mae: 15.6759\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 445.2474 - mae: 18.5092 - val_loss: 282.9188 - val_mae: 15.6463\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 444.1536 - mae: 18.4801 - val_loss: 281.9888 - val_mae: 15.6165\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 443.0557 - mae: 18.4499 - val_loss: 281.0662 - val_mae: 15.5870\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 441.9662 - mae: 18.4199 - val_loss: 280.1480 - val_mae: 15.5575\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 440.8822 - mae: 18.3909 - val_loss: 279.2368 - val_mae: 15.5282\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 439.8049 - mae: 18.3617 - val_loss: 278.3219 - val_mae: 15.4987\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 438.7234 - mae: 18.3320 - val_loss: 277.4014 - val_mae: 15.4690\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 437.6356 - mae: 18.3021 - val_loss: 276.4971 - val_mae: 15.4397\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 436.5659 - mae: 18.2730 - val_loss: 275.5912 - val_mae: 15.4103\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 435.4964 - mae: 18.2437 - val_loss: 274.7059 - val_mae: 15.3816\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 434.4482 - mae: 18.2152 - val_loss: 273.8148 - val_mae: 15.3526\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 433.3925 - mae: 18.1860 - val_loss: 272.9254 - val_mae: 15.3236\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 432.3396 - mae: 18.1571 - val_loss: 272.0363 - val_mae: 15.2946\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 431.2879 - mae: 18.1280 - val_loss: 271.1614 - val_mae: 15.2659\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 430.2495 - mae: 18.0995 - val_loss: 270.2670 - val_mae: 15.2366\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 429.1898 - mae: 18.0700 - val_loss: 269.3848 - val_mae: 15.2076\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 428.1446 - mae: 18.0412 - val_loss: 268.5077 - val_mae: 15.1788\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 427.1037 - mae: 18.0126 - val_loss: 267.6258 - val_mae: 15.1497\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 426.0583 - mae: 17.9829 - val_loss: 266.7523 - val_mae: 15.1208\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 425.0229 - mae: 17.9544 - val_loss: 265.8918 - val_mae: 15.0924\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 424.0017 - mae: 17.9259 - val_loss: 265.0331 - val_mae: 15.0639\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 422.9814 - mae: 17.8972 - val_loss: 264.1711 - val_mae: 15.0352\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 421.9578 - mae: 17.8691 - val_loss: 263.3100 - val_mae: 15.0066\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 420.9354 - mae: 17.8399 - val_loss: 262.4543 - val_mae: 14.9780\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 419.9185 - mae: 17.8113 - val_loss: 261.6035 - val_mae: 14.9496\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 418.9069 - mae: 17.7833 - val_loss: 260.7531 - val_mae: 14.9211\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 417.8958 - mae: 17.7549 - val_loss: 259.9031 - val_mae: 14.8926\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 416.8862 - mae: 17.7255 - val_loss: 259.0610 - val_mae: 14.8643\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 415.8839 - mae: 17.6980 - val_loss: 258.2268 - val_mae: 14.8362\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 414.8915 - mae: 17.6697 - val_loss: 257.3980 - val_mae: 14.8083\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 413.9043 - mae: 17.6420 - val_loss: 256.5581 - val_mae: 14.7799\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 412.9031 - mae: 17.6139 - val_loss: 255.7161 - val_mae: 14.7514\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 411.9020 - mae: 17.5854 - val_loss: 254.8945 - val_mae: 14.7235\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 410.9220 - mae: 17.5573 - val_loss: 254.0672 - val_mae: 14.6954\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 409.9370 - mae: 17.5291 - val_loss: 253.2433 - val_mae: 14.6673\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 408.9532 - mae: 17.5012 - val_loss: 252.4217 - val_mae: 14.6393\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 407.9750 - mae: 17.4730 - val_loss: 251.6126 - val_mae: 14.6116\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 407.0078 - mae: 17.4455 - val_loss: 250.7915 - val_mae: 14.5835\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 406.0287 - mae: 17.4174 - val_loss: 249.9755 - val_mae: 14.5555\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 405.0552 - mae: 17.3896 - val_loss: 249.1682 - val_mae: 14.5277\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 404.0906 - mae: 17.3616 - val_loss: 248.3638 - val_mae: 14.5000\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 403.1295 - mae: 17.3344 - val_loss: 247.5518 - val_mae: 14.4720\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 402.1592 - mae: 17.3058 - val_loss: 246.7455 - val_mae: 14.4441\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 401.1970 - mae: 17.2778 - val_loss: 245.9521 - val_mae: 14.4166\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 400.2475 - mae: 17.2504 - val_loss: 245.1614 - val_mae: 14.3892\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 399.3016 - mae: 17.2230 - val_loss: 244.3714 - val_mae: 14.3617\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 398.3564 - mae: 17.1956 - val_loss: 243.5830 - val_mae: 14.3342\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 397.4144 - mae: 17.1686 - val_loss: 242.8033 - val_mae: 14.3070\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 396.4792 - mae: 17.1413 - val_loss: 242.0154 - val_mae: 14.2794\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 395.5369 - mae: 17.1132 - val_loss: 241.2431 - val_mae: 14.2524\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 394.6107 - mae: 17.0865 - val_loss: 240.4578 - val_mae: 14.2248\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 393.6707 - mae: 17.0585 - val_loss: 239.6828 - val_mae: 14.1975\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 392.7408 - mae: 17.0318 - val_loss: 238.9093 - val_mae: 14.1703\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 391.8147 - mae: 17.0038 - val_loss: 238.1485 - val_mae: 14.1434\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 390.9009 - mae: 16.9778 - val_loss: 237.3783 - val_mae: 14.1161\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 389.9768 - mae: 16.9503 - val_loss: 236.6108 - val_mae: 14.0889\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 389.0553 - mae: 16.9232 - val_loss: 235.8427 - val_mae: 14.0616\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 388.1334 - mae: 16.8958 - val_loss: 235.0800 - val_mae: 14.0345\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 387.2195 - mae: 16.8695 - val_loss: 234.3296 - val_mae: 14.0077\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 386.3154 - mae: 16.8421 - val_loss: 233.5643 - val_mae: 13.9804\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 385.3975 - mae: 16.8141 - val_loss: 232.8215 - val_mae: 13.9538\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 384.5036 - mae: 16.7882 - val_loss: 232.0727 - val_mae: 13.9269\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 383.6039 - mae: 16.7613 - val_loss: 231.3239 - val_mae: 13.9000\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 382.7037 - mae: 16.7349 - val_loss: 230.5884 - val_mae: 13.8736\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 381.8177 - mae: 16.7090 - val_loss: 229.8484 - val_mae: 13.8469\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 380.9264 - mae: 16.6827 - val_loss: 229.1035 - val_mae: 13.8199\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 380.0303 - mae: 16.6555 - val_loss: 228.3719 - val_mae: 13.7934\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 379.1485 - mae: 16.6296 - val_loss: 227.6392 - val_mae: 13.7669\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 378.2659 - mae: 16.6031 - val_loss: 226.9125 - val_mae: 13.7404\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 377.3893 - mae: 16.5771 - val_loss: 226.1790 - val_mae: 13.7137\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 376.5049 - mae: 16.5508 - val_loss: 225.4454 - val_mae: 13.6869\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 375.6206 - mae: 16.5238 - val_loss: 224.7195 - val_mae: 13.6604\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 374.7457 - mae: 16.4973 - val_loss: 224.0057 - val_mae: 13.6343\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 373.8834 - mae: 16.4716 - val_loss: 223.2857 - val_mae: 13.6078\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 373.0152 - mae: 16.4454 - val_loss: 222.5716 - val_mae: 13.5816\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 372.1513 - mae: 16.4199 - val_loss: 221.8506 - val_mae: 13.5550\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 371.2812 - mae: 16.3933 - val_loss: 221.1426 - val_mae: 13.5288\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 370.4253 - mae: 16.3668 - val_loss: 220.4377 - val_mae: 13.5028\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 369.5729 - mae: 16.3416 - val_loss: 219.7341 - val_mae: 13.4767\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 368.7214 - mae: 16.3155 - val_loss: 219.0232 - val_mae: 13.4503\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 367.8623 - mae: 16.2891 - val_loss: 218.3251 - val_mae: 13.4243\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 367.0177 - mae: 16.2635 - val_loss: 217.6257 - val_mae: 13.3982\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 366.1713 - mae: 16.2379 - val_loss: 216.9376 - val_mae: 13.3725\n",
      "5/5 [==============================] - 0s 694us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3998773432006213632.0000 - mae: 726574912.0000 - val_loss: 1212483895296.0000 - val_mae: 1101128.5000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1210878787584.0000 - mae: 1100399.2500 - val_loss: 1208609931264.0000 - val_mae: 1099367.8750\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1207009804288.0000 - mae: 1098639.7500 - val_loss: 1204748025856.0000 - val_mae: 1097610.1250\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1203153141760.0000 - mae: 1096883.3750 - val_loss: 1200898703360.0000 - val_mae: 1095855.2500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1199308800000.0000 - mae: 1095129.5000 - val_loss: 1197061570560.0000 - val_mae: 1094103.1250\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1195476647936.0000 - mae: 1093378.3750 - val_loss: 1193236627456.0000 - val_mae: 1092353.7500\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1191657078784.0000 - mae: 1091630.2500 - val_loss: 1189424136192.0000 - val_mae: 1090607.2500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1187849568256.0000 - mae: 1089885.0000 - val_loss: 1185623310336.0000 - val_mae: 1088863.2500\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1184053854208.0000 - mae: 1088142.2500 - val_loss: 1181835198464.0000 - val_mae: 1087122.5000\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1180270854144.0000 - mae: 1086402.5000 - val_loss: 1178059145216.0000 - val_mae: 1085384.2500\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1176499519488.0000 - mae: 1084665.5000 - val_loss: 1174294757376.0000 - val_mae: 1083648.7500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1172740374528.0000 - mae: 1082931.1250 - val_loss: 1170542952448.0000 - val_mae: 1081916.2500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1168993288192.0000 - mae: 1081199.8750 - val_loss: 1166802812928.0000 - val_mae: 1080186.5000\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1165258129408.0000 - mae: 1079471.1250 - val_loss: 1163074469888.0000 - val_mae: 1078459.2500\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1161534767104.0000 - mae: 1077745.0000 - val_loss: 1159358185472.0000 - val_mae: 1076735.0000\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1157823332352.0000 - mae: 1076021.8750 - val_loss: 1155653959680.0000 - val_mae: 1075013.3750\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1154123956224.0000 - mae: 1074301.6250 - val_loss: 1151961268224.0000 - val_mae: 1073294.6250\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1150436245504.0000 - mae: 1072583.8750 - val_loss: 1148280373248.0000 - val_mae: 1071578.5000\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1146760331264.0000 - mae: 1070868.7500 - val_loss: 1144611536896.0000 - val_mae: 1069865.2500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1143096475648.0000 - mae: 1069156.7500 - val_loss: 1140954497024.0000 - val_mae: 1068154.7500\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1139443892224.0000 - mae: 1067447.3750 - val_loss: 1137308860416.0000 - val_mae: 1066446.7500\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1135803236352.0000 - mae: 1065740.6250 - val_loss: 1133675020288.0000 - val_mae: 1064741.7500\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1132173983744.0000 - mae: 1064036.5000 - val_loss: 1130052321280.0000 - val_mae: 1063039.2500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1128556134400.0000 - mae: 1062335.1250 - val_loss: 1126441418752.0000 - val_mae: 1061339.3750\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1124950343680.0000 - mae: 1060636.6250 - val_loss: 1122842574848.0000 - val_mae: 1059642.6250\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1121356087296.0000 - mae: 1058940.8750 - val_loss: 1119255003136.0000 - val_mae: 1057948.5000\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1117773103104.0000 - mae: 1057247.7500 - val_loss: 1115678834688.0000 - val_mae: 1056256.8750\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1114201653248.0000 - mae: 1055557.5000 - val_loss: 1112113807360.0000 - val_mae: 1054568.1250\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1110641737728.0000 - mae: 1053869.7500 - val_loss: 1108560445440.0000 - val_mae: 1052881.8750\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1107092701184.0000 - mae: 1052184.7500 - val_loss: 1105018224640.0000 - val_mae: 1051198.5000\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1103555198976.0000 - mae: 1050502.2500 - val_loss: 1101487144960.0000 - val_mae: 1049517.6250\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1100028968960.0000 - mae: 1048822.5000 - val_loss: 1097967665152.0000 - val_mae: 1047839.5625\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1096514076672.0000 - mae: 1047145.5625 - val_loss: 1094459588608.0000 - val_mae: 1046164.1250\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1093010653184.0000 - mae: 1045471.3125 - val_loss: 1090962522112.0000 - val_mae: 1044491.5625\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1089518370816.0000 - mae: 1043799.8125 - val_loss: 1087476662272.0000 - val_mae: 1042821.5625\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1086037098496.0000 - mae: 1042130.8750 - val_loss: 1084002074624.0000 - val_mae: 1041154.2500\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1082566967296.0000 - mae: 1040464.6875 - val_loss: 1080538431488.0000 - val_mae: 1039489.5625\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1079107977216.0000 - mae: 1038801.1250 - val_loss: 1077085929472.0000 - val_mae: 1037827.5625\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1075660062720.0000 - mae: 1037140.1875 - val_loss: 1073644568576.0000 - val_mae: 1036168.1250\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1072223223808.0000 - mae: 1035482.0625 - val_loss: 1070214086656.0000 - val_mae: 1034511.5625\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1068797329408.0000 - mae: 1033826.5000 - val_loss: 1066794680320.0000 - val_mae: 1032857.5625\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1065382510592.0000 - mae: 1032173.5625 - val_loss: 1063386087424.0000 - val_mae: 1031206.0625\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1061978243072.0000 - mae: 1030523.2500 - val_loss: 1059988307968.0000 - val_mae: 1029557.2500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1058585051136.0000 - mae: 1028875.5000 - val_loss: 1056601276416.0000 - val_mae: 1027911.1250\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1055202672640.0000 - mae: 1027230.4375 - val_loss: 1053225254912.0000 - val_mae: 1026267.6250\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1051830976512.0000 - mae: 1025588.0000 - val_loss: 1049860177920.0000 - val_mae: 1024626.7500\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1048470290432.0000 - mae: 1023948.3125 - val_loss: 1046505586688.0000 - val_mae: 1022988.5625\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1045120221184.0000 - mae: 1022311.1250 - val_loss: 1043161677824.0000 - val_mae: 1021352.8750\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1041780768768.0000 - mae: 1020676.5625 - val_loss: 1039828713472.0000 - val_mae: 1019719.9375\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1038452129792.0000 - mae: 1019044.6250 - val_loss: 1036506300416.0000 - val_mae: 1018089.5625\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1035134042112.0000 - mae: 1017415.2500 - val_loss: 1033194373120.0000 - val_mae: 1016461.6875\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1031826636800.0000 - mae: 1015788.5625 - val_loss: 1029893193728.0000 - val_mae: 1014836.5625\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1028529782784.0000 - mae: 1014164.4375 - val_loss: 1026602369024.0000 - val_mae: 1013213.9375\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1025243348992.0000 - mae: 1012542.9375 - val_loss: 1023322226688.0000 - val_mae: 1011593.9375\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1021967466496.0000 - mae: 1010924.0000 - val_loss: 1020052373504.0000 - val_mae: 1009976.4375\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1018702069760.0000 - mae: 1009307.5625 - val_loss: 1016793268224.0000 - val_mae: 1008361.6250\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1015447158784.0000 - mae: 1007693.8750 - val_loss: 1013544386560.0000 - val_mae: 1006749.4375\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1012202471424.0000 - mae: 1006082.5625 - val_loss: 1010305859584.0000 - val_mae: 1005139.6875\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1008968400896.0000 - mae: 1004474.0625 - val_loss: 1007077687296.0000 - val_mae: 1003532.5625\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1005744488448.0000 - mae: 1002868.0000 - val_loss: 1003859804160.0000 - val_mae: 1001928.0625\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1002530865152.0000 - mae: 1001264.5000 - val_loss: 1000652210176.0000 - val_mae: 1000326.0625\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 999327596544.0000 - mae: 999663.6250 - val_loss: 997455036416.0000 - val_mae: 998726.6875\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 996134551552.0000 - mae: 998065.3125 - val_loss: 994268086272.0000 - val_mae: 997129.9375\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 992951861248.0000 - mae: 996469.5000 - val_loss: 991091097600.0000 - val_mae: 995535.5625\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 989779132416.0000 - mae: 994876.3125 - val_loss: 987924398080.0000 - val_mae: 993943.7500\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 986616496128.0000 - mae: 993285.5000 - val_loss: 984767725568.0000 - val_mae: 992354.5625\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 983463952384.0000 - mae: 991697.4375 - val_loss: 981621145600.0000 - val_mae: 990767.9375\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 980321566720.0000 - mae: 990111.8750 - val_loss: 978484658176.0000 - val_mae: 989183.7500\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 977189339136.0000 - mae: 988528.7500 - val_loss: 975358197760.0000 - val_mae: 987602.2500\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 974067073024.0000 - mae: 986948.1875 - val_loss: 972241698816.0000 - val_mae: 986023.1875\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 970954637312.0000 - mae: 985370.2500 - val_loss: 969135357952.0000 - val_mae: 984446.6875\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 967852359680.0000 - mae: 983794.6875 - val_loss: 966038781952.0000 - val_mae: 982872.6875\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 964759912448.0000 - mae: 982221.6875 - val_loss: 962951970816.0000 - val_mae: 981301.1875\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 961677295616.0000 - mae: 980651.3125 - val_loss: 959875186688.0000 - val_mae: 979732.1250\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 958604443648.0000 - mae: 979083.3125 - val_loss: 956808232960.0000 - val_mae: 978165.6875\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 955541487616.0000 - mae: 977518.0000 - val_loss: 953751044096.0000 - val_mae: 976601.7500\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 952488296448.0000 - mae: 975955.0000 - val_loss: 950703620096.0000 - val_mae: 975040.3125\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 949444870144.0000 - mae: 974394.5000 - val_loss: 947665764352.0000 - val_mae: 973481.2500\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 946411274240.0000 - mae: 972836.5625 - val_loss: 944637739008.0000 - val_mae: 971924.7500\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 943387181056.0000 - mae: 971281.1250 - val_loss: 941619412992.0000 - val_mae: 970370.7500\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 940372918272.0000 - mae: 969728.1250 - val_loss: 938610851840.0000 - val_mae: 968819.2500\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 937368158208.0000 - mae: 968177.6250 - val_loss: 935611727872.0000 - val_mae: 967270.2500\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 934373163008.0000 - mae: 966629.6875 - val_loss: 932622303232.0000 - val_mae: 965723.6875\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 931387604992.0000 - mae: 965084.2500 - val_loss: 929642381312.0000 - val_mae: 964179.6250\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 928411615232.0000 - mae: 963541.1250 - val_loss: 926671962112.0000 - val_mae: 962638.0625\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 925445193728.0000 - mae: 962000.5000 - val_loss: 923711111168.0000 - val_mae: 961098.9375\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 922488274944.0000 - mae: 960462.5000 - val_loss: 920759631872.0000 - val_mae: 959562.2500\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 919540662272.0000 - mae: 958926.6875 - val_loss: 917817655296.0000 - val_mae: 958027.9375\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 916602486784.0000 - mae: 957393.5000 - val_loss: 914885050368.0000 - val_mae: 956496.1250\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 913673814016.0000 - mae: 955862.8125 - val_loss: 911961817088.0000 - val_mae: 954966.9375\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 910754512896.0000 - mae: 954334.5000 - val_loss: 909047889920.0000 - val_mae: 953440.0625\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 907844452352.0000 - mae: 952808.6250 - val_loss: 906143268864.0000 - val_mae: 951915.5625\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 904943697920.0000 - mae: 951285.0625 - val_loss: 903248019456.0000 - val_mae: 950393.6250\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 902052249600.0000 - mae: 949764.1875 - val_loss: 900361945088.0000 - val_mae: 948874.0625\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 899169910784.0000 - mae: 948245.6875 - val_loss: 897485111296.0000 - val_mae: 947356.8750\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 896296943616.0000 - mae: 946729.5000 - val_loss: 894617387008.0000 - val_mae: 945842.0625\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 893433020416.0000 - mae: 945215.8125 - val_loss: 891758968832.0000 - val_mae: 944329.9375\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 890578403328.0000 - mae: 943704.5625 - val_loss: 888909660160.0000 - val_mae: 942820.0625\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 887732830208.0000 - mae: 942195.6875 - val_loss: 886069395456.0000 - val_mae: 941312.5625\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 884896301056.0000 - mae: 940689.2500 - val_loss: 883238240256.0000 - val_mae: 939807.5625\n",
      "5/5 [==============================] - 0s 766us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1088.3831 - mae: 25.9830 - val_loss: 915.5035 - val_mae: 24.2518\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 712.2251 - mae: 20.9731 - val_loss: 651.4563 - val_mae: 19.1461\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 608.4282 - mae: 19.2290 - val_loss: 584.7203 - val_mae: 17.3504\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 598.4624 - mae: 18.7224 - val_loss: 546.9972 - val_mae: 18.7480\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 525.6404 - mae: 17.8731 - val_loss: 462.4715 - val_mae: 15.7277\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 501.9924 - mae: 17.3537 - val_loss: 467.6665 - val_mae: 15.2972\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 490.7305 - mae: 17.1663 - val_loss: 417.9750 - val_mae: 15.3082\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 466.4659 - mae: 16.7796 - val_loss: 388.3526 - val_mae: 14.1657\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 426.1703 - mae: 15.9482 - val_loss: 373.4864 - val_mae: 14.2568\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 417.2809 - mae: 15.6811 - val_loss: 389.4441 - val_mae: 13.5926\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 435.1100 - mae: 16.0312 - val_loss: 337.1861 - val_mae: 13.2166\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 381.5798 - mae: 15.3690 - val_loss: 330.0582 - val_mae: 13.6734\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 376.2922 - mae: 15.0959 - val_loss: 323.8694 - val_mae: 13.7836\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 390.0617 - mae: 15.6562 - val_loss: 295.7563 - val_mae: 12.3213\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 342.6599 - mae: 14.4345 - val_loss: 315.4480 - val_mae: 14.3403\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 362.5185 - mae: 15.1114 - val_loss: 355.7878 - val_mae: 13.3395\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 339.4076 - mae: 14.2310 - val_loss: 288.2376 - val_mae: 13.5125\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 324.0168 - mae: 14.3100 - val_loss: 256.3142 - val_mae: 11.7421\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 301.1315 - mae: 13.5465 - val_loss: 269.3005 - val_mae: 12.9419\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 304.5441 - mae: 13.6807 - val_loss: 245.6767 - val_mae: 11.7382\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 285.1952 - mae: 13.2168 - val_loss: 240.3655 - val_mae: 12.0547\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 291.2406 - mae: 13.6054 - val_loss: 218.3554 - val_mae: 10.5387\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 276.9929 - mae: 13.2824 - val_loss: 217.1147 - val_mae: 10.4435\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 255.0170 - mae: 12.3236 - val_loss: 277.5643 - val_mae: 14.1418\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 279.2755 - mae: 13.2105 - val_loss: 207.9937 - val_mae: 10.6723\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 247.1082 - mae: 12.3656 - val_loss: 195.1263 - val_mae: 9.9973\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 247.1920 - mae: 12.2870 - val_loss: 190.1405 - val_mae: 9.8322\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 236.1006 - mae: 11.9411 - val_loss: 183.2240 - val_mae: 9.6669\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 239.5358 - mae: 12.1400 - val_loss: 212.0965 - val_mae: 11.7349\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 225.3142 - mae: 11.8783 - val_loss: 187.9495 - val_mae: 9.7601\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 229.7587 - mae: 11.8188 - val_loss: 172.8110 - val_mae: 9.4059\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 214.5679 - mae: 11.3982 - val_loss: 195.7358 - val_mae: 11.3634\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 215.2543 - mae: 11.4650 - val_loss: 160.4929 - val_mae: 9.1328\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 209.5136 - mae: 11.2315 - val_loss: 156.3520 - val_mae: 9.0233\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 202.0344 - mae: 11.0606 - val_loss: 151.5466 - val_mae: 8.7861\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 214.5197 - mae: 11.3562 - val_loss: 147.5175 - val_mae: 8.7891\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 192.5568 - mae: 10.8373 - val_loss: 146.1701 - val_mae: 8.7392\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 195.3376 - mae: 10.9251 - val_loss: 152.4199 - val_mae: 8.9340\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 189.1133 - mae: 10.7560 - val_loss: 142.1442 - val_mae: 8.7916\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.8658 - mae: 10.8828 - val_loss: 135.7534 - val_mae: 8.3786\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 184.7054 - mae: 10.6235 - val_loss: 128.4918 - val_mae: 8.1263\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 176.9705 - mae: 10.4466 - val_loss: 124.4218 - val_mae: 8.1413\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.3582 - mae: 10.4103 - val_loss: 121.1126 - val_mae: 8.0420\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 169.0334 - mae: 10.1306 - val_loss: 116.8910 - val_mae: 7.9051\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.0952 - mae: 10.1000 - val_loss: 120.1635 - val_mae: 7.9903\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.4474 - mae: 10.0311 - val_loss: 112.6565 - val_mae: 7.6847\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.6353 - mae: 10.0403 - val_loss: 110.6219 - val_mae: 7.6144\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.7065 - mae: 9.9455 - val_loss: 118.4743 - val_mae: 8.1223\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.1636 - mae: 9.8819 - val_loss: 157.4758 - val_mae: 10.4125\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 159.9293 - mae: 9.7715 - val_loss: 105.5530 - val_mae: 7.4446\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 153.2591 - mae: 9.5195 - val_loss: 109.2128 - val_mae: 7.7054\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.3426 - mae: 9.7085 - val_loss: 138.7350 - val_mae: 9.6326\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.2229 - mae: 9.5162 - val_loss: 106.8991 - val_mae: 7.7509\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.0448 - mae: 9.5842 - val_loss: 119.2634 - val_mae: 8.2265\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.6657 - mae: 9.5410 - val_loss: 96.0758 - val_mae: 7.1085\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.2135 - mae: 9.2575 - val_loss: 101.7492 - val_mae: 7.5851\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 148.3653 - mae: 9.2836 - val_loss: 88.7753 - val_mae: 6.9340\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.2292 - mae: 9.1533 - val_loss: 93.3372 - val_mae: 7.0737\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.7765 - mae: 9.1520 - val_loss: 108.3384 - val_mae: 7.8718\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.4657 - mae: 9.1882 - val_loss: 92.9954 - val_mae: 7.2319\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.2412 - mae: 9.2008 - val_loss: 84.0229 - val_mae: 6.7167\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.2576 - mae: 8.9392 - val_loss: 82.0299 - val_mae: 6.7035\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.9865 - mae: 8.7696 - val_loss: 78.6773 - val_mae: 6.4849\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.9682 - mae: 8.9803 - val_loss: 87.5646 - val_mae: 7.0850\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.3562 - mae: 9.0516 - val_loss: 76.5948 - val_mae: 6.5132\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.2098 - mae: 8.9202 - val_loss: 74.3005 - val_mae: 6.3110\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.7273 - mae: 8.4901 - val_loss: 94.1107 - val_mae: 7.5944\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.9486 - mae: 8.5877 - val_loss: 95.8625 - val_mae: 7.7112\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.0542 - mae: 8.7254 - val_loss: 73.2665 - val_mae: 6.2801\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.4365 - mae: 8.4017 - val_loss: 71.6606 - val_mae: 6.3065\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 127.0890 - mae: 8.5080 - val_loss: 73.1237 - val_mae: 6.3829\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.8590 - mae: 8.4823 - val_loss: 72.7848 - val_mae: 6.3178\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.0983 - mae: 8.3317 - val_loss: 69.4742 - val_mae: 6.2112\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.7474 - mae: 8.3392 - val_loss: 105.2560 - val_mae: 8.3590\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.7089 - mae: 8.4558 - val_loss: 65.6664 - val_mae: 5.9435\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.8230 - mae: 8.1505 - val_loss: 71.9293 - val_mae: 6.4567\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.6424 - mae: 8.1996 - val_loss: 65.4920 - val_mae: 5.8947\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.7286 - mae: 7.9751 - val_loss: 65.2933 - val_mae: 5.9194\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.5912 - mae: 8.1487 - val_loss: 68.1831 - val_mae: 6.2497\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.7410 - mae: 8.2828 - val_loss: 81.7049 - val_mae: 7.1588\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.2871 - mae: 8.1143 - val_loss: 63.1217 - val_mae: 5.9097\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.3693 - mae: 7.9201 - val_loss: 69.2796 - val_mae: 6.3206\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.5586 - mae: 7.9155 - val_loss: 59.9413 - val_mae: 5.6495\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.4661 - mae: 7.8300 - val_loss: 60.1866 - val_mae: 5.6466\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.1297 - mae: 7.8358 - val_loss: 60.5561 - val_mae: 5.7832\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.3120 - mae: 8.0044 - val_loss: 75.3619 - val_mae: 6.8828\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.6020 - mae: 7.8888 - val_loss: 58.0221 - val_mae: 5.5880\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.6129 - mae: 7.8963 - val_loss: 81.7173 - val_mae: 7.3283\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.6373 - mae: 7.9663 - val_loss: 60.1431 - val_mae: 5.7394\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.0206 - mae: 7.8750 - val_loss: 56.2864 - val_mae: 5.4718\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.4005 - mae: 7.8640 - val_loss: 59.4357 - val_mae: 5.8720\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.4549 - mae: 7.7538 - val_loss: 54.2432 - val_mae: 5.3648\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.1732 - mae: 7.7777 - val_loss: 58.4096 - val_mae: 5.7262\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.8138 - mae: 7.7600 - val_loss: 61.5202 - val_mae: 5.9978\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.9702 - mae: 7.8078 - val_loss: 55.8483 - val_mae: 5.6018\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.9794 - mae: 7.7447 - val_loss: 58.3416 - val_mae: 5.8373\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.5662 - mae: 7.5507 - val_loss: 75.0136 - val_mae: 7.0760\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.0331 - mae: 7.8565 - val_loss: 58.6279 - val_mae: 5.7882\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.1696 - mae: 7.6090 - val_loss: 58.0682 - val_mae: 5.8792\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.0846 - mae: 7.6717 - val_loss: 56.3590 - val_mae: 5.7283\n",
      "5/5 [==============================] - 0s 750us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1482.1564 - mae: 29.4729 - val_loss: 643.2405 - val_mae: 16.8116\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 795.3029 - mae: 20.1582 - val_loss: 542.4739 - val_mae: 18.6741\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 668.2382 - mae: 18.9978 - val_loss: 693.6390 - val_mae: 23.2828\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 863.0515 - mae: 22.9435 - val_loss: 468.0391 - val_mae: 17.8124\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 602.8066 - mae: 18.7325 - val_loss: 444.3359 - val_mae: 13.7811\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 818.6275 - mae: 21.9490 - val_loss: 363.2639 - val_mae: 13.6605\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 533.6867 - mae: 17.5908 - val_loss: 635.9314 - val_mae: 22.7189\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 717.4846 - mae: 21.7299 - val_loss: 411.9215 - val_mae: 17.1641\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 515.5013 - mae: 16.9971 - val_loss: 370.6145 - val_mae: 15.9184\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 488.2685 - mae: 16.4546 - val_loss: 968.6737 - val_mae: 28.2534\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 589.4370 - mae: 18.7624 - val_loss: 347.9611 - val_mae: 12.2409\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 429.3947 - mae: 15.3097 - val_loss: 415.0302 - val_mae: 17.9323\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 723.5507 - mae: 21.6425 - val_loss: 432.9464 - val_mae: 18.5930\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 541.1480 - mae: 18.3184 - val_loss: 301.7447 - val_mae: 14.7459\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 384.6322 - mae: 14.9280 - val_loss: 305.9081 - val_mae: 15.0045\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 336.3827 - mae: 13.9040 - val_loss: 290.9563 - val_mae: 14.5673\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 363.4590 - mae: 14.5463 - val_loss: 287.7552 - val_mae: 14.6173\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 377.1385 - mae: 14.6487 - val_loss: 293.5742 - val_mae: 14.8337\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 334.3958 - mae: 14.0963 - val_loss: 240.8081 - val_mae: 12.9292\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 340.8200 - mae: 14.2368 - val_loss: 295.3645 - val_mae: 12.3634\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 329.0682 - mae: 13.4936 - val_loss: 199.8759 - val_mae: 11.7454\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 274.7390 - mae: 12.5253 - val_loss: 172.6777 - val_mae: 10.0983\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 275.5291 - mae: 12.4227 - val_loss: 172.7068 - val_mae: 8.9597\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 269.4355 - mae: 12.3533 - val_loss: 425.6184 - val_mae: 18.5349\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 289.4791 - mae: 12.8385 - val_loss: 224.4154 - val_mae: 12.7533\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 266.6251 - mae: 12.1250 - val_loss: 167.2721 - val_mae: 10.5136\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 262.4712 - mae: 12.1953 - val_loss: 260.9113 - val_mae: 12.2808\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 252.7680 - mae: 12.5797 - val_loss: 143.8992 - val_mae: 9.2419\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 262.3935 - mae: 12.3816 - val_loss: 143.2168 - val_mae: 8.1884\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 229.0851 - mae: 11.4614 - val_loss: 133.4527 - val_mae: 8.8612\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 314.0039 - mae: 13.3826 - val_loss: 181.6044 - val_mae: 9.7983\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 287.3290 - mae: 12.9908 - val_loss: 325.0183 - val_mae: 15.8322\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.0927 - mae: 12.1356 - val_loss: 117.3858 - val_mae: 8.0329\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 206.2033 - mae: 11.0328 - val_loss: 136.5584 - val_mae: 9.5243\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 203.1181 - mae: 10.5351 - val_loss: 138.1380 - val_mae: 9.7167\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 218.3667 - mae: 10.9121 - val_loss: 115.0077 - val_mae: 8.3487\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 234.6690 - mae: 11.4744 - val_loss: 119.7521 - val_mae: 8.6238\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 200.4683 - mae: 10.4359 - val_loss: 106.7735 - val_mae: 7.6150\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 187.8210 - mae: 10.2701 - val_loss: 120.4967 - val_mae: 8.8577\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 200.1340 - mae: 10.3647 - val_loss: 152.5253 - val_mae: 10.2301\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 198.3560 - mae: 10.1713 - val_loss: 97.0084 - val_mae: 7.5265\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 181.0821 - mae: 9.8863 - val_loss: 96.6885 - val_mae: 7.6387\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 191.1472 - mae: 10.2125 - val_loss: 98.5954 - val_mae: 7.9006\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 193.6944 - mae: 10.3413 - val_loss: 90.9540 - val_mae: 7.3227\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.1202 - mae: 10.3843 - val_loss: 88.9671 - val_mae: 7.1842\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 191.3294 - mae: 10.0899 - val_loss: 143.4671 - val_mae: 9.3493\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 192.6785 - mae: 10.3397 - val_loss: 88.4804 - val_mae: 6.7090\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 173.5520 - mae: 9.6462 - val_loss: 122.5954 - val_mae: 9.1663\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 172.0756 - mae: 9.6123 - val_loss: 289.3890 - val_mae: 14.3391\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.0124 - mae: 9.4607 - val_loss: 83.1139 - val_mae: 6.5597\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.8443 - mae: 10.1152 - val_loss: 79.9318 - val_mae: 6.4977\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 198.2830 - mae: 10.4008 - val_loss: 331.1913 - val_mae: 15.3803\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 241.4006 - mae: 11.3905 - val_loss: 87.8584 - val_mae: 7.5174\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.0261 - mae: 9.1634 - val_loss: 112.4225 - val_mae: 8.6314\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 158.9853 - mae: 9.2205 - val_loss: 120.7717 - val_mae: 9.0976\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 218.1959 - mae: 10.7553 - val_loss: 80.1412 - val_mae: 7.1669\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.3873 - mae: 8.8571 - val_loss: 153.4894 - val_mae: 10.2969\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 159.2162 - mae: 8.8414 - val_loss: 72.0329 - val_mae: 6.4188\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.7855 - mae: 8.9302 - val_loss: 88.5708 - val_mae: 7.7501\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.9549 - mae: 8.8819 - val_loss: 93.8816 - val_mae: 7.9221\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.3342 - mae: 9.0031 - val_loss: 79.7966 - val_mae: 7.1636\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.0456 - mae: 8.4823 - val_loss: 74.4472 - val_mae: 6.2464\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.1122 - mae: 9.3665 - val_loss: 67.3330 - val_mae: 6.2156\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.7819 - mae: 8.7052 - val_loss: 83.2384 - val_mae: 7.4798\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.0098 - mae: 9.6941 - val_loss: 88.8383 - val_mae: 7.6266\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.0915 - mae: 8.4967 - val_loss: 118.9598 - val_mae: 8.9854\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.6234 - mae: 8.5774 - val_loss: 65.9039 - val_mae: 6.0126\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.2686 - mae: 8.5307 - val_loss: 85.8429 - val_mae: 7.5992\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.7605 - mae: 8.4602 - val_loss: 82.7168 - val_mae: 7.4032\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.0517 - mae: 8.3946 - val_loss: 67.2377 - val_mae: 6.0616\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 167.5921 - mae: 9.3534 - val_loss: 81.7281 - val_mae: 7.3124\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.7407 - mae: 8.4619 - val_loss: 90.3055 - val_mae: 7.4838\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.3395 - mae: 9.1659 - val_loss: 64.0281 - val_mae: 6.2406\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.9636 - mae: 8.3239 - val_loss: 119.0571 - val_mae: 8.9417\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.1527 - mae: 8.9597 - val_loss: 107.8412 - val_mae: 8.4575\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.2600 - mae: 8.7692 - val_loss: 59.0080 - val_mae: 5.7180\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.1757 - mae: 8.1016 - val_loss: 126.3189 - val_mae: 9.2509\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.6364 - mae: 8.5493 - val_loss: 67.7351 - val_mae: 6.1047\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.9091 - mae: 8.3166 - val_loss: 72.7734 - val_mae: 6.8301\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.7915 - mae: 8.3255 - val_loss: 132.5637 - val_mae: 9.3752\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.5095 - mae: 8.4515 - val_loss: 56.2918 - val_mae: 5.6424\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.3469 - mae: 8.0359 - val_loss: 110.8083 - val_mae: 8.4654\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.6259 - mae: 8.6310 - val_loss: 150.2736 - val_mae: 10.1542\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.2654 - mae: 8.8278 - val_loss: 96.6142 - val_mae: 7.8600\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.0156 - mae: 8.4078 - val_loss: 55.7377 - val_mae: 5.6556\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.3914 - mae: 8.0259 - val_loss: 98.8506 - val_mae: 7.9108\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.4633 - mae: 8.3730 - val_loss: 90.5768 - val_mae: 7.6313\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.0445 - mae: 8.6124 - val_loss: 164.1476 - val_mae: 10.6585\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.6191 - mae: 8.4576 - val_loss: 55.9869 - val_mae: 5.8767\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.1119 - mae: 8.1110 - val_loss: 54.5761 - val_mae: 5.5702\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.0135 - mae: 8.1211 - val_loss: 94.6692 - val_mae: 7.7668\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.9358 - mae: 8.8385 - val_loss: 60.4899 - val_mae: 6.1448\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.7245 - mae: 8.1840 - val_loss: 119.7637 - val_mae: 8.9371\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.5105 - mae: 8.7747 - val_loss: 96.1034 - val_mae: 7.7707\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.1357 - mae: 8.6643 - val_loss: 55.3518 - val_mae: 5.6662\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.8261 - mae: 8.8491 - val_loss: 87.1568 - val_mae: 7.8268\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.6901 - mae: 8.0528 - val_loss: 55.1847 - val_mae: 5.8313\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.9646 - mae: 8.5809 - val_loss: 54.9612 - val_mae: 5.8164\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.0552 - mae: 8.0158 - val_loss: 94.4340 - val_mae: 8.1187\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.7537 - mae: 8.2884 - val_loss: 58.2655 - val_mae: 5.7803\n",
      "Epoch 00100: early stopping\n",
      "5/5 [==============================] - 0s 632us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 18184.7168 - mae: 79.1844 - val_loss: 149.4977 - val_mae: 9.8851\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 195.0755 - mae: 10.7749 - val_loss: 93.4646 - val_mae: 7.7437\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.5445 - mae: 9.3565 - val_loss: 92.6346 - val_mae: 7.7955\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.9975 - mae: 9.0860 - val_loss: 88.2773 - val_mae: 7.4969\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 137.2000 - mae: 8.8150 - val_loss: 94.1645 - val_mae: 7.8205\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.1657 - mae: 8.6818 - val_loss: 87.6554 - val_mae: 7.3743\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.5765 - mae: 8.5848 - val_loss: 83.5445 - val_mae: 7.0512\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.1809 - mae: 8.4250 - val_loss: 88.5664 - val_mae: 7.4486\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.3512 - mae: 8.3771 - val_loss: 81.5503 - val_mae: 6.9050\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.9313 - mae: 8.3262 - val_loss: 80.4897 - val_mae: 6.8482\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.0333 - mae: 8.2392 - val_loss: 79.3366 - val_mae: 6.7973\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.3590 - mae: 8.2528 - val_loss: 78.0669 - val_mae: 6.7326\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.8545 - mae: 8.2722 - val_loss: 78.8015 - val_mae: 6.7567\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 117.8895 - mae: 8.1743 - val_loss: 81.8723 - val_mae: 6.8652\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 117.0191 - mae: 8.1560 - val_loss: 81.9572 - val_mae: 6.8462\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.9519 - mae: 8.0966 - val_loss: 81.1613 - val_mae: 6.8006\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.1946 - mae: 8.0967 - val_loss: 83.4714 - val_mae: 6.8977\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.1944 - mae: 8.1301 - val_loss: 85.3348 - val_mae: 6.9827\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.6607 - mae: 8.0162 - val_loss: 82.8376 - val_mae: 6.8468\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.9750 - mae: 7.9920 - val_loss: 85.9268 - val_mae: 6.9809\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.0807 - mae: 7.9960 - val_loss: 83.3339 - val_mae: 6.8652\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.8695 - mae: 7.9450 - val_loss: 80.3668 - val_mae: 6.7680\n",
      "Epoch 00022: early stopping\n",
      "5/5 [==============================] - 0s 720us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2915.7563 - mae: 43.5838 - val_loss: 2634.0872 - val_mae: 42.6061\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1581.5527 - mae: 31.3484 - val_loss: 1538.8702 - val_mae: 31.6242\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 929.7245 - mae: 23.2767 - val_loss: 943.8728 - val_mae: 24.5936\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.1696 - mae: 19.0196 - val_loss: 619.1978 - val_mae: 19.5235\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 464.3731 - mae: 16.6109 - val_loss: 426.8708 - val_mae: 15.9645\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 382.7537 - mae: 15.2954 - val_loss: 330.1802 - val_mae: 14.0857\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 337.8393 - mae: 14.5863 - val_loss: 259.9201 - val_mae: 12.6636\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 314.8206 - mae: 14.2945 - val_loss: 240.3159 - val_mae: 12.3638\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 299.2231 - mae: 13.8703 - val_loss: 241.0966 - val_mae: 12.7141\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 293.8146 - mae: 13.8047 - val_loss: 220.8681 - val_mae: 12.1844\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 289.8752 - mae: 13.7181 - val_loss: 174.1950 - val_mae: 10.9313\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 278.9196 - mae: 13.5117 - val_loss: 156.4675 - val_mae: 10.4545\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 266.3413 - mae: 13.1384 - val_loss: 186.6176 - val_mae: 11.2656\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 266.0396 - mae: 13.2013 - val_loss: 146.9473 - val_mae: 10.1613\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 257.4666 - mae: 12.9345 - val_loss: 148.7679 - val_mae: 10.1722\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 256.5148 - mae: 12.8762 - val_loss: 148.4243 - val_mae: 10.2354\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 250.5408 - mae: 12.7497 - val_loss: 134.1808 - val_mae: 9.7426\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 248.6610 - mae: 12.7069 - val_loss: 133.3685 - val_mae: 9.7796\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 240.6825 - mae: 12.4896 - val_loss: 137.5009 - val_mae: 9.8711\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 239.0210 - mae: 12.4349 - val_loss: 137.3712 - val_mae: 9.8532\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 234.6016 - mae: 12.3054 - val_loss: 121.6679 - val_mae: 9.2446\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 229.5053 - mae: 12.1580 - val_loss: 120.8555 - val_mae: 9.3039\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 226.9638 - mae: 12.1391 - val_loss: 166.4426 - val_mae: 10.7343\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 222.5589 - mae: 12.0879 - val_loss: 122.0429 - val_mae: 9.1198\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 220.8101 - mae: 11.7964 - val_loss: 116.8276 - val_mae: 9.1423\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 214.2669 - mae: 11.7872 - val_loss: 122.2247 - val_mae: 9.3230\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 215.0951 - mae: 11.8167 - val_loss: 111.6169 - val_mae: 8.9230\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 217.5204 - mae: 11.7809 - val_loss: 106.7853 - val_mae: 8.6262\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 205.6856 - mae: 11.4473 - val_loss: 109.0369 - val_mae: 8.8302\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 205.1101 - mae: 11.4162 - val_loss: 111.6605 - val_mae: 8.9240\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 198.3266 - mae: 11.3457 - val_loss: 100.8742 - val_mae: 8.2823\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 197.5718 - mae: 11.1834 - val_loss: 109.1523 - val_mae: 8.8099\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 200.8954 - mae: 11.3407 - val_loss: 131.5904 - val_mae: 9.6438\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 193.9918 - mae: 11.1112 - val_loss: 128.0063 - val_mae: 9.5170\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 188.7289 - mae: 10.9678 - val_loss: 96.0944 - val_mae: 8.1659\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 189.0001 - mae: 10.9608 - val_loss: 93.5173 - val_mae: 7.9054\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 184.9479 - mae: 10.7333 - val_loss: 92.4688 - val_mae: 8.0909\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 182.4507 - mae: 10.7831 - val_loss: 88.9812 - val_mae: 7.7049\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 177.6569 - mae: 10.5628 - val_loss: 90.3412 - val_mae: 7.9989\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 176.2433 - mae: 10.5192 - val_loss: 95.2955 - val_mae: 8.2157\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.5361 - mae: 10.4576 - val_loss: 83.7183 - val_mae: 7.6179\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 168.8226 - mae: 10.3247 - val_loss: 88.5377 - val_mae: 7.9368\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 167.2121 - mae: 10.2839 - val_loss: 82.2434 - val_mae: 7.5920\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.5672 - mae: 10.1792 - val_loss: 87.8324 - val_mae: 7.8836\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.6311 - mae: 10.1006 - val_loss: 77.9075 - val_mae: 7.1688\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 160.3265 - mae: 9.9858 - val_loss: 76.4063 - val_mae: 7.0419\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.0986 - mae: 9.8893 - val_loss: 80.1050 - val_mae: 7.5725\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.5112 - mae: 9.9866 - val_loss: 82.6165 - val_mae: 7.6558\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.3486 - mae: 9.8563 - val_loss: 78.5899 - val_mae: 7.4881\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.7662 - mae: 9.7516 - val_loss: 72.6610 - val_mae: 7.2068\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.1181 - mae: 9.7279 - val_loss: 71.7053 - val_mae: 7.1242\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.3552 - mae: 9.5958 - val_loss: 68.1989 - val_mae: 6.5997\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.4472 - mae: 9.4747 - val_loss: 70.8951 - val_mae: 7.0681\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.4135 - mae: 9.4786 - val_loss: 65.9093 - val_mae: 6.6004\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.6118 - mae: 9.1860 - val_loss: 89.9688 - val_mae: 8.2311\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.8751 - mae: 9.4760 - val_loss: 63.8207 - val_mae: 6.4801\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.0210 - mae: 9.2206 - val_loss: 68.6066 - val_mae: 6.9723\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.6740 - mae: 9.2397 - val_loss: 87.0516 - val_mae: 8.1340\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.3382 - mae: 9.3280 - val_loss: 62.7909 - val_mae: 6.2558\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.7506 - mae: 9.0186 - val_loss: 61.0032 - val_mae: 6.4246\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.4904 - mae: 9.0078 - val_loss: 60.1244 - val_mae: 6.0923\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.2379 - mae: 8.9071 - val_loss: 60.3815 - val_mae: 6.0326\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.8332 - mae: 8.9480 - val_loss: 59.1665 - val_mae: 6.3203\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.0927 - mae: 8.7858 - val_loss: 69.2131 - val_mae: 7.1298\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.8887 - mae: 8.8626 - val_loss: 57.2040 - val_mae: 6.0510\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.7590 - mae: 8.8379 - val_loss: 56.4517 - val_mae: 6.0038\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.7079 - mae: 8.6801 - val_loss: 54.7037 - val_mae: 5.8819\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.2298 - mae: 8.5317 - val_loss: 56.3024 - val_mae: 6.1692\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.7712 - mae: 8.5016 - val_loss: 61.2791 - val_mae: 6.6822\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.3749 - mae: 8.6217 - val_loss: 52.8590 - val_mae: 5.5696\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.1106 - mae: 8.5309 - val_loss: 61.9664 - val_mae: 5.5836\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.2034 - mae: 8.2965 - val_loss: 51.2402 - val_mae: 5.8170\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.6149 - mae: 8.3526 - val_loss: 51.0424 - val_mae: 5.4834\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.6143 - mae: 8.2418 - val_loss: 54.2791 - val_mae: 6.2212\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.8208 - mae: 8.1540 - val_loss: 50.4973 - val_mae: 5.8218\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.8710 - mae: 8.2540 - val_loss: 48.8340 - val_mae: 5.3771\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.9906 - mae: 8.1129 - val_loss: 49.0235 - val_mae: 5.6858\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.1992 - mae: 8.0472 - val_loss: 50.9076 - val_mae: 5.9721\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.9837 - mae: 8.0516 - val_loss: 48.4148 - val_mae: 5.6047\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.0216 - mae: 7.9895 - val_loss: 50.8775 - val_mae: 6.0105\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.6580 - mae: 8.0077 - val_loss: 46.4135 - val_mae: 5.4719\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.4713 - mae: 7.8493 - val_loss: 47.2688 - val_mae: 5.7151\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.4579 - mae: 7.7975 - val_loss: 53.9994 - val_mae: 6.3519\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.1858 - mae: 7.9720 - val_loss: 57.7492 - val_mae: 6.6341\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.9744 - mae: 7.9237 - val_loss: 48.4416 - val_mae: 5.9044\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.9137 - mae: 7.5971 - val_loss: 59.5900 - val_mae: 6.7843\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.7959 - mae: 7.9123 - val_loss: 42.6475 - val_mae: 5.0516\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.5504 - mae: 7.5985 - val_loss: 54.4515 - val_mae: 6.4223\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.6905 - mae: 7.8672 - val_loss: 42.2679 - val_mae: 5.2399\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.0880 - mae: 7.6553 - val_loss: 41.9404 - val_mae: 4.8121\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.9669 - mae: 7.6092 - val_loss: 47.5988 - val_mae: 5.9051\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.3454 - mae: 7.8212 - val_loss: 44.5189 - val_mae: 4.7027\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.9452 - mae: 7.6644 - val_loss: 41.6920 - val_mae: 4.6727\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.7496 - mae: 7.3481 - val_loss: 42.7373 - val_mae: 5.4610\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.3932 - mae: 7.4938 - val_loss: 40.8267 - val_mae: 5.2241\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.4701 - mae: 7.4236 - val_loss: 40.9839 - val_mae: 5.2535\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.6077 - mae: 7.3667 - val_loss: 44.0103 - val_mae: 5.6123\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.3143 - mae: 7.5832 - val_loss: 38.3546 - val_mae: 4.6955\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.2412 - mae: 7.4387 - val_loss: 38.8320 - val_mae: 4.6898\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.9559 - mae: 7.1163 - val_loss: 44.7211 - val_mae: 5.6884\n",
      "5/5 [==============================] - 0s 673us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7562.5151 - mae: 50.8187 - val_loss: 129.2476 - val_mae: 9.9027\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 253.4351 - mae: 12.7774 - val_loss: 90.6284 - val_mae: 8.0863\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 220.6269 - mae: 11.4674 - val_loss: 85.9100 - val_mae: 7.8304\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 201.3137 - mae: 10.6208 - val_loss: 101.5714 - val_mae: 8.3325\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 195.9976 - mae: 10.4785 - val_loss: 68.6747 - val_mae: 6.9429\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 191.9020 - mae: 10.2360 - val_loss: 76.4573 - val_mae: 7.2167\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.0478 - mae: 10.3294 - val_loss: 65.0590 - val_mae: 6.6710\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.8209 - mae: 10.3281 - val_loss: 98.9023 - val_mae: 8.1253\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 182.3328 - mae: 10.0348 - val_loss: 114.0695 - val_mae: 8.7497\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.8580 - mae: 10.3543 - val_loss: 71.4009 - val_mae: 6.9207\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.9974 - mae: 9.9566 - val_loss: 96.2104 - val_mae: 7.9764\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 181.0725 - mae: 10.0374 - val_loss: 76.0127 - val_mae: 7.1300\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.2991 - mae: 9.8454 - val_loss: 89.1924 - val_mae: 7.6730\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 176.4406 - mae: 9.8001 - val_loss: 62.4210 - val_mae: 6.4708\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 170.8711 - mae: 9.7273 - val_loss: 70.2401 - val_mae: 6.8517\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 169.9530 - mae: 9.7032 - val_loss: 62.1014 - val_mae: 6.4384\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 172.3548 - mae: 9.7425 - val_loss: 81.6282 - val_mae: 7.3181\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 170.5776 - mae: 9.7042 - val_loss: 69.8372 - val_mae: 6.8354\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.1249 - mae: 9.5626 - val_loss: 76.5254 - val_mae: 7.1134\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 168.2406 - mae: 9.6871 - val_loss: 83.7243 - val_mae: 7.3796\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.9192 - mae: 9.5996 - val_loss: 53.1223 - val_mae: 6.0668\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.8288 - mae: 9.6695 - val_loss: 56.6821 - val_mae: 6.1295\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.6379 - mae: 9.4615 - val_loss: 66.6128 - val_mae: 6.6516\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 160.5430 - mae: 9.3940 - val_loss: 52.9262 - val_mae: 5.9261\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.5036 - mae: 9.2961 - val_loss: 69.5300 - val_mae: 6.7901\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.3437 - mae: 9.4315 - val_loss: 94.8412 - val_mae: 7.9687\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 160.8772 - mae: 9.5147 - val_loss: 53.2773 - val_mae: 5.9555\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.0806 - mae: 9.5794 - val_loss: 55.4271 - val_mae: 6.0824\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.9126 - mae: 9.1996 - val_loss: 56.6240 - val_mae: 6.1360\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 153.3561 - mae: 9.0546 - val_loss: 68.5816 - val_mae: 6.7095\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 153.2703 - mae: 9.2313 - val_loss: 52.4065 - val_mae: 5.9215\n",
      "Epoch 00031: early stopping\n",
      "5/5 [==============================] - 0s 604us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3674.0132 - mae: 50.7295 - val_loss: 1208.9849 - val_mae: 31.1230\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1817.9620 - mae: 35.3709 - val_loss: 866.3803 - val_mae: 25.6707\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1162.0731 - mae: 28.4800 - val_loss: 760.0897 - val_mae: 23.2345\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 896.3179 - mae: 24.4148 - val_loss: 728.7410 - val_mae: 22.6650\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 771.4944 - mae: 22.4769 - val_loss: 717.4563 - val_mae: 21.5877\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 715.4733 - mae: 21.4444 - val_loss: 720.9850 - val_mae: 21.0569\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 680.7737 - mae: 20.5347 - val_loss: 735.1666 - val_mae: 22.6394\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 640.3805 - mae: 20.0215 - val_loss: 720.9237 - val_mae: 20.3539\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 643.3779 - mae: 19.7161 - val_loss: 698.4405 - val_mae: 20.8874\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 616.1654 - mae: 19.4030 - val_loss: 728.2628 - val_mae: 19.9483\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 602.0452 - mae: 18.5761 - val_loss: 681.3950 - val_mae: 21.4400\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 595.6550 - mae: 19.1027 - val_loss: 664.4146 - val_mae: 19.7709\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 573.6351 - mae: 18.3305 - val_loss: 643.7101 - val_mae: 19.9057\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 555.6746 - mae: 18.0556 - val_loss: 656.1960 - val_mae: 21.4746\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 555.2277 - mae: 18.3041 - val_loss: 616.8452 - val_mae: 19.4897\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 538.3241 - mae: 17.8238 - val_loss: 604.3905 - val_mae: 19.6865\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 512.6290 - mae: 17.8236 - val_loss: 657.3772 - val_mae: 18.5669\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 532.6434 - mae: 17.5949 - val_loss: 578.7952 - val_mae: 19.2272\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 508.5558 - mae: 17.3388 - val_loss: 585.9778 - val_mae: 20.1221\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 492.0679 - mae: 17.2188 - val_loss: 551.6626 - val_mae: 18.5425\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 482.4654 - mae: 16.8060 - val_loss: 548.1068 - val_mae: 17.7781\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 473.9291 - mae: 16.6233 - val_loss: 526.0855 - val_mae: 18.0314\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 452.4501 - mae: 16.3617 - val_loss: 552.3908 - val_mae: 19.9033\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 454.8246 - mae: 16.5174 - val_loss: 512.5781 - val_mae: 18.4916\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 441.4230 - mae: 16.2683 - val_loss: 491.7602 - val_mae: 17.6355\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 425.5105 - mae: 15.7551 - val_loss: 534.1210 - val_mae: 19.8360\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 427.3502 - mae: 16.2096 - val_loss: 473.5020 - val_mae: 17.5694\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 414.7897 - mae: 15.8575 - val_loss: 464.1422 - val_mae: 16.6515\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 408.2636 - mae: 15.6961 - val_loss: 453.2668 - val_mae: 16.5546\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 407.6068 - mae: 15.6311 - val_loss: 453.1416 - val_mae: 17.4508\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 395.3220 - mae: 15.4498 - val_loss: 446.8910 - val_mae: 17.4192\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 379.6315 - mae: 15.0358 - val_loss: 459.9378 - val_mae: 18.1820\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 376.7813 - mae: 15.3059 - val_loss: 419.4730 - val_mae: 15.8186\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 371.3597 - mae: 14.8874 - val_loss: 410.4940 - val_mae: 15.7474\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 355.7623 - mae: 14.5269 - val_loss: 417.8065 - val_mae: 17.0039\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 369.0683 - mae: 14.9466 - val_loss: 394.5674 - val_mae: 15.2046\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 348.9291 - mae: 14.4337 - val_loss: 382.8859 - val_mae: 15.1893\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 343.0139 - mae: 14.3281 - val_loss: 375.1464 - val_mae: 14.9325\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 336.2448 - mae: 14.1386 - val_loss: 365.7455 - val_mae: 15.0829\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 327.8031 - mae: 14.0030 - val_loss: 364.3091 - val_mae: 15.3586\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 329.3601 - mae: 14.0690 - val_loss: 355.7960 - val_mae: 14.4681\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 316.4582 - mae: 13.4942 - val_loss: 435.0880 - val_mae: 18.3426\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 322.7711 - mae: 14.1642 - val_loss: 343.4655 - val_mae: 15.0269\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 305.5262 - mae: 13.6406 - val_loss: 336.1988 - val_mae: 13.9538\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 299.9090 - mae: 13.3870 - val_loss: 350.6196 - val_mae: 15.7867\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 292.5211 - mae: 13.3080 - val_loss: 324.6387 - val_mae: 14.5862\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 281.5302 - mae: 13.2191 - val_loss: 326.8111 - val_mae: 13.4154\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 228.5993 - mae: 11.79 - 0s 3ms/step - loss: 285.5660 - mae: 12.9183 - val_loss: 307.7824 - val_mae: 13.8775\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 277.9250 - mae: 12.9194 - val_loss: 320.9445 - val_mae: 14.9621\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 274.0985 - mae: 12.8506 - val_loss: 301.9467 - val_mae: 13.9709\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 265.0971 - mae: 12.6682 - val_loss: 290.5689 - val_mae: 13.1793\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 270.3377 - mae: 12.6553 - val_loss: 289.4821 - val_mae: 13.6414\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 260.8673 - mae: 12.4856 - val_loss: 296.4379 - val_mae: 14.2777\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 253.6711 - mae: 12.4327 - val_loss: 283.5297 - val_mae: 13.6597\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 253.2646 - mae: 12.2777 - val_loss: 276.9581 - val_mae: 13.4413\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 243.4291 - mae: 12.2087 - val_loss: 274.1738 - val_mae: 12.2897\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 247.4723 - mae: 12.1544 - val_loss: 263.3835 - val_mae: 13.0816\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 243.5577 - mae: 12.1514 - val_loss: 256.1064 - val_mae: 12.6068\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 231.8094 - mae: 11.7256 - val_loss: 258.1775 - val_mae: 13.0709\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 231.5023 - mae: 11.8515 - val_loss: 249.2698 - val_mae: 12.5463\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 225.3420 - mae: 11.6348 - val_loss: 241.9324 - val_mae: 12.1541\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 222.7816 - mae: 11.5744 - val_loss: 242.7321 - val_mae: 12.3533\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 225.3560 - mae: 11.6900 - val_loss: 234.4469 - val_mae: 11.7945\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 215.9998 - mae: 11.3572 - val_loss: 234.6268 - val_mae: 12.1559\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 209.8962 - mae: 11.2638 - val_loss: 227.4688 - val_mae: 11.6993\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 212.1461 - mae: 11.2697 - val_loss: 226.5170 - val_mae: 11.8640\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 209.0641 - mae: 11.2168 - val_loss: 227.4524 - val_mae: 12.0149\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 208.6689 - mae: 11.2569 - val_loss: 237.7519 - val_mae: 11.2454\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 204.5759 - mae: 11.0543 - val_loss: 218.3912 - val_mae: 11.8458\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 197.8302 - mae: 10.8240 - val_loss: 209.3849 - val_mae: 11.3237\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 194.3344 - mae: 10.7568 - val_loss: 214.7337 - val_mae: 11.7695\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 192.9615 - mae: 10.7399 - val_loss: 202.6106 - val_mae: 11.0370\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 188.9916 - mae: 10.4469 - val_loss: 231.9617 - val_mae: 12.7878\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 194.3106 - mae: 10.9776 - val_loss: 199.0377 - val_mae: 10.9192\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 184.8253 - mae: 10.4317 - val_loss: 196.1425 - val_mae: 10.8971\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 183.7450 - mae: 10.3690 - val_loss: 189.9934 - val_mae: 10.6668\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.7500 - mae: 10.3142 - val_loss: 185.0566 - val_mae: 10.4907\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.3039 - mae: 10.2772 - val_loss: 186.7085 - val_mae: 10.6692\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.5978 - mae: 10.1907 - val_loss: 182.5337 - val_mae: 10.2084\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 174.7204 - mae: 10.0287 - val_loss: 178.9846 - val_mae: 10.1422\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 172.8010 - mae: 10.0372 - val_loss: 193.8335 - val_mae: 11.3586\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 168.7402 - mae: 10.0152 - val_loss: 194.1748 - val_mae: 11.3984\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.5461 - mae: 9.8098 - val_loss: 192.3358 - val_mae: 11.3540\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.6243 - mae: 9.9766 - val_loss: 175.9780 - val_mae: 10.4539\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.5091 - mae: 9.7010 - val_loss: 166.3544 - val_mae: 9.7478\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 163.8596 - mae: 9.7461 - val_loss: 163.8026 - val_mae: 9.6410\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 159.1793 - mae: 9.6640 - val_loss: 163.0911 - val_mae: 9.6550\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 158.4860 - mae: 9.5382 - val_loss: 167.8070 - val_mae: 9.5630\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.5979 - mae: 9.6265 - val_loss: 159.3781 - val_mae: 9.7052\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.6172 - mae: 9.6066 - val_loss: 168.5916 - val_mae: 10.3584\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 153.1655 - mae: 9.4069 - val_loss: 159.8594 - val_mae: 9.8413\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.3875 - mae: 9.3093 - val_loss: 173.4243 - val_mae: 10.6458\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.0508 - mae: 9.4597 - val_loss: 150.7966 - val_mae: 9.3099\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.7210 - mae: 9.2583 - val_loss: 150.6476 - val_mae: 9.3144\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.9983 - mae: 9.1323 - val_loss: 151.0911 - val_mae: 9.4857\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.9262 - mae: 9.1114 - val_loss: 146.7224 - val_mae: 9.1870\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.9888 - mae: 8.9941 - val_loss: 142.2585 - val_mae: 9.0006\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.4708 - mae: 9.0638 - val_loss: 142.5901 - val_mae: 9.0488\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.6137 - mae: 8.9302 - val_loss: 141.1021 - val_mae: 8.9828\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.5210 - mae: 8.8586 - val_loss: 138.4198 - val_mae: 8.8111\n",
      "5/5 [==============================] - 0s 697us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 259.4425 - mae: 13.2205 - val_loss: 158.1159 - val_mae: 10.5792\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.3629 - mae: 13.2178 - val_loss: 158.0516 - val_mae: 10.5768\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.2840 - mae: 13.2152 - val_loss: 157.9874 - val_mae: 10.5743\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.2035 - mae: 13.2125 - val_loss: 157.9240 - val_mae: 10.5719\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.1241 - mae: 13.2099 - val_loss: 157.8599 - val_mae: 10.5694\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.0459 - mae: 13.2072 - val_loss: 157.7958 - val_mae: 10.5670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.9656 - mae: 13.2046 - val_loss: 157.7328 - val_mae: 10.5646\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.8857 - mae: 13.2019 - val_loss: 157.6702 - val_mae: 10.5622\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.8078 - mae: 13.1993 - val_loss: 157.6064 - val_mae: 10.5597\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.7289 - mae: 13.1966 - val_loss: 157.5420 - val_mae: 10.5572\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.6479 - mae: 13.1939 - val_loss: 157.4786 - val_mae: 10.5548\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 785us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 5764.8555 - mae: 63.9291 - val_loss: 4132.2432 - val_mae: 53.4474\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5763.3550 - mae: 63.9205 - val_loss: 4131.0776 - val_mae: 53.4393\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5761.8550 - mae: 63.9119 - val_loss: 4129.9072 - val_mae: 53.4312\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5760.3809 - mae: 63.9033 - val_loss: 4128.7212 - val_mae: 53.4230\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5758.8906 - mae: 63.8946 - val_loss: 4127.5464 - val_mae: 53.4150\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5757.3843 - mae: 63.8860 - val_loss: 4126.3745 - val_mae: 53.4069\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5755.8940 - mae: 63.8775 - val_loss: 4125.2065 - val_mae: 53.3988\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5754.4009 - mae: 63.8688 - val_loss: 4124.0640 - val_mae: 53.3909\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5752.9629 - mae: 63.8605 - val_loss: 4122.8931 - val_mae: 53.3828\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5751.4761 - mae: 63.8519 - val_loss: 4121.7246 - val_mae: 53.3748\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5749.9893 - mae: 63.8433 - val_loss: 4120.5630 - val_mae: 53.3667\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5748.5034 - mae: 63.8348 - val_loss: 4119.3926 - val_mae: 53.3587\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5747.0161 - mae: 63.8262 - val_loss: 4118.2319 - val_mae: 53.3506\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5745.5088 - mae: 63.8175 - val_loss: 4117.0894 - val_mae: 53.3427\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5744.0542 - mae: 63.8091 - val_loss: 4115.9351 - val_mae: 53.3347\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5742.6045 - mae: 63.8006 - val_loss: 4114.7534 - val_mae: 53.3266\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5741.0957 - mae: 63.7919 - val_loss: 4113.5942 - val_mae: 53.3185\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5739.6196 - mae: 63.7835 - val_loss: 4112.4287 - val_mae: 53.3105\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5738.1348 - mae: 63.7748 - val_loss: 4111.2583 - val_mae: 53.3024\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5736.6636 - mae: 63.7663 - val_loss: 4110.0806 - val_mae: 53.2942\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5735.1753 - mae: 63.7576 - val_loss: 4108.9165 - val_mae: 53.2862\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5733.6816 - mae: 63.7492 - val_loss: 4107.7720 - val_mae: 53.2782\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5732.2432 - mae: 63.7407 - val_loss: 4106.6152 - val_mae: 53.2702\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5730.7749 - mae: 63.7321 - val_loss: 4105.4492 - val_mae: 53.2621\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5729.2778 - mae: 63.7236 - val_loss: 4104.2896 - val_mae: 53.2540\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5727.8140 - mae: 63.7150 - val_loss: 4103.1274 - val_mae: 53.2459\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5726.3672 - mae: 63.7066 - val_loss: 4101.9478 - val_mae: 53.2378\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5724.8550 - mae: 63.6978 - val_loss: 4100.8081 - val_mae: 53.2299\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5723.4043 - mae: 63.6895 - val_loss: 4099.6553 - val_mae: 53.2218\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5721.9346 - mae: 63.6810 - val_loss: 4098.5024 - val_mae: 53.2138\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5720.4824 - mae: 63.6725 - val_loss: 4097.3213 - val_mae: 53.2056\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5718.9927 - mae: 63.6639 - val_loss: 4096.1475 - val_mae: 53.1975\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5717.4985 - mae: 63.6553 - val_loss: 4094.9878 - val_mae: 53.1894\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5716.0518 - mae: 63.6468 - val_loss: 4093.8096 - val_mae: 53.1813\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5714.5210 - mae: 63.6380 - val_loss: 4092.6638 - val_mae: 53.1733\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5713.0806 - mae: 63.6297 - val_loss: 4091.4817 - val_mae: 53.1651\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5711.5981 - mae: 63.6210 - val_loss: 4090.3208 - val_mae: 53.1571\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5710.0991 - mae: 63.6124 - val_loss: 4089.1714 - val_mae: 53.1491\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5708.6353 - mae: 63.6040 - val_loss: 4088.0115 - val_mae: 53.1410\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5707.1748 - mae: 63.5954 - val_loss: 4086.8530 - val_mae: 53.1330\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5705.7163 - mae: 63.5869 - val_loss: 4085.6853 - val_mae: 53.1249\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5704.2251 - mae: 63.5782 - val_loss: 4084.5271 - val_mae: 53.1169\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5702.7739 - mae: 63.5698 - val_loss: 4083.3599 - val_mae: 53.1088\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5701.2715 - mae: 63.5611 - val_loss: 4082.2217 - val_mae: 53.1009\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5699.8364 - mae: 63.5528 - val_loss: 4081.0605 - val_mae: 53.0928\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5698.3608 - mae: 63.5442 - val_loss: 4079.9082 - val_mae: 53.0848\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5696.9121 - mae: 63.5359 - val_loss: 4078.7502 - val_mae: 53.0767\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5695.4272 - mae: 63.5272 - val_loss: 4077.6077 - val_mae: 53.0687\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5693.9717 - mae: 63.5188 - val_loss: 4076.4607 - val_mae: 53.0607\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5692.5190 - mae: 63.5103 - val_loss: 4075.3162 - val_mae: 53.0528\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5691.0776 - mae: 63.5019 - val_loss: 4074.1689 - val_mae: 53.0448\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5689.6367 - mae: 63.4935 - val_loss: 4073.0205 - val_mae: 53.0368\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5688.1890 - mae: 63.4850 - val_loss: 4071.8845 - val_mae: 53.0289\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5686.7568 - mae: 63.4768 - val_loss: 4070.7366 - val_mae: 53.0209\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5685.2954 - mae: 63.4683 - val_loss: 4069.6050 - val_mae: 53.0130\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5683.8345 - mae: 63.4598 - val_loss: 4068.4685 - val_mae: 53.0051\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5682.4033 - mae: 63.4515 - val_loss: 4067.3115 - val_mae: 52.9970\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5680.9385 - mae: 63.4429 - val_loss: 4066.1733 - val_mae: 52.9891\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5679.4966 - mae: 63.4346 - val_loss: 4065.0261 - val_mae: 52.9811\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5678.0503 - mae: 63.4261 - val_loss: 4063.8635 - val_mae: 52.9730\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5676.5742 - mae: 63.4175 - val_loss: 4062.7036 - val_mae: 52.9649\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5675.1060 - mae: 63.4089 - val_loss: 4061.5569 - val_mae: 52.9569\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5673.6528 - mae: 63.4005 - val_loss: 4060.4138 - val_mae: 52.9489\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5672.2046 - mae: 63.3921 - val_loss: 4059.2646 - val_mae: 52.9409\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5670.7583 - mae: 63.3837 - val_loss: 4058.1077 - val_mae: 52.9329\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5669.2729 - mae: 63.3751 - val_loss: 4056.9810 - val_mae: 52.9250\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5667.8643 - mae: 63.3669 - val_loss: 4055.8293 - val_mae: 52.9170\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5666.3770 - mae: 63.3583 - val_loss: 4054.7012 - val_mae: 52.9091\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5664.9595 - mae: 63.3500 - val_loss: 4053.5513 - val_mae: 52.9010\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5663.5215 - mae: 63.3415 - val_loss: 4052.3975 - val_mae: 52.8930\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5662.0503 - mae: 63.3330 - val_loss: 4051.2419 - val_mae: 52.8850\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5660.5684 - mae: 63.3244 - val_loss: 4050.1072 - val_mae: 52.8770\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5659.1338 - mae: 63.3161 - val_loss: 4048.9541 - val_mae: 52.8690\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5657.7002 - mae: 63.3077 - val_loss: 4047.7925 - val_mae: 52.8609\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5656.2056 - mae: 63.2991 - val_loss: 4046.6604 - val_mae: 52.8530\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5654.7676 - mae: 63.2908 - val_loss: 4045.5200 - val_mae: 52.8450\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5653.3208 - mae: 63.2822 - val_loss: 4044.3777 - val_mae: 52.8371\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5651.8779 - mae: 63.2739 - val_loss: 4043.2280 - val_mae: 52.8290\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5650.4248 - mae: 63.2654 - val_loss: 4042.0996 - val_mae: 52.8211\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5648.9971 - mae: 63.2571 - val_loss: 4040.9548 - val_mae: 52.8131\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5647.5161 - mae: 63.2485 - val_loss: 4039.8237 - val_mae: 52.8052\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5646.0928 - mae: 63.2402 - val_loss: 4038.6865 - val_mae: 52.7972\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5644.6431 - mae: 63.2318 - val_loss: 4037.5576 - val_mae: 52.7893\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5643.2417 - mae: 63.2235 - val_loss: 4036.3989 - val_mae: 52.7813\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5641.7734 - mae: 63.2150 - val_loss: 4035.2495 - val_mae: 52.7733\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5640.2798 - mae: 63.2064 - val_loss: 4034.1221 - val_mae: 52.7654\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5638.8643 - mae: 63.1981 - val_loss: 4032.9846 - val_mae: 52.7574\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5637.4038 - mae: 63.1897 - val_loss: 4031.8535 - val_mae: 52.7495\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5635.9849 - mae: 63.1812 - val_loss: 4030.6985 - val_mae: 52.7414\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5634.5098 - mae: 63.1727 - val_loss: 4029.5498 - val_mae: 52.7334\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5633.0405 - mae: 63.1642 - val_loss: 4028.4197 - val_mae: 52.7255\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5631.6426 - mae: 63.1560 - val_loss: 4027.2583 - val_mae: 52.7173\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5630.1528 - mae: 63.1473 - val_loss: 4026.1228 - val_mae: 52.7094\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5628.7349 - mae: 63.1390 - val_loss: 4024.9810 - val_mae: 52.7014\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5627.2988 - mae: 63.1306 - val_loss: 4023.8362 - val_mae: 52.6934\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5625.8159 - mae: 63.1221 - val_loss: 4022.7090 - val_mae: 52.6855\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5624.4087 - mae: 63.1138 - val_loss: 4021.5591 - val_mae: 52.6775\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5622.9316 - mae: 63.1053 - val_loss: 4020.4285 - val_mae: 52.6696\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5621.5195 - mae: 63.0970 - val_loss: 4019.2913 - val_mae: 52.6616\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5620.0498 - mae: 63.0884 - val_loss: 4018.1631 - val_mae: 52.6537\n",
      "5/5 [==============================] - 0s 835us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 496.8282 - mae: 20.4608 - val_loss: 508.4523 - val_mae: 21.0504\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.7999 - mae: 20.4600 - val_loss: 508.4390 - val_mae: 21.0502\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.7755 - mae: 20.4594 - val_loss: 508.4254 - val_mae: 21.0499\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.7520 - mae: 20.4587 - val_loss: 508.4115 - val_mae: 21.0496\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.7265 - mae: 20.4580 - val_loss: 508.3979 - val_mae: 21.0492\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.7037 - mae: 20.4573 - val_loss: 508.3840 - val_mae: 21.0489\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.6778 - mae: 20.4566 - val_loss: 508.3705 - val_mae: 21.0486\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.6543 - mae: 20.4559 - val_loss: 508.3570 - val_mae: 21.0483\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.6292 - mae: 20.4552 - val_loss: 508.3435 - val_mae: 21.0480\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.6039 - mae: 20.4545 - val_loss: 508.3298 - val_mae: 21.0477\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 496.5811 - mae: 20.4539 - val_loss: 508.3163 - val_mae: 21.0474\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 721us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   0.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 85704491466735494953930063872.0000 - mae: 79929374932992.0000 - val_loss: 4699560513153097181747604088684544.0000 - val_mae: 67100892540698624.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 17322127843505214934705242112.0000 - val_loss: inf - val_mae: 14298254601853107145622698328064.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 736us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 5289934723937919980405784576.0000 - mae: 19928532910080.0000 - val_loss: 257071131471143027571288814125056.0000 - val_mae: 15739795640680448.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 3389805862106531754414178304.0000 - val_loss: inf - val_mae: 2795703519769995023567823044608.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 702us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 355504028475051992303009792.0000 - mae: 5357587398656.0000 - val_loss: 15525901846710288467728374169600.0000 - val_mae: 3852755852591104.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: inf - mae: 763679561812600931202629632.0000 - val_loss: inf - val_mae: 602265876024824313613868072960.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00011: early stopping\n",
      "5/5 [==============================] - 0s 604us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 20941.0820 - mae: 142.3782 - val_loss: 20084.8965 - val_mae: 139.1639\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20919.8379 - mae: 142.3055 - val_loss: 20064.1270 - val_mae: 139.0908\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20898.5234 - mae: 142.2324 - val_loss: 20043.3789 - val_mae: 139.0178\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20877.2578 - mae: 142.1594 - val_loss: 20022.6426 - val_mae: 138.9448\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20856.1699 - mae: 142.0868 - val_loss: 20001.9102 - val_mae: 138.8717\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20834.7422 - mae: 142.0136 - val_loss: 19981.2871 - val_mae: 138.7990\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20813.7070 - mae: 141.9410 - val_loss: 19960.5488 - val_mae: 138.7259\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20792.2793 - mae: 141.8680 - val_loss: 19939.9883 - val_mae: 138.6533\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20771.3164 - mae: 141.7953 - val_loss: 19919.2910 - val_mae: 138.5802\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20750.1094 - mae: 141.7224 - val_loss: 19898.7363 - val_mae: 138.5076\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20729.1230 - mae: 141.6502 - val_loss: 19878.1348 - val_mae: 138.4348\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20707.9434 - mae: 141.5775 - val_loss: 19857.7090 - val_mae: 138.3626\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20686.8965 - mae: 141.5050 - val_loss: 19837.2383 - val_mae: 138.2901\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20665.9434 - mae: 141.4329 - val_loss: 19816.6953 - val_mae: 138.2174\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20644.6992 - mae: 141.3599 - val_loss: 19796.2363 - val_mae: 138.1449\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20623.6309 - mae: 141.2871 - val_loss: 19775.6641 - val_mae: 138.0720\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20602.6738 - mae: 141.2147 - val_loss: 19755.0371 - val_mae: 137.9989\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20581.6738 - mae: 141.1419 - val_loss: 19734.4844 - val_mae: 137.9259\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20560.6094 - mae: 141.0690 - val_loss: 19714.1270 - val_mae: 137.8537\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20539.5996 - mae: 140.9967 - val_loss: 19693.8223 - val_mae: 137.7816\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20518.6348 - mae: 140.9245 - val_loss: 19673.4707 - val_mae: 137.7093\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20497.8281 - mae: 140.8521 - val_loss: 19652.9727 - val_mae: 137.6364\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20476.9883 - mae: 140.7796 - val_loss: 19632.4473 - val_mae: 137.5634\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20455.8594 - mae: 140.7068 - val_loss: 19612.1426 - val_mae: 137.4911\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20435.0215 - mae: 140.6346 - val_loss: 19591.8301 - val_mae: 137.4188\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20414.1992 - mae: 140.5622 - val_loss: 19571.5254 - val_mae: 137.3464\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20393.3105 - mae: 140.4899 - val_loss: 19551.2168 - val_mae: 137.2740\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20372.4492 - mae: 140.4176 - val_loss: 19530.9629 - val_mae: 137.2018\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20351.9258 - mae: 140.3455 - val_loss: 19510.4883 - val_mae: 137.1287\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20330.6641 - mae: 140.2724 - val_loss: 19490.3418 - val_mae: 137.0568\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20309.9961 - mae: 140.2005 - val_loss: 19470.0664 - val_mae: 136.9843\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20289.2520 - mae: 140.1281 - val_loss: 19449.6895 - val_mae: 136.9115\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20268.5469 - mae: 140.0556 - val_loss: 19429.3164 - val_mae: 136.8386\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20247.5312 - mae: 139.9829 - val_loss: 19409.2734 - val_mae: 136.7669\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20226.9043 - mae: 139.9112 - val_loss: 19389.2031 - val_mae: 136.6951\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20206.2090 - mae: 139.8389 - val_loss: 19369.0977 - val_mae: 136.6230\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20185.7559 - mae: 139.7669 - val_loss: 19348.6367 - val_mae: 136.5497\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20164.6465 - mae: 139.6940 - val_loss: 19328.4961 - val_mae: 136.4775\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20144.1387 - mae: 139.6218 - val_loss: 19308.2227 - val_mae: 136.4047\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20123.3105 - mae: 139.5494 - val_loss: 19288.1250 - val_mae: 136.3326\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20102.6133 - mae: 139.4772 - val_loss: 19268.0586 - val_mae: 136.2605\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20081.9004 - mae: 139.4050 - val_loss: 19248.0977 - val_mae: 136.1888\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20061.3105 - mae: 139.3331 - val_loss: 19228.1094 - val_mae: 136.1169\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20040.8945 - mae: 139.2615 - val_loss: 19207.9336 - val_mae: 136.0443\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20020.0801 - mae: 139.1889 - val_loss: 19187.9844 - val_mae: 135.9725\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19999.7266 - mae: 139.1171 - val_loss: 19167.9023 - val_mae: 135.9002\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19979.2578 - mae: 139.0450 - val_loss: 19147.7949 - val_mae: 135.8277\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19958.4863 - mae: 138.9727 - val_loss: 19127.9395 - val_mae: 135.7561\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19938.1582 - mae: 138.9010 - val_loss: 19108.0020 - val_mae: 135.6842\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19917.6660 - mae: 138.8293 - val_loss: 19088.0645 - val_mae: 135.6122\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19897.1582 - mae: 138.7573 - val_loss: 19068.1973 - val_mae: 135.5405\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19876.7852 - mae: 138.6855 - val_loss: 19048.2148 - val_mae: 135.4683\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19856.3262 - mae: 138.6134 - val_loss: 19028.2871 - val_mae: 135.3963\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19835.7422 - mae: 138.5414 - val_loss: 19008.4121 - val_mae: 135.3244\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19815.5195 - mae: 138.4695 - val_loss: 18988.3730 - val_mae: 135.2518\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19794.8164 - mae: 138.3971 - val_loss: 18968.5605 - val_mae: 135.1801\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19774.5508 - mae: 138.3254 - val_loss: 18948.6309 - val_mae: 135.1079\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19754.0449 - mae: 138.2534 - val_loss: 18928.7988 - val_mae: 135.0360\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19733.5410 - mae: 138.1814 - val_loss: 18909.0527 - val_mae: 134.9644\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19713.5547 - mae: 138.1098 - val_loss: 18888.9863 - val_mae: 134.8916\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19692.8594 - mae: 138.0372 - val_loss: 18869.2441 - val_mae: 134.8199\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19672.5391 - mae: 137.9656 - val_loss: 18849.4902 - val_mae: 134.7481\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19652.2070 - mae: 137.8937 - val_loss: 18829.6855 - val_mae: 134.6761\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19631.8574 - mae: 137.8218 - val_loss: 18809.9082 - val_mae: 134.6042\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19611.6055 - mae: 137.7498 - val_loss: 18790.0664 - val_mae: 134.5320\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19591.2520 - mae: 137.6779 - val_loss: 18770.2383 - val_mae: 134.4598\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19571.0020 - mae: 137.6057 - val_loss: 18750.3594 - val_mae: 134.3874\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19550.5391 - mae: 137.5337 - val_loss: 18730.6582 - val_mae: 134.3156\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19530.5000 - mae: 137.4619 - val_loss: 18710.8652 - val_mae: 134.2434\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19510.0645 - mae: 137.3899 - val_loss: 18691.2500 - val_mae: 134.1719\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19489.8145 - mae: 137.3181 - val_loss: 18671.6953 - val_mae: 134.1005\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19469.7480 - mae: 137.2467 - val_loss: 18651.9102 - val_mae: 134.0282\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19449.4648 - mae: 137.1746 - val_loss: 18632.2969 - val_mae: 133.9565\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19429.2070 - mae: 137.1030 - val_loss: 18612.6719 - val_mae: 133.8848\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19409.2891 - mae: 137.0313 - val_loss: 18592.9219 - val_mae: 133.8125\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19388.9688 - mae: 136.9593 - val_loss: 18573.3574 - val_mae: 133.7409\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19368.6914 - mae: 136.8876 - val_loss: 18553.8281 - val_mae: 133.6694\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19348.7227 - mae: 136.8159 - val_loss: 18534.1992 - val_mae: 133.5974\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19328.5742 - mae: 136.7441 - val_loss: 18514.5801 - val_mae: 133.5255\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19308.4219 - mae: 136.6722 - val_loss: 18494.9883 - val_mae: 133.4536\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19288.3496 - mae: 136.6006 - val_loss: 18475.4414 - val_mae: 133.3819\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19268.2617 - mae: 136.5289 - val_loss: 18455.9336 - val_mae: 133.3102\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19248.3730 - mae: 136.4574 - val_loss: 18436.3594 - val_mae: 133.2383\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19228.2480 - mae: 136.3857 - val_loss: 18416.9434 - val_mae: 133.1669\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19208.3477 - mae: 136.3143 - val_loss: 18397.5352 - val_mae: 133.0955\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19188.3438 - mae: 136.2429 - val_loss: 18378.1738 - val_mae: 133.0243\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19168.3535 - mae: 136.1716 - val_loss: 18358.8418 - val_mae: 132.9531\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19148.6562 - mae: 136.1007 - val_loss: 18339.3496 - val_mae: 132.8812\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19128.6875 - mae: 136.0291 - val_loss: 18319.9258 - val_mae: 132.8096\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19108.6836 - mae: 135.9576 - val_loss: 18300.6680 - val_mae: 132.7386\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19088.9766 - mae: 135.8866 - val_loss: 18281.2402 - val_mae: 132.6669\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19068.8086 - mae: 135.8149 - val_loss: 18262.0918 - val_mae: 132.5962\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19049.0410 - mae: 135.7438 - val_loss: 18242.7324 - val_mae: 132.5247\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19029.2168 - mae: 135.6724 - val_loss: 18223.3516 - val_mae: 132.4530\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19009.4395 - mae: 135.6008 - val_loss: 18203.9316 - val_mae: 132.3812\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18989.4375 - mae: 135.5292 - val_loss: 18184.6641 - val_mae: 132.3099\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18969.7344 - mae: 135.4580 - val_loss: 18165.2637 - val_mae: 132.2381\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18949.8516 - mae: 135.3862 - val_loss: 18145.9238 - val_mae: 132.1664\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18929.9062 - mae: 135.3147 - val_loss: 18126.6758 - val_mae: 132.0951\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18910.2402 - mae: 135.2435 - val_loss: 18107.4082 - val_mae: 132.0236\n",
      "5/5 [==============================] - 0s 609us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3511.3940 - mae: 43.4124 - val_loss: 4329.6460 - val_mae: 49.0970\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3508.3181 - mae: 43.4073 - val_loss: 4325.2461 - val_mae: 49.0892\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3505.3413 - mae: 43.4046 - val_loss: 4320.7505 - val_mae: 49.0823\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3502.2227 - mae: 43.4000 - val_loss: 4316.4175 - val_mae: 49.0747\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3499.2097 - mae: 43.3947 - val_loss: 4312.2090 - val_mae: 49.0667\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3496.2625 - mae: 43.3893 - val_loss: 4308.0400 - val_mae: 49.0586\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3493.4287 - mae: 43.3838 - val_loss: 4303.8442 - val_mae: 49.0506\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3490.6953 - mae: 43.3810 - val_loss: 4299.3862 - val_mae: 49.0434\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3487.4961 - mae: 43.3750 - val_loss: 4295.2949 - val_mae: 49.0356\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3484.6570 - mae: 43.3710 - val_loss: 4291.1772 - val_mae: 49.0281\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3481.8591 - mae: 43.3652 - val_loss: 4286.9751 - val_mae: 49.0198\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3478.9226 - mae: 43.3602 - val_loss: 4282.9365 - val_mae: 49.0118\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3476.0889 - mae: 43.3556 - val_loss: 4278.8467 - val_mae: 49.0039\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3473.5217 - mae: 43.3536 - val_loss: 4274.4878 - val_mae: 48.9961\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3470.4963 - mae: 43.3486 - val_loss: 4270.3457 - val_mae: 48.9882\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3467.5776 - mae: 43.3428 - val_loss: 4266.4902 - val_mae: 48.9794\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3464.9707 - mae: 43.3391 - val_loss: 4262.3931 - val_mae: 48.9715\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3462.0740 - mae: 43.3346 - val_loss: 4258.3770 - val_mae: 48.9637\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3459.3342 - mae: 43.3305 - val_loss: 4254.3042 - val_mae: 48.9551\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3456.4685 - mae: 43.3239 - val_loss: 4250.3213 - val_mae: 48.9462\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3453.7192 - mae: 43.3199 - val_loss: 4246.2090 - val_mae: 48.9380\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3450.8542 - mae: 43.3166 - val_loss: 4242.0752 - val_mae: 48.9304\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3447.9607 - mae: 43.3120 - val_loss: 4238.0474 - val_mae: 48.9225\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3445.2783 - mae: 43.3073 - val_loss: 4233.9482 - val_mae: 48.9139\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3442.4688 - mae: 43.3027 - val_loss: 4230.0513 - val_mae: 48.9055\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3439.7891 - mae: 43.2981 - val_loss: 4226.0317 - val_mae: 48.8969\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3436.9133 - mae: 43.2920 - val_loss: 4222.1650 - val_mae: 48.8883\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3434.4680 - mae: 43.2902 - val_loss: 4217.8882 - val_mae: 48.8801\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3431.4001 - mae: 43.2853 - val_loss: 4214.0020 - val_mae: 48.8720\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3428.7332 - mae: 43.2832 - val_loss: 4210.0020 - val_mae: 48.8637\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3425.9985 - mae: 43.2779 - val_loss: 4206.1807 - val_mae: 48.8547\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3423.3167 - mae: 43.2727 - val_loss: 4202.3535 - val_mae: 48.8461\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3420.6326 - mae: 43.2682 - val_loss: 4198.6069 - val_mae: 48.8374\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3417.9612 - mae: 43.2627 - val_loss: 4194.8350 - val_mae: 48.8279\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3415.5698 - mae: 43.2602 - val_loss: 4190.6665 - val_mae: 48.8189\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3412.5210 - mae: 43.2538 - val_loss: 4187.0801 - val_mae: 48.8098\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3410.0605 - mae: 43.2497 - val_loss: 4183.1973 - val_mae: 48.8013\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3407.3555 - mae: 43.2455 - val_loss: 4179.3467 - val_mae: 48.7926\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3404.7390 - mae: 43.2423 - val_loss: 4175.4434 - val_mae: 48.7841\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3402.0105 - mae: 43.2359 - val_loss: 4171.5791 - val_mae: 48.7750\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3399.3210 - mae: 43.2307 - val_loss: 4167.7715 - val_mae: 48.7649\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3396.6636 - mae: 43.2262 - val_loss: 4163.9497 - val_mae: 48.7560\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3394.0615 - mae: 43.2212 - val_loss: 4160.1489 - val_mae: 48.7470\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3391.5774 - mae: 43.2180 - val_loss: 4156.1963 - val_mae: 48.7385\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3388.7107 - mae: 43.2112 - val_loss: 4152.6265 - val_mae: 48.7289\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3386.1978 - mae: 43.2074 - val_loss: 4148.8696 - val_mae: 48.7205\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3383.5276 - mae: 43.2022 - val_loss: 4145.1606 - val_mae: 48.7117\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3381.0625 - mae: 43.1988 - val_loss: 4141.2437 - val_mae: 48.7025\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3378.4329 - mae: 43.1943 - val_loss: 4137.4316 - val_mae: 48.6935\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3375.7595 - mae: 43.1894 - val_loss: 4133.6377 - val_mae: 48.6840\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3373.0774 - mae: 43.1844 - val_loss: 4129.8149 - val_mae: 48.6746\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3370.3887 - mae: 43.1795 - val_loss: 4125.9473 - val_mae: 48.6654\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3367.8076 - mae: 43.1757 - val_loss: 4122.0542 - val_mae: 48.6559\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3365.0325 - mae: 43.1702 - val_loss: 4118.3628 - val_mae: 48.6466\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3362.4578 - mae: 43.1655 - val_loss: 4114.5439 - val_mae: 48.6379\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3359.9473 - mae: 43.1623 - val_loss: 4110.7275 - val_mae: 48.6289\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3357.3347 - mae: 43.1585 - val_loss: 4107.1035 - val_mae: 48.6195\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3354.8337 - mae: 43.1537 - val_loss: 4103.4043 - val_mae: 48.6102\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3352.2432 - mae: 43.1489 - val_loss: 4099.7417 - val_mae: 48.6007\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3349.7104 - mae: 43.1445 - val_loss: 4096.1870 - val_mae: 48.5912\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3347.1807 - mae: 43.1394 - val_loss: 4092.7380 - val_mae: 48.5814\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3344.8115 - mae: 43.1344 - val_loss: 4089.1514 - val_mae: 48.5716\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3342.2944 - mae: 43.1295 - val_loss: 4085.5293 - val_mae: 48.5618\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3339.6616 - mae: 43.1239 - val_loss: 4082.0300 - val_mae: 48.5525\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3337.2539 - mae: 43.1204 - val_loss: 4078.2957 - val_mae: 48.5432\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3334.8132 - mae: 43.1178 - val_loss: 4074.4688 - val_mae: 48.5336\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3332.0679 - mae: 43.1115 - val_loss: 4070.9944 - val_mae: 48.5236\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3329.7795 - mae: 43.1092 - val_loss: 4067.2156 - val_mae: 48.5145\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3327.1248 - mae: 43.1044 - val_loss: 4063.7322 - val_mae: 48.5047\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3324.7241 - mae: 43.0996 - val_loss: 4060.2036 - val_mae: 48.4949\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3322.2905 - mae: 43.0951 - val_loss: 4056.6650 - val_mae: 48.4848\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3319.9004 - mae: 43.0904 - val_loss: 4053.0222 - val_mae: 48.4750\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3317.2634 - mae: 43.0847 - val_loss: 4049.6777 - val_mae: 48.4651\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3314.8687 - mae: 43.0790 - val_loss: 4046.2705 - val_mae: 48.4547\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3312.4412 - mae: 43.0747 - val_loss: 4042.7410 - val_mae: 48.4452\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3310.0847 - mae: 43.0708 - val_loss: 4039.1248 - val_mae: 48.4352\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3307.6926 - mae: 43.0668 - val_loss: 4035.4902 - val_mae: 48.4250\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3304.9990 - mae: 43.0590 - val_loss: 4032.2693 - val_mae: 48.4145\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3302.7856 - mae: 43.0556 - val_loss: 4028.5818 - val_mae: 48.4049\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3300.2397 - mae: 43.0505 - val_loss: 4025.1279 - val_mae: 48.3951\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3297.9043 - mae: 43.0463 - val_loss: 4021.6406 - val_mae: 48.3853\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3295.4744 - mae: 43.0410 - val_loss: 4018.1199 - val_mae: 48.3752\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3292.9592 - mae: 43.0367 - val_loss: 4014.6260 - val_mae: 48.3652\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3290.4814 - mae: 43.0300 - val_loss: 4011.3359 - val_mae: 48.3544\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3288.1084 - mae: 43.0244 - val_loss: 4007.8752 - val_mae: 48.3444\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3285.7739 - mae: 43.0204 - val_loss: 4004.3420 - val_mae: 48.3340\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3283.4167 - mae: 43.0152 - val_loss: 4000.8025 - val_mae: 48.3233\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3280.9351 - mae: 43.0088 - val_loss: 3997.4536 - val_mae: 48.3129\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3278.6990 - mae: 43.0040 - val_loss: 3994.0081 - val_mae: 48.3020\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3276.1902 - mae: 42.9980 - val_loss: 3990.7542 - val_mae: 48.2919\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3273.9653 - mae: 42.9935 - val_loss: 3987.2466 - val_mae: 48.2824\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3271.3777 - mae: 42.9863 - val_loss: 3984.0811 - val_mae: 48.2712\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3269.2971 - mae: 42.9814 - val_loss: 3980.6267 - val_mae: 48.2607\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3266.7004 - mae: 42.9748 - val_loss: 3977.4824 - val_mae: 48.2507\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3264.3496 - mae: 42.9685 - val_loss: 3974.2688 - val_mae: 48.2399\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3262.2397 - mae: 42.9628 - val_loss: 3970.7412 - val_mae: 48.2288\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3259.7051 - mae: 42.9559 - val_loss: 3967.5000 - val_mae: 48.2178\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3257.3462 - mae: 42.9506 - val_loss: 3964.1475 - val_mae: 48.2077\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3254.9817 - mae: 42.9447 - val_loss: 3960.7588 - val_mae: 48.1969\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3252.7224 - mae: 42.9394 - val_loss: 3957.3162 - val_mae: 48.1865\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 69694.7891 - mae: 254.4068 - val_loss: 73412.4297 - val_mae: 259.2350\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69655.9922 - mae: 254.3349 - val_loss: 73371.8750 - val_mae: 259.1610\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69618.2969 - mae: 254.2637 - val_loss: 73330.9531 - val_mae: 259.0863\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69579.9766 - mae: 254.1917 - val_loss: 73290.1797 - val_mae: 259.0119\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69541.7734 - mae: 254.1201 - val_loss: 73249.4141 - val_mae: 258.9375\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69503.1328 - mae: 254.0483 - val_loss: 73208.9062 - val_mae: 258.8635\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69465.0625 - mae: 253.9768 - val_loss: 73168.3359 - val_mae: 258.7894\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69426.9141 - mae: 253.9053 - val_loss: 73127.7656 - val_mae: 258.7153\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69388.6641 - mae: 253.8337 - val_loss: 73087.2422 - val_mae: 258.6412\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69350.6484 - mae: 253.7622 - val_loss: 73046.4844 - val_mae: 258.5667\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69312.4609 - mae: 253.6904 - val_loss: 73005.8203 - val_mae: 258.4923\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69274.2578 - mae: 253.6187 - val_loss: 72965.3594 - val_mae: 258.4183\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69236.7969 - mae: 253.5473 - val_loss: 72924.3984 - val_mae: 258.3434\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69198.1953 - mae: 253.4752 - val_loss: 72883.8359 - val_mae: 258.2692\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69160.0156 - mae: 253.4038 - val_loss: 72843.4375 - val_mae: 258.1951\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69121.7734 - mae: 253.3324 - val_loss: 72803.3359 - val_mae: 258.1217\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69083.6484 - mae: 253.2613 - val_loss: 72763.1562 - val_mae: 258.0481\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69046.2734 - mae: 253.1901 - val_loss: 72722.3438 - val_mae: 257.9733\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69008.2500 - mae: 253.1182 - val_loss: 72681.5547 - val_mae: 257.8986\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68969.7500 - mae: 253.0463 - val_loss: 72641.1172 - val_mae: 257.8245\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68931.9062 - mae: 252.9748 - val_loss: 72600.6562 - val_mae: 257.7502\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68893.9297 - mae: 252.9032 - val_loss: 72560.2422 - val_mae: 257.6761\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68855.8750 - mae: 252.8317 - val_loss: 72520.0391 - val_mae: 257.6024\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68818.3203 - mae: 252.7606 - val_loss: 72479.7031 - val_mae: 257.5283\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68780.1719 - mae: 252.6893 - val_loss: 72439.7422 - val_mae: 257.4549\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68742.2734 - mae: 252.6185 - val_loss: 72399.9219 - val_mae: 257.3817\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68704.9297 - mae: 252.5475 - val_loss: 72359.6484 - val_mae: 257.3078\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68667.5938 - mae: 252.4764 - val_loss: 72319.0859 - val_mae: 257.2332\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68629.2344 - mae: 252.4047 - val_loss: 72279.2266 - val_mae: 257.1599\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68591.3203 - mae: 252.3338 - val_loss: 72239.3672 - val_mae: 257.0866\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68554.1875 - mae: 252.2631 - val_loss: 72198.9453 - val_mae: 257.0123\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68516.6484 - mae: 252.1915 - val_loss: 72158.5625 - val_mae: 256.9380\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68478.4844 - mae: 252.1200 - val_loss: 72118.5859 - val_mae: 256.8644\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68440.7656 - mae: 252.0489 - val_loss: 72078.5625 - val_mae: 256.7907\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68403.2969 - mae: 251.9778 - val_loss: 72038.4922 - val_mae: 256.7170\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68365.4297 - mae: 251.9066 - val_loss: 71998.5703 - val_mae: 256.6434\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68327.3281 - mae: 251.8353 - val_loss: 71958.7188 - val_mae: 256.5700\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68290.0312 - mae: 251.7643 - val_loss: 71918.3203 - val_mae: 256.4955\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68252.6016 - mae: 251.6927 - val_loss: 71877.5938 - val_mae: 256.4205\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68214.4609 - mae: 251.6207 - val_loss: 71837.3672 - val_mae: 256.3463\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68176.3359 - mae: 251.5492 - val_loss: 71797.6484 - val_mae: 256.2731\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68138.8047 - mae: 251.4782 - val_loss: 71757.7891 - val_mae: 256.1996\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68101.5312 - mae: 251.4073 - val_loss: 71717.6641 - val_mae: 256.1255\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68063.7031 - mae: 251.3358 - val_loss: 71677.7109 - val_mae: 256.0517\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68026.3750 - mae: 251.2647 - val_loss: 71637.5391 - val_mae: 255.9776\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67988.2266 - mae: 251.1933 - val_loss: 71597.8906 - val_mae: 255.9043\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67950.9844 - mae: 251.1224 - val_loss: 71557.8203 - val_mae: 255.8303\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67913.6016 - mae: 251.0510 - val_loss: 71517.5859 - val_mae: 255.7560\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67876.2891 - mae: 250.9795 - val_loss: 71477.2031 - val_mae: 255.6813\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67837.7734 - mae: 250.9077 - val_loss: 71437.6406 - val_mae: 255.6081\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67800.5469 - mae: 250.8369 - val_loss: 71397.7812 - val_mae: 255.5344\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67762.7812 - mae: 250.7656 - val_loss: 71357.8438 - val_mae: 255.4605\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67725.9062 - mae: 250.6945 - val_loss: 71317.4688 - val_mae: 255.3858\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67688.0703 - mae: 250.6228 - val_loss: 71277.6875 - val_mae: 255.3122\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67650.4297 - mae: 250.5518 - val_loss: 71238.2734 - val_mae: 255.2392\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67613.4219 - mae: 250.4811 - val_loss: 71198.4531 - val_mae: 255.1655\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67576.0469 - mae: 250.4100 - val_loss: 71158.5391 - val_mae: 255.0915\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67538.5312 - mae: 250.3387 - val_loss: 71118.7812 - val_mae: 255.0178\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67501.1250 - mae: 250.2676 - val_loss: 71079.2031 - val_mae: 254.9445\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67463.8984 - mae: 250.1969 - val_loss: 71039.5547 - val_mae: 254.8709\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67427.0234 - mae: 250.1260 - val_loss: 70999.5859 - val_mae: 254.7968\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67389.2266 - mae: 250.0546 - val_loss: 70960.2188 - val_mae: 254.7237\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67351.9375 - mae: 249.9840 - val_loss: 70920.8672 - val_mae: 254.6507\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67315.4062 - mae: 249.9133 - val_loss: 70880.6797 - val_mae: 254.5761\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67276.9844 - mae: 249.8416 - val_loss: 70841.4688 - val_mae: 254.5033\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67240.2656 - mae: 249.7711 - val_loss: 70801.7422 - val_mae: 254.4295\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67203.0781 - mae: 249.6999 - val_loss: 70762.0078 - val_mae: 254.3557\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67165.7266 - mae: 249.6288 - val_loss: 70722.3438 - val_mae: 254.2819\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67128.3594 - mae: 249.5575 - val_loss: 70682.5469 - val_mae: 254.2080\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67091.1172 - mae: 249.4865 - val_loss: 70643.0000 - val_mae: 254.1344\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67054.2266 - mae: 249.4156 - val_loss: 70603.2656 - val_mae: 254.0605\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67016.6562 - mae: 249.3443 - val_loss: 70564.0078 - val_mae: 253.9875\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66980.0156 - mae: 249.2739 - val_loss: 70524.4453 - val_mae: 253.9138\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66942.3281 - mae: 249.2027 - val_loss: 70485.1172 - val_mae: 253.8406\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66905.4531 - mae: 249.1319 - val_loss: 70445.3359 - val_mae: 253.7665\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66868.1641 - mae: 249.0606 - val_loss: 70405.7031 - val_mae: 253.6927\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66830.7266 - mae: 248.9894 - val_loss: 70366.3594 - val_mae: 253.6194\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66793.7188 - mae: 248.9187 - val_loss: 70326.9375 - val_mae: 253.5459\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66756.6562 - mae: 248.8479 - val_loss: 70287.5938 - val_mae: 253.4726\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66719.8828 - mae: 248.7769 - val_loss: 70247.8125 - val_mae: 253.3984\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66682.8359 - mae: 248.7055 - val_loss: 70207.9297 - val_mae: 253.3241\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66645.2031 - mae: 248.6341 - val_loss: 70168.6797 - val_mae: 253.2508\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66608.2891 - mae: 248.5634 - val_loss: 70129.5625 - val_mae: 253.1778\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66571.3906 - mae: 248.4929 - val_loss: 70090.5312 - val_mae: 253.1049\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66534.1328 - mae: 248.4224 - val_loss: 70051.6562 - val_mae: 253.0322\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66497.5312 - mae: 248.3520 - val_loss: 70012.3125 - val_mae: 252.9587\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66460.9922 - mae: 248.2811 - val_loss: 69972.4375 - val_mae: 252.8842\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66423.4766 - mae: 248.2094 - val_loss: 69933.0625 - val_mae: 252.8106\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66386.3516 - mae: 248.1384 - val_loss: 69893.7891 - val_mae: 252.7372\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66349.3828 - mae: 248.0676 - val_loss: 69854.5000 - val_mae: 252.6637\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66312.5625 - mae: 247.9967 - val_loss: 69815.0156 - val_mae: 252.5899\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66275.7109 - mae: 247.9256 - val_loss: 69775.7109 - val_mae: 252.5164\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66238.8594 - mae: 247.8549 - val_loss: 69736.5625 - val_mae: 252.4431\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66201.4141 - mae: 247.7840 - val_loss: 69697.8438 - val_mae: 252.3706\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66165.3359 - mae: 247.7136 - val_loss: 69658.0703 - val_mae: 252.2961\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66127.8516 - mae: 247.6421 - val_loss: 69618.8125 - val_mae: 252.2225\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66091.3906 - mae: 247.5714 - val_loss: 69579.3906 - val_mae: 252.1487\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66053.8125 - mae: 247.5002 - val_loss: 69540.6016 - val_mae: 252.0760\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66017.7109 - mae: 247.4300 - val_loss: 69501.3203 - val_mae: 252.0023\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65980.4609 - mae: 247.3590 - val_loss: 69462.6797 - val_mae: 251.9298\n",
      "5/5 [==============================] - 0s 727us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 15ms/step - loss: 6733.5972 - mae: 81.4008 - val_loss: 6201.3496 - val_mae: 78.4010\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6701.7202 - mae: 81.2062 - val_loss: 6170.2021 - val_mae: 78.2037\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6669.7393 - mae: 81.0106 - val_loss: 6139.2378 - val_mae: 78.0070\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6637.9307 - mae: 80.8163 - val_loss: 6108.3496 - val_mae: 77.8102\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6606.4751 - mae: 80.6227 - val_loss: 6077.5498 - val_mae: 77.6135\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6574.7134 - mae: 80.4275 - val_loss: 6047.0439 - val_mae: 77.4182\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6543.3848 - mae: 80.2344 - val_loss: 6016.6069 - val_mae: 77.2228\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6511.8774 - mae: 80.0412 - val_loss: 5986.3506 - val_mae: 77.0280\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6480.8276 - mae: 79.8478 - val_loss: 5956.0552 - val_mae: 76.8325\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6449.6553 - mae: 79.6541 - val_loss: 5926.0249 - val_mae: 76.6382\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6418.9146 - mae: 79.4629 - val_loss: 5896.0366 - val_mae: 76.4436\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6388.0356 - mae: 79.2703 - val_loss: 5866.4512 - val_mae: 76.2511\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6357.3599 - mae: 79.0788 - val_loss: 5836.9756 - val_mae: 76.0589\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6327.1880 - mae: 78.8887 - val_loss: 5807.3481 - val_mae: 75.8651\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6296.4180 - mae: 78.6958 - val_loss: 5778.1592 - val_mae: 75.6737\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6266.2183 - mae: 78.5051 - val_loss: 5748.9619 - val_mae: 75.4818\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6236.2388 - mae: 78.3157 - val_loss: 5719.6851 - val_mae: 75.2888\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6206.1938 - mae: 78.1253 - val_loss: 5690.6035 - val_mae: 75.0967\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6176.0620 - mae: 77.9348 - val_loss: 5661.9136 - val_mae: 74.9066\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6146.4487 - mae: 77.7461 - val_loss: 5633.2178 - val_mae: 74.7159\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6116.9277 - mae: 77.5570 - val_loss: 5604.5010 - val_mae: 74.5246\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6087.3521 - mae: 77.3677 - val_loss: 5575.9404 - val_mae: 74.3339\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6058.1108 - mae: 77.1801 - val_loss: 5547.4126 - val_mae: 74.1428\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6028.7490 - mae: 76.9912 - val_loss: 5519.1377 - val_mae: 73.9529\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5999.6655 - mae: 76.8030 - val_loss: 5490.9629 - val_mae: 73.7632\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5970.5073 - mae: 76.6149 - val_loss: 5462.9229 - val_mae: 73.5739\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5941.5454 - mae: 76.4279 - val_loss: 5434.9443 - val_mae: 73.3844\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5912.8130 - mae: 76.2407 - val_loss: 5407.1606 - val_mae: 73.1958\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5884.3135 - mae: 76.0544 - val_loss: 5379.4180 - val_mae: 73.0071\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5855.4712 - mae: 75.8662 - val_loss: 5351.8853 - val_mae: 72.8192\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5827.1016 - mae: 75.6805 - val_loss: 5324.3989 - val_mae: 72.6310\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5798.7832 - mae: 75.4943 - val_loss: 5297.0073 - val_mae: 72.4431\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5770.6724 - mae: 75.3088 - val_loss: 5269.6572 - val_mae: 72.2550\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5742.2729 - mae: 75.1224 - val_loss: 5242.7104 - val_mae: 72.0691\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5714.4351 - mae: 74.9383 - val_loss: 5215.8091 - val_mae: 71.8830\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5686.5249 - mae: 74.7525 - val_loss: 5189.0742 - val_mae: 71.6977\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5659.0566 - mae: 74.5692 - val_loss: 5162.0093 - val_mae: 71.5095\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5631.0918 - mae: 74.3838 - val_loss: 5135.2354 - val_mae: 71.3228\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5603.7095 - mae: 74.2001 - val_loss: 5108.5532 - val_mae: 71.1363\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5576.1187 - mae: 74.0163 - val_loss: 5082.1050 - val_mae: 70.9509\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5548.9995 - mae: 73.8328 - val_loss: 5055.6255 - val_mae: 70.7647\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5521.5942 - mae: 73.6491 - val_loss: 5029.4058 - val_mae: 70.5799\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5494.5718 - mae: 73.4662 - val_loss: 5003.1968 - val_mae: 70.3947\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5467.7827 - mae: 73.2849 - val_loss: 4976.8911 - val_mae: 70.2082\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5440.6479 - mae: 73.1007 - val_loss: 4950.9507 - val_mae: 70.0239\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5414.0361 - mae: 72.9193 - val_loss: 4925.0728 - val_mae: 69.8395\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5387.4233 - mae: 72.7373 - val_loss: 4899.3301 - val_mae: 69.6556\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5360.7661 - mae: 72.5556 - val_loss: 4873.7432 - val_mae: 69.4721\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5334.3721 - mae: 72.3748 - val_loss: 4848.2500 - val_mae: 69.2889\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5308.0874 - mae: 72.1939 - val_loss: 4822.7075 - val_mae: 69.1047\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5281.6978 - mae: 72.0122 - val_loss: 4797.3481 - val_mae: 68.9214\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 5036.4287 - mae: 70.318 - 0s 3ms/step - loss: 5255.6270 - mae: 71.8313 - val_loss: 4771.9229 - val_mae: 68.7372\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5229.4458 - mae: 71.6500 - val_loss: 4746.7148 - val_mae: 68.5540\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5203.4980 - mae: 71.4688 - val_loss: 4721.5132 - val_mae: 68.3703\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5177.6406 - mae: 71.2890 - val_loss: 4696.4116 - val_mae: 68.1868\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5151.6021 - mae: 71.1076 - val_loss: 4671.5957 - val_mae: 68.0049\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5126.0503 - mae: 70.9284 - val_loss: 4646.7461 - val_mae: 67.8224\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5100.4829 - mae: 70.7494 - val_loss: 4621.8877 - val_mae: 67.6390\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5074.8184 - mae: 70.5681 - val_loss: 4597.2612 - val_mae: 67.4571\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5049.7290 - mae: 70.3900 - val_loss: 4572.5635 - val_mae: 67.2742\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5024.1182 - mae: 70.2098 - val_loss: 4548.2036 - val_mae: 67.0931\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4999.0381 - mae: 70.0307 - val_loss: 4523.9116 - val_mae: 66.9121\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4973.9023 - mae: 69.8523 - val_loss: 4499.7026 - val_mae: 66.7311\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4948.8872 - mae: 69.6740 - val_loss: 4475.5161 - val_mae: 66.5498\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4923.8613 - mae: 69.4952 - val_loss: 4451.5010 - val_mae: 66.3694\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4899.1484 - mae: 69.3176 - val_loss: 4427.3960 - val_mae: 66.1876\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4874.3486 - mae: 69.1389 - val_loss: 4403.4253 - val_mae: 66.0064\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4849.6089 - mae: 68.9607 - val_loss: 4379.5825 - val_mae: 65.8256\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4825.1865 - mae: 68.7823 - val_loss: 4355.8457 - val_mae: 65.6452\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4800.4888 - mae: 68.6045 - val_loss: 4332.3120 - val_mae: 65.4658\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4776.1768 - mae: 68.4273 - val_loss: 4308.8994 - val_mae: 65.2867\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4751.9453 - mae: 68.2515 - val_loss: 4285.4419 - val_mae: 65.1068\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4727.7847 - mae: 68.0738 - val_loss: 4262.2222 - val_mae: 64.9283\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4703.6602 - mae: 67.8981 - val_loss: 4239.0664 - val_mae: 64.7496\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4679.8628 - mae: 67.7221 - val_loss: 4215.8335 - val_mae: 64.5699\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4655.7695 - mae: 67.5452 - val_loss: 4192.8848 - val_mae: 64.3919\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4632.0093 - mae: 67.3692 - val_loss: 4169.9961 - val_mae: 64.2137\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4608.3506 - mae: 67.1930 - val_loss: 4147.1597 - val_mae: 64.0356\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4584.6924 - mae: 67.0175 - val_loss: 4124.3848 - val_mae: 63.8573\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4561.1133 - mae: 66.8417 - val_loss: 4101.7559 - val_mae: 63.6797\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4537.7773 - mae: 66.6665 - val_loss: 4079.1323 - val_mae: 63.5016\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4514.2378 - mae: 66.4914 - val_loss: 4056.7185 - val_mae: 63.3245\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4491.1836 - mae: 66.3170 - val_loss: 4034.2996 - val_mae: 63.1469\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4467.9287 - mae: 66.1423 - val_loss: 4012.0713 - val_mae: 62.9703\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4444.9023 - mae: 65.9687 - val_loss: 3989.9565 - val_mae: 62.7941\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4421.9697 - mae: 65.7947 - val_loss: 3967.9604 - val_mae: 62.6184\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4399.1851 - mae: 65.6209 - val_loss: 3945.9692 - val_mae: 62.4422\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4376.5156 - mae: 65.4487 - val_loss: 3923.9980 - val_mae: 62.2656\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4353.8442 - mae: 65.2753 - val_loss: 3902.0427 - val_mae: 62.0885\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4331.1216 - mae: 65.1002 - val_loss: 3880.3767 - val_mae: 61.9133\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4308.6909 - mae: 64.9281 - val_loss: 3858.6965 - val_mae: 61.7374\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4286.0806 - mae: 64.7542 - val_loss: 3837.2239 - val_mae: 61.5627\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4263.6968 - mae: 64.5818 - val_loss: 3815.7969 - val_mae: 61.3879\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4241.4893 - mae: 64.4097 - val_loss: 3794.4028 - val_mae: 61.2128\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4219.4077 - mae: 64.2368 - val_loss: 3773.0215 - val_mae: 61.0374\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4197.1523 - mae: 64.0642 - val_loss: 3751.8044 - val_mae: 60.8627\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4175.1948 - mae: 63.8929 - val_loss: 3730.6067 - val_mae: 60.6877\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4153.2461 - mae: 63.7188 - val_loss: 3709.4788 - val_mae: 60.5129\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4131.2520 - mae: 63.5471 - val_loss: 3688.4153 - val_mae: 60.3378\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4109.3999 - mae: 63.3758 - val_loss: 3667.5193 - val_mae: 60.1637\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   3.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1023.3624 - mae: 30.2141 - val_loss: 857.9648 - val_mae: 28.5548\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1008.2813 - mae: 29.9691 - val_loss: 843.5176 - val_mae: 28.3066\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 993.4005 - mae: 29.7251 - val_loss: 829.2251 - val_mae: 28.0590\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 978.5767 - mae: 29.4813 - val_loss: 815.1019 - val_mae: 27.8119\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 963.9130 - mae: 29.2380 - val_loss: 801.1660 - val_mae: 27.5660\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 949.6048 - mae: 28.9946 - val_loss: 787.3853 - val_mae: 27.3206\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 935.3326 - mae: 28.7542 - val_loss: 773.8207 - val_mae: 27.0769\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 921.2349 - mae: 28.5154 - val_loss: 760.4089 - val_mae: 26.8341\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 907.2880 - mae: 28.2753 - val_loss: 747.1733 - val_mae: 26.5919\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 893.6840 - mae: 28.0373 - val_loss: 734.0245 - val_mae: 26.3491\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 880.0872 - mae: 27.7988 - val_loss: 721.0463 - val_mae: 26.1074\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 866.6182 - mae: 27.5630 - val_loss: 708.4181 - val_mae: 25.8699\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 853.4831 - mae: 27.3287 - val_loss: 695.8885 - val_mae: 25.6321\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 840.7397 - mae: 27.0949 - val_loss: 683.3478 - val_mae: 25.3921\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 827.6878 - mae: 26.8612 - val_loss: 671.1770 - val_mae: 25.1568\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 815.1026 - mae: 26.6287 - val_loss: 659.1622 - val_mae: 24.9219\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 802.6744 - mae: 26.3995 - val_loss: 647.2753 - val_mae: 24.6877\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 790.3419 - mae: 26.1705 - val_loss: 635.5283 - val_mae: 24.4537\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 778.0784 - mae: 25.9422 - val_loss: 624.0342 - val_mae: 24.2229\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 766.1551 - mae: 25.7145 - val_loss: 612.6362 - val_mae: 23.9917\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 754.5295 - mae: 25.4869 - val_loss: 601.2338 - val_mae: 23.7582\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 742.6212 - mae: 25.2609 - val_loss: 590.1790 - val_mae: 23.5295\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 731.0931 - mae: 25.0373 - val_loss: 579.3036 - val_mae: 23.3023\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 719.8589 - mae: 24.8141 - val_loss: 568.5056 - val_mae: 23.0746\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 708.7234 - mae: 24.5920 - val_loss: 557.8708 - val_mae: 22.8479\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 697.5864 - mae: 24.3703 - val_loss: 547.4070 - val_mae: 22.6228\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 686.6445 - mae: 24.1499 - val_loss: 537.1072 - val_mae: 22.3989\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 676.0736 - mae: 23.9320 - val_loss: 526.9033 - val_mae: 22.1753\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 665.4122 - mae: 23.7145 - val_loss: 516.9758 - val_mae: 21.9550\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 655.0665 - mae: 23.4994 - val_loss: 507.1490 - val_mae: 21.7350\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 644.7851 - mae: 23.2832 - val_loss: 497.5327 - val_mae: 21.5173\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.6862 - mae: 23.0709 - val_loss: 488.0990 - val_mae: 21.3016\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.8331 - mae: 22.8593 - val_loss: 478.6885 - val_mae: 21.0842\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 614.8773 - mae: 22.6465 - val_loss: 469.4749 - val_mae: 20.8692\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 605.3317 - mae: 22.4360 - val_loss: 460.2415 - val_mae: 20.6521\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 595.6625 - mae: 22.2233 - val_loss: 451.2725 - val_mae: 20.4382\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 586.2369 - mae: 22.0146 - val_loss: 442.4094 - val_mae: 20.2248\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 576.9393 - mae: 21.8064 - val_loss: 433.7094 - val_mae: 20.0131\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 567.9177 - mae: 21.5998 - val_loss: 425.1010 - val_mae: 19.8015\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 558.7116 - mae: 21.3940 - val_loss: 416.7271 - val_mae: 19.5935\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 549.9871 - mae: 21.1882 - val_loss: 408.3376 - val_mae: 19.3830\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 541.1774 - mae: 20.9830 - val_loss: 400.1604 - val_mae: 19.1753\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 532.4944 - mae: 20.7807 - val_loss: 392.1782 - val_mae: 18.9704\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 524.1563 - mae: 20.5797 - val_loss: 384.2167 - val_mae: 18.7640\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 515.7845 - mae: 20.3763 - val_loss: 376.4226 - val_mae: 18.5593\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 507.4106 - mae: 20.1786 - val_loss: 368.8701 - val_mae: 18.3587\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 499.3653 - mae: 19.9818 - val_loss: 361.4299 - val_mae: 18.1591\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 491.5197 - mae: 19.7856 - val_loss: 354.0219 - val_mae: 17.9585\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 483.5571 - mae: 19.5904 - val_loss: 346.7928 - val_mae: 17.7604\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 475.8776 - mae: 19.3941 - val_loss: 339.5324 - val_mae: 17.5594\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 468.1735 - mae: 19.1970 - val_loss: 332.4396 - val_mae: 17.3607\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.6004 - mae: 19.0024 - val_loss: 325.4551 - val_mae: 17.1626\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 453.1589 - mae: 18.8079 - val_loss: 318.5852 - val_mae: 16.9658\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 445.8651 - mae: 18.6151 - val_loss: 311.8358 - val_mae: 16.7701\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 438.6007 - mae: 18.4248 - val_loss: 305.2698 - val_mae: 16.5774\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 431.6220 - mae: 18.2362 - val_loss: 298.7456 - val_mae: 16.3837\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 424.6027 - mae: 18.0475 - val_loss: 292.4295 - val_mae: 16.1936\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 417.8835 - mae: 17.8608 - val_loss: 286.1524 - val_mae: 16.0027\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 411.0698 - mae: 17.6743 - val_loss: 280.0289 - val_mae: 15.8144\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 404.5202 - mae: 17.4896 - val_loss: 273.9908 - val_mae: 15.6259\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 398.0124 - mae: 17.3038 - val_loss: 268.0689 - val_mae: 15.4389\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 391.6896 - mae: 17.1213 - val_loss: 262.2469 - val_mae: 15.2531\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 385.4154 - mae: 16.9390 - val_loss: 256.5363 - val_mae: 15.0685\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 379.1884 - mae: 16.7592 - val_loss: 250.9712 - val_mae: 14.8862\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 373.0860 - mae: 16.5824 - val_loss: 245.5128 - val_mae: 14.7053\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 367.2337 - mae: 16.4045 - val_loss: 240.0665 - val_mae: 14.5229\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 361.3204 - mae: 16.2258 - val_loss: 234.7539 - val_mae: 14.3423\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 355.6343 - mae: 16.0499 - val_loss: 229.5113 - val_mae: 14.1622\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 349.9260 - mae: 15.8738 - val_loss: 224.3935 - val_mae: 13.9836\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 344.2841 - mae: 15.7002 - val_loss: 219.4070 - val_mae: 13.8076\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 338.9874 - mae: 15.5263 - val_loss: 214.4305 - val_mae: 13.6296\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 333.4977 - mae: 15.3552 - val_loss: 209.6503 - val_mae: 13.4567\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 328.2211 - mae: 15.1859 - val_loss: 204.9854 - val_mae: 13.2852\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 323.0527 - mae: 15.0173 - val_loss: 200.3861 - val_mae: 13.1141\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 317.9855 - mae: 14.8495 - val_loss: 195.7993 - val_mae: 12.9412\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 312.8830 - mae: 14.6821 - val_loss: 191.3530 - val_mae: 12.7716\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 308.1174 - mae: 14.5152 - val_loss: 186.9226 - val_mae: 12.6003\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 303.2745 - mae: 14.3463 - val_loss: 182.5685 - val_mae: 12.4293\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 298.4442 - mae: 14.1817 - val_loss: 178.3548 - val_mae: 12.2618\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 293.7931 - mae: 14.0183 - val_loss: 174.2352 - val_mae: 12.0957\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 289.2475 - mae: 13.8556 - val_loss: 170.1763 - val_mae: 11.9300\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 284.6558 - mae: 13.6951 - val_loss: 166.2886 - val_mae: 11.7689\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 280.2882 - mae: 13.5362 - val_loss: 162.4113 - val_mae: 11.6063\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 275.9433 - mae: 13.3770 - val_loss: 158.6142 - val_mae: 11.4441\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 271.6478 - mae: 13.2192 - val_loss: 154.9086 - val_mae: 11.2841\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 267.5508 - mae: 13.0632 - val_loss: 151.2402 - val_mae: 11.1233\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 263.4639 - mae: 12.9057 - val_loss: 147.6595 - val_mae: 10.9643\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.4170 - mae: 12.7513 - val_loss: 144.1828 - val_mae: 10.8074\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 255.5819 - mae: 12.5959 - val_loss: 140.6978 - val_mae: 10.6478\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 251.6597 - mae: 12.4421 - val_loss: 137.3597 - val_mae: 10.4921\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 247.8962 - mae: 12.2929 - val_loss: 134.0986 - val_mae: 10.3382\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 244.1150 - mae: 12.1468 - val_loss: 130.9553 - val_mae: 10.1873\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 240.5273 - mae: 12.0038 - val_loss: 127.8702 - val_mae: 10.0374\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 236.9251 - mae: 11.8633 - val_loss: 124.8690 - val_mae: 9.8885\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 233.5316 - mae: 11.7260 - val_loss: 121.8528 - val_mae: 9.7372\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 230.0965 - mae: 11.5864 - val_loss: 118.9202 - val_mae: 9.5885\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 226.7304 - mae: 11.4512 - val_loss: 116.1005 - val_mae: 9.4427\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 223.5072 - mae: 11.3204 - val_loss: 113.2832 - val_mae: 9.2945\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 220.2453 - mae: 11.1892 - val_loss: 110.5452 - val_mae: 9.1485\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 217.0585 - mae: 11.0570 - val_loss: 107.9300 - val_mae: 9.0068\n",
      "5/5 [==============================] - 0s 798us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1605.1686 - mae: 38.6072 - val_loss: 1603.2638 - val_mae: 39.3751\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1587.1930 - mae: 38.3764 - val_loss: 1584.9310 - val_mae: 39.1412\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1569.7335 - mae: 38.1475 - val_loss: 1566.6405 - val_mae: 38.9063\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1552.0116 - mae: 37.9176 - val_loss: 1548.5952 - val_mae: 38.6732\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1534.6793 - mae: 37.6893 - val_loss: 1530.6814 - val_mae: 38.4403\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1517.4038 - mae: 37.4606 - val_loss: 1512.9689 - val_mae: 38.2086\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1500.3376 - mae: 37.2336 - val_loss: 1495.4260 - val_mae: 37.9777\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1483.5203 - mae: 37.0070 - val_loss: 1477.9720 - val_mae: 37.7466\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1466.6768 - mae: 36.7809 - val_loss: 1460.7354 - val_mae: 37.5168\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1450.1099 - mae: 36.5566 - val_loss: 1443.6545 - val_mae: 37.2877\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1433.7163 - mae: 36.3318 - val_loss: 1426.6702 - val_mae: 37.0585\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1417.4250 - mae: 36.1082 - val_loss: 1409.9111 - val_mae: 36.8309\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1401.4189 - mae: 35.8868 - val_loss: 1393.2773 - val_mae: 36.6035\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1385.3441 - mae: 35.6642 - val_loss: 1376.8563 - val_mae: 36.3776\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1369.6711 - mae: 35.4424 - val_loss: 1360.4836 - val_mae: 36.1509\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1353.8080 - mae: 35.2207 - val_loss: 1344.4352 - val_mae: 35.9273\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1338.3093 - mae: 35.0006 - val_loss: 1328.4305 - val_mae: 35.7030\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1322.9996 - mae: 34.7814 - val_loss: 1312.5055 - val_mae: 35.4781\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1307.7366 - mae: 34.5626 - val_loss: 1296.7838 - val_mae: 35.2547\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1292.4893 - mae: 34.3441 - val_loss: 1281.3641 - val_mae: 35.0342\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1277.7848 - mae: 34.1273 - val_loss: 1265.8737 - val_mae: 34.8113\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1262.7823 - mae: 33.9106 - val_loss: 1250.7188 - val_mae: 34.5917\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1248.0903 - mae: 33.6955 - val_loss: 1235.7368 - val_mae: 34.3733\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1233.7628 - mae: 33.4810 - val_loss: 1220.7135 - val_mae: 34.1528\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1219.3363 - mae: 33.2650 - val_loss: 1205.8573 - val_mae: 33.9333\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1204.8623 - mae: 33.0508 - val_loss: 1191.3091 - val_mae: 33.7170\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1190.8911 - mae: 32.8380 - val_loss: 1176.7534 - val_mae: 33.4990\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1177.0461 - mae: 32.6252 - val_loss: 1162.2634 - val_mae: 33.2804\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1163.0277 - mae: 32.4114 - val_loss: 1148.0748 - val_mae: 33.0652\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1149.2593 - mae: 32.2002 - val_loss: 1134.0625 - val_mae: 32.8511\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1135.9072 - mae: 31.9907 - val_loss: 1119.9908 - val_mae: 32.6346\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1122.3436 - mae: 31.7807 - val_loss: 1106.1787 - val_mae: 32.4206\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1109.1580 - mae: 31.5710 - val_loss: 1092.4379 - val_mae: 32.2062\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1095.8674 - mae: 31.3624 - val_loss: 1078.9696 - val_mae: 31.9947\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1082.8585 - mae: 31.1563 - val_loss: 1065.6244 - val_mae: 31.7835\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1070.0795 - mae: 30.9491 - val_loss: 1052.2992 - val_mae: 31.5713\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1057.2871 - mae: 30.7415 - val_loss: 1039.1921 - val_mae: 31.3611\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1044.7413 - mae: 30.5370 - val_loss: 1026.1530 - val_mae: 31.1504\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1032.3097 - mae: 30.3328 - val_loss: 1013.2101 - val_mae: 30.9398\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1019.7220 - mae: 30.1283 - val_loss: 1000.5952 - val_mae: 30.7331\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1007.5563 - mae: 29.9261 - val_loss: 988.0504 - val_mae: 30.5263\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 995.3505 - mae: 29.7238 - val_loss: 975.6793 - val_mae: 30.3209\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 983.4249 - mae: 29.5227 - val_loss: 963.2589 - val_mae: 30.1131\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 971.4973 - mae: 29.3198 - val_loss: 950.9807 - val_mae: 29.9063\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 959.6835 - mae: 29.1186 - val_loss: 938.8540 - val_mae: 29.7005\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 948.0022 - mae: 28.9177 - val_loss: 926.8517 - val_mae: 29.4956\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 936.5510 - mae: 28.7199 - val_loss: 914.9367 - val_mae: 29.2905\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 925.0327 - mae: 28.5223 - val_loss: 903.2800 - val_mae: 29.0884\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 913.9398 - mae: 28.3268 - val_loss: 891.5954 - val_mae: 28.8843\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 902.6505 - mae: 28.1295 - val_loss: 880.1708 - val_mae: 28.6836\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 891.7205 - mae: 27.9355 - val_loss: 868.7812 - val_mae: 28.4819\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 880.7063 - mae: 27.7397 - val_loss: 857.5884 - val_mae: 28.2822\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 869.9628 - mae: 27.5481 - val_loss: 846.4675 - val_mae: 28.0823\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 859.2910 - mae: 27.3564 - val_loss: 835.5270 - val_mae: 27.8843\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 848.6537 - mae: 27.1671 - val_loss: 824.7914 - val_mae: 27.6888\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 838.3334 - mae: 26.9797 - val_loss: 814.0431 - val_mae: 27.4914\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 828.0026 - mae: 26.7924 - val_loss: 803.3958 - val_mae: 27.2945\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 817.7858 - mae: 26.6050 - val_loss: 792.8231 - val_mae: 27.0975\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 807.5548 - mae: 26.4196 - val_loss: 782.4491 - val_mae: 26.9029\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 797.6281 - mae: 26.2331 - val_loss: 772.1031 - val_mae: 26.7072\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 787.6909 - mae: 26.0491 - val_loss: 761.9191 - val_mae: 26.5131\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 778.0119 - mae: 25.8630 - val_loss: 751.7672 - val_mae: 26.3182\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 768.2471 - mae: 25.6791 - val_loss: 741.8446 - val_mae: 26.1264\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 758.6724 - mae: 25.4984 - val_loss: 732.0376 - val_mae: 25.9351\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 749.2327 - mae: 25.3152 - val_loss: 722.3640 - val_mae: 25.7452\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 739.9210 - mae: 25.1349 - val_loss: 712.7717 - val_mae: 25.5553\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 730.7151 - mae: 24.9553 - val_loss: 703.2018 - val_mae: 25.3642\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 721.5568 - mae: 24.7760 - val_loss: 693.7538 - val_mae: 25.1741\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 712.4685 - mae: 24.5966 - val_loss: 684.4587 - val_mae: 24.9856\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 703.5071 - mae: 24.4207 - val_loss: 675.2679 - val_mae: 24.7979\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 694.7115 - mae: 24.2449 - val_loss: 666.1941 - val_mae: 24.6111\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 686.0096 - mae: 24.0681 - val_loss: 657.2121 - val_mae: 24.4247\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 677.3457 - mae: 23.8944 - val_loss: 648.3878 - val_mae: 24.2402\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 668.8674 - mae: 23.7214 - val_loss: 639.6671 - val_mae: 24.0565\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 660.4059 - mae: 23.5516 - val_loss: 631.0425 - val_mae: 23.8732\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 652.2066 - mae: 23.3817 - val_loss: 622.4041 - val_mae: 23.6882\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 643.8957 - mae: 23.2091 - val_loss: 613.9573 - val_mae: 23.5060\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.7800 - mae: 23.0379 - val_loss: 605.6011 - val_mae: 23.3243\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.8384 - mae: 22.8712 - val_loss: 597.3040 - val_mae: 23.1424\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 619.8091 - mae: 22.7036 - val_loss: 589.2286 - val_mae: 22.9639\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 611.9793 - mae: 22.5397 - val_loss: 581.2415 - val_mae: 22.7857\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 604.3040 - mae: 22.3754 - val_loss: 573.2310 - val_mae: 22.6059\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 596.5049 - mae: 22.2108 - val_loss: 565.4147 - val_mae: 22.4291\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 588.9127 - mae: 22.0477 - val_loss: 557.6719 - val_mae: 22.2525\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 581.5132 - mae: 21.8863 - val_loss: 549.9651 - val_mae: 22.0753\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 574.0584 - mae: 21.7252 - val_loss: 542.4189 - val_mae: 21.9002\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 566.8369 - mae: 21.5686 - val_loss: 534.9306 - val_mae: 21.7249\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 559.6392 - mae: 21.4114 - val_loss: 527.5953 - val_mae: 21.5518\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 552.5184 - mae: 21.2569 - val_loss: 520.4003 - val_mae: 21.3808\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 545.5435 - mae: 21.1023 - val_loss: 513.3010 - val_mae: 21.2106\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 538.6550 - mae: 20.9474 - val_loss: 506.2911 - val_mae: 21.0410\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 531.8455 - mae: 20.7959 - val_loss: 499.3262 - val_mae: 20.8711\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 525.1462 - mae: 20.6431 - val_loss: 492.3521 - val_mae: 20.6996\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 518.4267 - mae: 20.4892 - val_loss: 485.5163 - val_mae: 20.5302\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 511.8217 - mae: 20.3408 - val_loss: 478.7747 - val_mae: 20.3613\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 505.3578 - mae: 20.1915 - val_loss: 472.0787 - val_mae: 20.1925\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 498.8490 - mae: 20.0419 - val_loss: 465.6067 - val_mae: 20.0276\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 492.5712 - mae: 19.8973 - val_loss: 459.1642 - val_mae: 19.8625\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 486.3235 - mae: 19.7522 - val_loss: 452.7901 - val_mae: 19.6976\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 480.2023 - mae: 19.6065 - val_loss: 446.4785 - val_mae: 19.5331\n",
      "5/5 [==============================] - 0s 749us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   3.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 522.9745 - mae: 16.1307 - val_loss: 155.0013 - val_mae: 9.7530\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.4572 - mae: 8.0581 - val_loss: 66.0250 - val_mae: 5.9952\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.2384 - mae: 6.9882 - val_loss: 40.4332 - val_mae: 4.8340\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4223 - mae: 6.7153 - val_loss: 46.4054 - val_mae: 5.5113\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.7478 - mae: 6.4116 - val_loss: 26.5448 - val_mae: 3.8702\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.0028 - mae: 6.1306 - val_loss: 23.8687 - val_mae: 3.7559\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.9186 - mae: 6.1829 - val_loss: 22.7568 - val_mae: 3.6441\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.2303 - mae: 6.1965 - val_loss: 36.0403 - val_mae: 5.1038\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.8630 - mae: 6.0855 - val_loss: 38.8381 - val_mae: 5.3371\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.6342 - mae: 6.0771 - val_loss: 24.9529 - val_mae: 4.0658\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.9444 - mae: 6.1705 - val_loss: 19.6738 - val_mae: 3.4248\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.0295 - mae: 6.0598 - val_loss: 19.5783 - val_mae: 3.4472\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.4313 - mae: 5.9977 - val_loss: 47.8524 - val_mae: 5.9366\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.7792 - mae: 6.5504 - val_loss: 18.2782 - val_mae: 3.3327\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.9793 - mae: 5.9132 - val_loss: 21.6711 - val_mae: 3.8045\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.7233 - mae: 6.0524 - val_loss: 17.5031 - val_mae: 3.2324\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.2670 - mae: 5.7862 - val_loss: 25.5324 - val_mae: 4.2442\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.4138 - mae: 5.9617 - val_loss: 16.6084 - val_mae: 3.1326\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.2403 - mae: 5.6901 - val_loss: 20.2628 - val_mae: 3.6489\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.4100 - mae: 5.8062 - val_loss: 34.1280 - val_mae: 4.9624\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.1024 - mae: 5.8723 - val_loss: 17.6968 - val_mae: 3.3698\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.9791 - mae: 5.9868 - val_loss: 16.3864 - val_mae: 3.0960\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.0348 - mae: 5.7597 - val_loss: 19.5804 - val_mae: 3.5940\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.9814 - mae: 5.6574 - val_loss: 24.4187 - val_mae: 4.1570\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.4445 - mae: 5.8047 - val_loss: 23.3200 - val_mae: 4.0386\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.3273 - mae: 5.7719 - val_loss: 26.8844 - val_mae: 4.3893\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.3911 - mae: 5.8431 - val_loss: 16.0895 - val_mae: 3.1394\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1407 - mae: 5.9711 - val_loss: 16.2164 - val_mae: 3.1922\n",
      "Epoch 00028: early stopping\n",
      "5/5 [==============================] - 0s 818us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 232.2494 - mae: 11.5283 - val_loss: 47.3500 - val_mae: 5.8755\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.9585 - mae: 8.8882 - val_loss: 25.4719 - val_mae: 4.1728\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.1279 - mae: 8.2250 - val_loss: 40.4662 - val_mae: 5.1474\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.2930 - mae: 7.8602 - val_loss: 34.9954 - val_mae: 4.8390\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.9284 - mae: 7.7495 - val_loss: 30.0899 - val_mae: 4.4963\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.2200 - mae: 7.5634 - val_loss: 18.6322 - val_mae: 3.5763\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.3426 - mae: 7.3305 - val_loss: 27.8078 - val_mae: 4.3715\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.2364 - mae: 7.6584 - val_loss: 32.3900 - val_mae: 4.7164\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.6765 - mae: 7.0613 - val_loss: 43.1927 - val_mae: 5.5080\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.9932 - mae: 7.1191 - val_loss: 52.9778 - val_mae: 6.3232\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.4569 - mae: 7.2568 - val_loss: 20.8831 - val_mae: 3.7763\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.8739 - mae: 6.9128 - val_loss: 47.4464 - val_mae: 5.9874\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.3686 - mae: 6.9655 - val_loss: 43.8366 - val_mae: 5.7382\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.6048 - mae: 7.1418 - val_loss: 19.7432 - val_mae: 3.6698\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.6602 - mae: 6.7770 - val_loss: 22.3148 - val_mae: 3.9358\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.1621 - mae: 6.7296 - val_loss: 22.5514 - val_mae: 3.9836\n",
      "Epoch 00016: early stopping\n",
      "5/5 [==============================] - 0s 754us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   0.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 321.7030 - mae: 12.0068 - val_loss: 53.9212 - val_mae: 5.2843\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.9746 - mae: 5.9262 - val_loss: 42.6355 - val_mae: 4.9649\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.9732 - mae: 5.5510 - val_loss: 40.8580 - val_mae: 5.1034\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.7207 - mae: 5.3156 - val_loss: 40.0900 - val_mae: 5.1146\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.3288 - mae: 5.3453 - val_loss: 42.9911 - val_mae: 5.0791\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.0923 - mae: 5.2128 - val_loss: 39.8953 - val_mae: 5.0302\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.2267 - mae: 5.3588 - val_loss: 41.3413 - val_mae: 5.1838\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.0659 - mae: 5.4474 - val_loss: 42.8805 - val_mae: 5.0506\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.5005 - mae: 5.1271 - val_loss: 39.7259 - val_mae: 5.0738\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.9844 - mae: 5.3034 - val_loss: 38.8669 - val_mae: 4.9716\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.0429 - mae: 5.1951 - val_loss: 39.6985 - val_mae: 4.8723\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.8901 - mae: 5.2068 - val_loss: 41.2157 - val_mae: 4.9569\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.2727 - mae: 5.1841 - val_loss: 38.7063 - val_mae: 4.8673\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.7507 - mae: 5.1397 - val_loss: 38.3790 - val_mae: 4.8947\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.5761 - mae: 5.1209 - val_loss: 38.6595 - val_mae: 4.8021\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.0433 - mae: 5.0767 - val_loss: 39.0001 - val_mae: 4.9534\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.2224 - mae: 5.2603 - val_loss: 40.0550 - val_mae: 4.8615\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.2970 - mae: 5.1827 - val_loss: 37.6773 - val_mae: 4.7528\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.9329 - mae: 5.1347 - val_loss: 38.5520 - val_mae: 4.7520\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.9874 - mae: 5.0119 - val_loss: 37.0935 - val_mae: 4.7429\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.4080 - mae: 5.2156 - val_loss: 37.5810 - val_mae: 4.7377\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.3279 - mae: 4.9237 - val_loss: 37.5858 - val_mae: 4.8550\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.1062 - mae: 5.1678 - val_loss: 37.4175 - val_mae: 4.6902\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.5002 - mae: 4.9804 - val_loss: 38.8221 - val_mae: 4.9305\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.7248 - mae: 5.1263 - val_loss: 36.4770 - val_mae: 4.7185\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6457 - mae: 4.9711 - val_loss: 37.9138 - val_mae: 4.8352\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.2562 - mae: 5.1645 - val_loss: 37.2220 - val_mae: 4.6898\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 52.4092 - mae: 5.0010 - val_loss: 36.1260 - val_mae: 4.6969\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.0964 - mae: 4.9760 - val_loss: 36.2552 - val_mae: 4.6273\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6793 - mae: 4.9532 - val_loss: 36.9700 - val_mae: 4.7423\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.3828 - mae: 4.9690 - val_loss: 36.4714 - val_mae: 4.6479\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6184 - mae: 4.8998 - val_loss: 37.6808 - val_mae: 4.8088\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.7181 - mae: 5.0974 - val_loss: 37.7005 - val_mae: 4.6907\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.9470 - mae: 4.9523 - val_loss: 35.4366 - val_mae: 4.6149\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.2451 - mae: 4.8922 - val_loss: 35.4273 - val_mae: 4.6213\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.1293 - mae: 4.9138 - val_loss: 35.8865 - val_mae: 4.5999\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6749 - mae: 5.0938 - val_loss: 37.8003 - val_mae: 4.6781\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.8456 - mae: 4.9704 - val_loss: 38.7079 - val_mae: 4.7599\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.7678 - mae: 4.9236 - val_loss: 34.8003 - val_mae: 4.6227\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 49.8844 - mae: 4.9400 - val_loss: 34.7301 - val_mae: 4.4934\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.5523 - mae: 4.9374 - val_loss: 34.7817 - val_mae: 4.4727\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 49.4872 - mae: 4.7015 - val_loss: 55.0315 - val_mae: 5.9386\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.3050 - mae: 5.3397 - val_loss: 35.1840 - val_mae: 4.5353\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 50.8861 - mae: 4.9089 - val_loss: 35.2006 - val_mae: 4.5028\n",
      "Epoch 00044: early stopping\n",
      "5/5 [==============================] - 0s 760us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>; total time=   1.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 635.5770 - mae: 23.5619 - val_loss: 463.4881 - val_mae: 20.6567\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 635.1619 - mae: 23.5541 - val_loss: 463.1586 - val_mae: 20.6491\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.7367 - mae: 23.5461 - val_loss: 462.8358 - val_mae: 20.6418\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 634.3232 - mae: 23.5383 - val_loss: 462.5100 - val_mae: 20.6343\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.9010 - mae: 23.5304 - val_loss: 462.1857 - val_mae: 20.6269\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.4843 - mae: 23.5225 - val_loss: 461.8632 - val_mae: 20.6195\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 633.0568 - mae: 23.5146 - val_loss: 461.5478 - val_mae: 20.6123\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.6637 - mae: 23.5072 - val_loss: 461.2184 - val_mae: 20.6047\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 632.2341 - mae: 23.4991 - val_loss: 460.9019 - val_mae: 20.5974\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.8093 - mae: 23.4912 - val_loss: 460.5887 - val_mae: 20.5903\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 631.4047 - mae: 23.4836 - val_loss: 460.2684 - val_mae: 20.5829\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.9807 - mae: 23.4756 - val_loss: 459.9517 - val_mae: 20.5756\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.5699 - mae: 23.4680 - val_loss: 459.6293 - val_mae: 20.5681\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 630.1541 - mae: 23.4601 - val_loss: 459.3109 - val_mae: 20.5608\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.7441 - mae: 23.4524 - val_loss: 458.9959 - val_mae: 20.5535\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 629.3354 - mae: 23.4447 - val_loss: 458.6832 - val_mae: 20.5463\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.9117 - mae: 23.4368 - val_loss: 458.3805 - val_mae: 20.5393\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.5071 - mae: 23.4291 - val_loss: 458.0721 - val_mae: 20.5322\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 628.0927 - mae: 23.4214 - val_loss: 457.7597 - val_mae: 20.5250\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.6945 - mae: 23.4138 - val_loss: 457.4396 - val_mae: 20.5175\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 627.2843 - mae: 23.4061 - val_loss: 457.1282 - val_mae: 20.5102\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.8659 - mae: 23.3982 - val_loss: 456.8213 - val_mae: 20.5031\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.4690 - mae: 23.3907 - val_loss: 456.5114 - val_mae: 20.4959\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 626.0673 - mae: 23.3830 - val_loss: 456.1995 - val_mae: 20.4887\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.6539 - mae: 23.3752 - val_loss: 455.8939 - val_mae: 20.4816\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 625.2466 - mae: 23.3675 - val_loss: 455.5892 - val_mae: 20.4745\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.8564 - mae: 23.3601 - val_loss: 455.2842 - val_mae: 20.4674\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.4511 - mae: 23.3524 - val_loss: 454.9816 - val_mae: 20.4604\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 624.0346 - mae: 23.3445 - val_loss: 454.6853 - val_mae: 20.4535\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.6522 - mae: 23.3372 - val_loss: 454.3719 - val_mae: 20.4461\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 623.2414 - mae: 23.3294 - val_loss: 454.0686 - val_mae: 20.4390\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.8339 - mae: 23.3217 - val_loss: 453.7726 - val_mae: 20.4321\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.4282 - mae: 23.3140 - val_loss: 453.4774 - val_mae: 20.4252\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 622.0384 - mae: 23.3066 - val_loss: 453.1740 - val_mae: 20.4181\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.6472 - mae: 23.2990 - val_loss: 452.8705 - val_mae: 20.4110\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 621.2375 - mae: 23.2913 - val_loss: 452.5742 - val_mae: 20.4041\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.8365 - mae: 23.2836 - val_loss: 452.2826 - val_mae: 20.3973\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.4438 - mae: 23.2762 - val_loss: 451.9886 - val_mae: 20.3903\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 620.0387 - mae: 23.2684 - val_loss: 451.7008 - val_mae: 20.3836\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 619.6405 - mae: 23.2608 - val_loss: 451.4048 - val_mae: 20.3766\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 619.2481 - mae: 23.2533 - val_loss: 451.0986 - val_mae: 20.3694\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 618.8286 - mae: 23.2453 - val_loss: 450.8020 - val_mae: 20.3624\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 618.4390 - mae: 23.2378 - val_loss: 450.4969 - val_mae: 20.3552\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 618.0169 - mae: 23.2297 - val_loss: 450.2061 - val_mae: 20.3483\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 617.6325 - mae: 23.2224 - val_loss: 449.9035 - val_mae: 20.3411\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 617.2087 - mae: 23.2142 - val_loss: 449.6125 - val_mae: 20.3342\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 616.8067 - mae: 23.2065 - val_loss: 449.3183 - val_mae: 20.3273\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 616.4053 - mae: 23.1988 - val_loss: 449.0207 - val_mae: 20.3201\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 615.9954 - mae: 23.1910 - val_loss: 448.7284 - val_mae: 20.3132\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 615.5987 - mae: 23.1834 - val_loss: 448.4390 - val_mae: 20.3063\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 615.1936 - mae: 23.1756 - val_loss: 448.1535 - val_mae: 20.2995\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 614.8067 - mae: 23.1681 - val_loss: 447.8599 - val_mae: 20.2925\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 614.3974 - mae: 23.1603 - val_loss: 447.5697 - val_mae: 20.2856\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 613.9891 - mae: 23.1523 - val_loss: 447.2837 - val_mae: 20.2788\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 613.5961 - mae: 23.1447 - val_loss: 446.9905 - val_mae: 20.2718\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 613.1958 - mae: 23.1370 - val_loss: 446.7019 - val_mae: 20.2649\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 612.7883 - mae: 23.1291 - val_loss: 446.4175 - val_mae: 20.2581\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 612.4055 - mae: 23.1217 - val_loss: 446.1227 - val_mae: 20.2509\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 611.9943 - mae: 23.1137 - val_loss: 445.8367 - val_mae: 20.2440\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 611.5931 - mae: 23.1059 - val_loss: 445.5530 - val_mae: 20.2373\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 611.2050 - mae: 23.0985 - val_loss: 445.2657 - val_mae: 20.2303\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 610.8039 - mae: 23.0907 - val_loss: 444.9854 - val_mae: 20.2236\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 610.4143 - mae: 23.0830 - val_loss: 444.7036 - val_mae: 20.2168\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 610.0203 - mae: 23.0755 - val_loss: 444.4252 - val_mae: 20.2101\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 609.6348 - mae: 23.0680 - val_loss: 444.1454 - val_mae: 20.2034\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 609.2433 - mae: 23.0602 - val_loss: 443.8704 - val_mae: 20.1968\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 608.8481 - mae: 23.0527 - val_loss: 443.5951 - val_mae: 20.1901\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 608.4619 - mae: 23.0451 - val_loss: 443.3119 - val_mae: 20.1833\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 608.0663 - mae: 23.0372 - val_loss: 443.0295 - val_mae: 20.1765\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 607.6656 - mae: 23.0295 - val_loss: 442.7469 - val_mae: 20.1696\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 607.2805 - mae: 23.0219 - val_loss: 442.4547 - val_mae: 20.1625\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 606.8638 - mae: 23.0138 - val_loss: 442.1770 - val_mae: 20.1557\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 606.4666 - mae: 23.0061 - val_loss: 441.8997 - val_mae: 20.1490\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 606.0858 - mae: 22.9985 - val_loss: 441.6189 - val_mae: 20.1422\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 605.6840 - mae: 22.9907 - val_loss: 441.3419 - val_mae: 20.1354\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 605.2826 - mae: 22.9829 - val_loss: 441.0723 - val_mae: 20.1288\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 604.9140 - mae: 22.9756 - val_loss: 440.7923 - val_mae: 20.1220\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 604.5130 - mae: 22.9677 - val_loss: 440.5199 - val_mae: 20.1153\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 604.1266 - mae: 22.9601 - val_loss: 440.2508 - val_mae: 20.1088\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 603.7418 - mae: 22.9526 - val_loss: 439.9804 - val_mae: 20.1022\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 603.3636 - mae: 22.9451 - val_loss: 439.7031 - val_mae: 20.0954\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 602.9636 - mae: 22.9373 - val_loss: 439.4332 - val_mae: 20.0887\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 602.5792 - mae: 22.9298 - val_loss: 439.1631 - val_mae: 20.0821\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 602.2006 - mae: 22.9222 - val_loss: 438.8866 - val_mae: 20.0752\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.8065 - mae: 22.9145 - val_loss: 438.6145 - val_mae: 20.0685\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.4225 - mae: 22.9070 - val_loss: 438.3448 - val_mae: 20.0618\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.0297 - mae: 22.8992 - val_loss: 438.0804 - val_mae: 20.0553\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 600.6479 - mae: 22.8917 - val_loss: 437.8118 - val_mae: 20.0486\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 600.2726 - mae: 22.8841 - val_loss: 437.5402 - val_mae: 20.0419\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 599.8756 - mae: 22.8762 - val_loss: 437.2747 - val_mae: 20.0354\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 599.4881 - mae: 22.8686 - val_loss: 437.0088 - val_mae: 20.0288\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 599.1127 - mae: 22.8612 - val_loss: 436.7362 - val_mae: 20.0220\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 598.7198 - mae: 22.8533 - val_loss: 436.4709 - val_mae: 20.0154\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 598.3285 - mae: 22.8456 - val_loss: 436.2079 - val_mae: 20.0089\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 597.9413 - mae: 22.8379 - val_loss: 435.9446 - val_mae: 20.0023\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 597.5648 - mae: 22.8305 - val_loss: 435.6725 - val_mae: 19.9955\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 597.1727 - mae: 22.8227 - val_loss: 435.4077 - val_mae: 19.9888\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 596.7810 - mae: 22.8148 - val_loss: 435.1477 - val_mae: 19.9824\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 596.4033 - mae: 22.8073 - val_loss: 434.8812 - val_mae: 19.9757\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 596.0173 - mae: 22.7997 - val_loss: 434.6146 - val_mae: 19.9690\n",
      "5/5 [==============================] - 0s 842us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2569.8069 - mae: 46.6545 - val_loss: 2207.6646 - val_mae: 43.5581\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2561.9990 - mae: 46.5723 - val_loss: 2200.5293 - val_mae: 43.4795\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2554.3088 - mae: 46.4904 - val_loss: 2193.3674 - val_mae: 43.4005\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2546.6167 - mae: 46.4112 - val_loss: 2186.2612 - val_mae: 43.3219\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2538.8977 - mae: 46.3286 - val_loss: 2179.1606 - val_mae: 43.2433\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2531.1824 - mae: 46.2483 - val_loss: 2172.1426 - val_mae: 43.1653\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2523.5361 - mae: 46.1664 - val_loss: 2165.0955 - val_mae: 43.0869\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2515.8430 - mae: 46.0879 - val_loss: 2158.1372 - val_mae: 43.0092\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2508.3198 - mae: 46.0066 - val_loss: 2151.1216 - val_mae: 42.9308\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2500.8608 - mae: 45.9261 - val_loss: 2144.0479 - val_mae: 42.8515\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2493.0654 - mae: 45.8444 - val_loss: 2137.1458 - val_mae: 42.7739\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2485.6162 - mae: 45.7638 - val_loss: 2130.2046 - val_mae: 42.6958\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2478.0591 - mae: 45.6848 - val_loss: 2123.1753 - val_mae: 42.6167\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2470.5493 - mae: 45.6042 - val_loss: 2116.1113 - val_mae: 42.5371\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2463.0034 - mae: 45.5243 - val_loss: 2109.1445 - val_mae: 42.4584\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2455.5288 - mae: 45.4433 - val_loss: 2102.1858 - val_mae: 42.3797\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2448.1106 - mae: 45.3650 - val_loss: 2095.2258 - val_mae: 42.3008\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2440.7200 - mae: 45.2867 - val_loss: 2088.2480 - val_mae: 42.2215\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2433.2070 - mae: 45.2067 - val_loss: 2081.4043 - val_mae: 42.1435\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2425.7671 - mae: 45.1290 - val_loss: 2074.5942 - val_mae: 42.0658\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2418.5300 - mae: 45.0497 - val_loss: 2067.6487 - val_mae: 41.9863\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2411.1152 - mae: 44.9718 - val_loss: 2060.8208 - val_mae: 41.9079\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2403.8232 - mae: 44.8941 - val_loss: 2054.0166 - val_mae: 41.8296\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2396.5332 - mae: 44.8158 - val_loss: 2047.2528 - val_mae: 41.7517\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2389.2927 - mae: 44.7380 - val_loss: 2040.5096 - val_mae: 41.6737\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2381.9980 - mae: 44.6601 - val_loss: 2033.8206 - val_mae: 41.5962\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2374.8472 - mae: 44.5833 - val_loss: 2027.0800 - val_mae: 41.5180\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2367.5901 - mae: 44.5046 - val_loss: 2020.4399 - val_mae: 41.4407\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2360.5371 - mae: 44.4276 - val_loss: 2013.7351 - val_mae: 41.3625\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2353.3882 - mae: 44.3515 - val_loss: 2007.1107 - val_mae: 41.2851\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2346.2417 - mae: 44.2725 - val_loss: 2000.5138 - val_mae: 41.2078\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2339.1704 - mae: 44.1966 - val_loss: 1993.9067 - val_mae: 41.1302\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2332.0942 - mae: 44.1202 - val_loss: 1987.2883 - val_mae: 41.0523\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2324.9443 - mae: 44.0428 - val_loss: 1980.7847 - val_mae: 40.9757\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2317.8604 - mae: 43.9661 - val_loss: 1974.2781 - val_mae: 40.8987\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2310.8784 - mae: 43.8905 - val_loss: 1967.7185 - val_mae: 40.8210\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2303.8301 - mae: 43.8127 - val_loss: 1961.1875 - val_mae: 40.7435\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2296.7383 - mae: 43.7358 - val_loss: 1954.7073 - val_mae: 40.6663\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2289.8542 - mae: 43.6600 - val_loss: 1948.2100 - val_mae: 40.5887\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2282.7590 - mae: 43.5837 - val_loss: 1941.8458 - val_mae: 40.5125\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2275.8105 - mae: 43.5063 - val_loss: 1935.5084 - val_mae: 40.4366\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2268.9265 - mae: 43.4312 - val_loss: 1929.1272 - val_mae: 40.3598\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2262.0020 - mae: 43.3542 - val_loss: 1922.7596 - val_mae: 40.2831\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2255.1123 - mae: 43.2781 - val_loss: 1916.4175 - val_mae: 40.2065\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2248.2234 - mae: 43.2019 - val_loss: 1910.1157 - val_mae: 40.1302\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2241.4607 - mae: 43.1276 - val_loss: 1903.7639 - val_mae: 40.0531\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2234.5713 - mae: 43.0509 - val_loss: 1897.5177 - val_mae: 39.9771\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2227.8298 - mae: 42.9743 - val_loss: 1891.2640 - val_mae: 39.9008\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2220.9456 - mae: 42.8996 - val_loss: 1885.0851 - val_mae: 39.8253\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2214.2241 - mae: 42.8227 - val_loss: 1878.8506 - val_mae: 39.7488\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2207.3977 - mae: 42.7465 - val_loss: 1872.6519 - val_mae: 39.6727\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2200.7261 - mae: 42.6715 - val_loss: 1866.4135 - val_mae: 39.5959\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2193.8735 - mae: 42.5952 - val_loss: 1860.3359 - val_mae: 39.5209\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2187.2876 - mae: 42.5210 - val_loss: 1854.1493 - val_mae: 39.4464\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2180.5269 - mae: 42.4466 - val_loss: 1848.0203 - val_mae: 39.3781\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2173.8376 - mae: 42.3713 - val_loss: 1841.9038 - val_mae: 39.3098\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2167.1750 - mae: 42.2969 - val_loss: 1835.8330 - val_mae: 39.2419\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2160.5715 - mae: 42.2220 - val_loss: 1829.7595 - val_mae: 39.1738\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2153.9324 - mae: 42.1461 - val_loss: 1823.6814 - val_mae: 39.1055\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2147.2661 - mae: 42.0715 - val_loss: 1817.6588 - val_mae: 39.0376\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2140.6765 - mae: 41.9973 - val_loss: 1811.6162 - val_mae: 38.9765\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2134.0505 - mae: 41.9209 - val_loss: 1805.6459 - val_mae: 38.9162\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2127.5271 - mae: 41.8465 - val_loss: 1799.6664 - val_mae: 38.8556\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2120.9329 - mae: 41.7721 - val_loss: 1793.7109 - val_mae: 38.7952\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2114.3794 - mae: 41.6975 - val_loss: 1787.7612 - val_mae: 38.7347\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2107.8484 - mae: 41.6225 - val_loss: 1781.8601 - val_mae: 38.6745\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2101.3083 - mae: 41.5473 - val_loss: 1775.9928 - val_mae: 38.6145\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2094.8408 - mae: 41.4722 - val_loss: 1770.0919 - val_mae: 38.5540\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2088.3103 - mae: 41.3973 - val_loss: 1764.2529 - val_mae: 38.4940\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2081.8389 - mae: 41.3224 - val_loss: 1758.3890 - val_mae: 38.4337\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2075.3704 - mae: 41.2478 - val_loss: 1752.5956 - val_mae: 38.3738\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2068.9023 - mae: 41.1716 - val_loss: 1746.8495 - val_mae: 38.3144\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2062.5618 - mae: 41.0991 - val_loss: 1741.0853 - val_mae: 38.2545\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2056.1489 - mae: 41.0237 - val_loss: 1735.3859 - val_mae: 38.1952\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2049.7769 - mae: 40.9501 - val_loss: 1729.6385 - val_mae: 38.1352\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2043.3888 - mae: 40.8760 - val_loss: 1723.9259 - val_mae: 38.0755\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2037.1274 - mae: 40.8014 - val_loss: 1718.2252 - val_mae: 38.0157\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2030.8333 - mae: 40.7264 - val_loss: 1712.5139 - val_mae: 37.9557\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2024.4813 - mae: 40.6527 - val_loss: 1706.8701 - val_mae: 37.8963\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2018.2593 - mae: 40.5792 - val_loss: 1701.1753 - val_mae: 37.8361\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2011.8662 - mae: 40.5067 - val_loss: 1695.5753 - val_mae: 37.7768\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2005.6346 - mae: 40.4345 - val_loss: 1690.0392 - val_mae: 37.7180\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1999.4268 - mae: 40.3623 - val_loss: 1684.5029 - val_mae: 37.6591\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1993.2504 - mae: 40.2913 - val_loss: 1678.9292 - val_mae: 37.5996\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1987.0835 - mae: 40.2195 - val_loss: 1673.3193 - val_mae: 37.5396\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1980.8220 - mae: 40.1475 - val_loss: 1667.8245 - val_mae: 37.4807\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1974.6293 - mae: 40.0744 - val_loss: 1662.3362 - val_mae: 37.4216\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1968.4417 - mae: 40.0039 - val_loss: 1656.8672 - val_mae: 37.3627\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1962.3632 - mae: 39.9328 - val_loss: 1651.3679 - val_mae: 37.3032\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1956.2478 - mae: 39.8605 - val_loss: 1645.8695 - val_mae: 37.2436\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1950.1952 - mae: 39.7903 - val_loss: 1640.3773 - val_mae: 37.1840\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1944.0802 - mae: 39.7187 - val_loss: 1634.9888 - val_mae: 37.1252\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1937.9628 - mae: 39.6457 - val_loss: 1629.6440 - val_mae: 37.0668\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1932.0983 - mae: 39.5777 - val_loss: 1624.2310 - val_mae: 37.0076\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1926.1140 - mae: 39.5077 - val_loss: 1618.9073 - val_mae: 36.9491\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1920.0515 - mae: 39.4353 - val_loss: 1613.6716 - val_mae: 36.8914\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1914.1771 - mae: 39.3671 - val_loss: 1608.4092 - val_mae: 36.8333\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1908.3843 - mae: 39.2971 - val_loss: 1603.0597 - val_mae: 36.7742\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1902.4387 - mae: 39.2272 - val_loss: 1597.7990 - val_mae: 36.7158\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1896.4569 - mae: 39.1572 - val_loss: 1592.6658 - val_mae: 36.6587\n",
      "5/5 [==============================] - 0s 731us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8447.7871 - mae: 84.8311 - val_loss: 8358.2559 - val_mae: 85.6745\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8429.8516 - mae: 84.7379 - val_loss: 8339.8770 - val_mae: 85.5697\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8411.7510 - mae: 84.6447 - val_loss: 8321.5801 - val_mae: 85.4651\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8393.6133 - mae: 84.5492 - val_loss: 8303.3076 - val_mae: 85.3607\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8375.4385 - mae: 84.4556 - val_loss: 8285.1191 - val_mae: 85.2565\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8357.7529 - mae: 84.3617 - val_loss: 8266.7803 - val_mae: 85.1515\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8339.7852 - mae: 84.2690 - val_loss: 8248.5615 - val_mae: 85.0469\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8321.6855 - mae: 84.1740 - val_loss: 8230.4863 - val_mae: 84.9432\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8304.0488 - mae: 84.0811 - val_loss: 8212.2246 - val_mae: 84.8382\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8286.1426 - mae: 83.9865 - val_loss: 8194.1318 - val_mae: 84.7340\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8268.3115 - mae: 83.8947 - val_loss: 8176.0659 - val_mae: 84.6297\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8250.4980 - mae: 83.7999 - val_loss: 8157.9912 - val_mae: 84.5254\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8232.6797 - mae: 83.7070 - val_loss: 8140.0034 - val_mae: 84.4213\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8215.0508 - mae: 83.6132 - val_loss: 8121.9790 - val_mae: 84.3171\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8197.3818 - mae: 83.5204 - val_loss: 8104.1660 - val_mae: 84.2143\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8179.6504 - mae: 83.4273 - val_loss: 8086.4551 - val_mae: 84.1121\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8162.0327 - mae: 83.3329 - val_loss: 8068.7256 - val_mae: 84.0096\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8144.4644 - mae: 83.2414 - val_loss: 8051.0503 - val_mae: 83.9071\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8127.0830 - mae: 83.1492 - val_loss: 8033.4180 - val_mae: 83.8048\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8109.5327 - mae: 83.0547 - val_loss: 8015.9346 - val_mae: 83.7034\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8092.3135 - mae: 82.9645 - val_loss: 7998.2871 - val_mae: 83.6008\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8074.7417 - mae: 82.8693 - val_loss: 7980.8145 - val_mae: 83.4991\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8057.3823 - mae: 82.7775 - val_loss: 7963.3555 - val_mae: 83.3974\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8040.0605 - mae: 82.6852 - val_loss: 7945.7686 - val_mae: 83.2948\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8022.6494 - mae: 82.5918 - val_loss: 7928.3081 - val_mae: 83.1929\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8005.4009 - mae: 82.4990 - val_loss: 7910.7593 - val_mae: 83.0904\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7987.8359 - mae: 82.4052 - val_loss: 7893.5449 - val_mae: 82.9896\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7970.8164 - mae: 82.3150 - val_loss: 7876.1919 - val_mae: 82.8879\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7953.6445 - mae: 82.2234 - val_loss: 7858.7637 - val_mae: 82.7856\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7936.3066 - mae: 82.1285 - val_loss: 7841.4941 - val_mae: 82.6842\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7919.2866 - mae: 82.0380 - val_loss: 7824.1631 - val_mae: 82.5822\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7902.0098 - mae: 81.9461 - val_loss: 7806.9429 - val_mae: 82.4807\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7885.0898 - mae: 81.8543 - val_loss: 7789.7231 - val_mae: 82.3792\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7867.9727 - mae: 81.7621 - val_loss: 7772.6182 - val_mae: 82.2783\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7851.0444 - mae: 81.6708 - val_loss: 7755.5342 - val_mae: 82.1772\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7834.2305 - mae: 81.5809 - val_loss: 7738.3438 - val_mae: 82.0754\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7817.2397 - mae: 81.4887 - val_loss: 7721.3887 - val_mae: 81.9749\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7800.4307 - mae: 81.3987 - val_loss: 7704.4443 - val_mae: 81.8743\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7783.6572 - mae: 81.3090 - val_loss: 7687.5161 - val_mae: 81.7736\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7766.7490 - mae: 81.2171 - val_loss: 7670.7363 - val_mae: 81.6737\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7750.3120 - mae: 81.1262 - val_loss: 7653.7349 - val_mae: 81.5725\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7733.4478 - mae: 81.0355 - val_loss: 7636.8560 - val_mae: 81.4718\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7716.5962 - mae: 80.9441 - val_loss: 7620.1362 - val_mae: 81.3720\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7699.9556 - mae: 80.8538 - val_loss: 7603.3774 - val_mae: 81.2718\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7683.3848 - mae: 80.7639 - val_loss: 7586.5713 - val_mae: 81.1711\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7666.9297 - mae: 80.6736 - val_loss: 7569.6870 - val_mae: 81.0700\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7650.2031 - mae: 80.5826 - val_loss: 7553.1445 - val_mae: 80.9709\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7633.7271 - mae: 80.4929 - val_loss: 7536.7153 - val_mae: 80.8724\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7617.1699 - mae: 80.4047 - val_loss: 7520.3008 - val_mae: 80.7738\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7601.1108 - mae: 80.3146 - val_loss: 7503.7095 - val_mae: 80.6744\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7584.5874 - mae: 80.2240 - val_loss: 7487.3252 - val_mae: 80.5759\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7568.1812 - mae: 80.1334 - val_loss: 7470.9966 - val_mae: 80.4777\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7551.7983 - mae: 80.0450 - val_loss: 7454.6885 - val_mae: 80.3793\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7535.5093 - mae: 79.9543 - val_loss: 7438.4458 - val_mae: 80.2814\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7519.2832 - mae: 79.8646 - val_loss: 7422.2334 - val_mae: 80.1836\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7502.9243 - mae: 79.7740 - val_loss: 7406.1167 - val_mae: 80.0862\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7486.8032 - mae: 79.6861 - val_loss: 7389.9028 - val_mae: 79.9879\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7470.5801 - mae: 79.5964 - val_loss: 7373.6226 - val_mae: 79.8893\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7454.3779 - mae: 79.5064 - val_loss: 7357.3682 - val_mae: 79.7906\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7438.1758 - mae: 79.4169 - val_loss: 7341.1772 - val_mae: 79.6923\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7421.9844 - mae: 79.3266 - val_loss: 7325.0381 - val_mae: 79.5941\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7405.9150 - mae: 79.2386 - val_loss: 7308.8408 - val_mae: 79.4955\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7389.8501 - mae: 79.1487 - val_loss: 7292.7183 - val_mae: 79.3972\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7373.6895 - mae: 79.0596 - val_loss: 7276.7622 - val_mae: 79.2997\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7357.8096 - mae: 78.9700 - val_loss: 7260.7837 - val_mae: 79.2021\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7341.8770 - mae: 78.8813 - val_loss: 7244.8521 - val_mae: 79.1047\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7326.0327 - mae: 78.7938 - val_loss: 7228.8428 - val_mae: 79.0065\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7310.0024 - mae: 78.7050 - val_loss: 7213.0405 - val_mae: 78.9095\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7294.3325 - mae: 78.6175 - val_loss: 7197.1367 - val_mae: 78.8117\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7278.3398 - mae: 78.5284 - val_loss: 7181.2720 - val_mae: 78.7142\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7262.6055 - mae: 78.4405 - val_loss: 7165.3916 - val_mae: 78.6163\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7246.7573 - mae: 78.3522 - val_loss: 7149.6182 - val_mae: 78.5191\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7231.1357 - mae: 78.2652 - val_loss: 7133.8037 - val_mae: 78.4214\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7215.5137 - mae: 78.1769 - val_loss: 7118.1250 - val_mae: 78.3246\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7199.7476 - mae: 78.0886 - val_loss: 7102.5522 - val_mae: 78.2281\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7184.5073 - mae: 78.0039 - val_loss: 7086.7217 - val_mae: 78.1300\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7168.7676 - mae: 77.9157 - val_loss: 7071.1543 - val_mae: 78.0334\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7153.1836 - mae: 77.8287 - val_loss: 7055.6519 - val_mae: 77.9371\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7137.8252 - mae: 77.7429 - val_loss: 7040.0879 - val_mae: 77.8403\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7122.3066 - mae: 77.6566 - val_loss: 7024.6738 - val_mae: 77.7442\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7106.7583 - mae: 77.5698 - val_loss: 7009.3667 - val_mae: 77.6486\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7091.6416 - mae: 77.4845 - val_loss: 6993.7637 - val_mae: 77.5512\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7076.1069 - mae: 77.3973 - val_loss: 6978.2754 - val_mae: 77.4545\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7060.5664 - mae: 77.3089 - val_loss: 6962.8853 - val_mae: 77.3582\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7045.3267 - mae: 77.2236 - val_loss: 6947.3755 - val_mae: 77.2611\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7029.9966 - mae: 77.1362 - val_loss: 6931.8887 - val_mae: 77.1639\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7014.6904 - mae: 77.0510 - val_loss: 6916.4106 - val_mae: 77.0665\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6999.3359 - mae: 76.9638 - val_loss: 6901.1309 - val_mae: 76.9704\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6984.1729 - mae: 76.8767 - val_loss: 6885.8438 - val_mae: 76.8741\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6969.1265 - mae: 76.7917 - val_loss: 6870.6240 - val_mae: 76.7781\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6953.9517 - mae: 76.7055 - val_loss: 6855.6318 - val_mae: 76.6835\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6938.8750 - mae: 76.6199 - val_loss: 6840.5659 - val_mae: 76.5882\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6923.8853 - mae: 76.5349 - val_loss: 6825.3774 - val_mae: 76.4921\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6908.9902 - mae: 76.4491 - val_loss: 6810.1064 - val_mae: 76.3955\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6893.6963 - mae: 76.3635 - val_loss: 6795.1812 - val_mae: 76.3005\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6879.1572 - mae: 76.2800 - val_loss: 6780.0103 - val_mae: 76.2041\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6863.8350 - mae: 76.1924 - val_loss: 6765.2822 - val_mae: 76.1103\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6849.3975 - mae: 76.1085 - val_loss: 6750.1997 - val_mae: 76.0144\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6834.4316 - mae: 76.0234 - val_loss: 6735.2788 - val_mae: 75.9196\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6819.5347 - mae: 75.9382 - val_loss: 6720.3604 - val_mae: 75.8247\n",
      "5/5 [==============================] - 0s 775us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=1, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 23463.8008 - mae: 83.1167 - val_loss: 390.1832 - val_mae: 17.9530\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 388.6627 - mae: 16.6824 - val_loss: 292.7111 - val_mae: 15.1550\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 320.6653 - mae: 14.6217 - val_loss: 218.4480 - val_mae: 13.0954\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 282.1841 - mae: 13.2824 - val_loss: 196.5943 - val_mae: 12.0374\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 268.1845 - mae: 12.5645 - val_loss: 167.7791 - val_mae: 10.9959\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 258.8217 - mae: 12.2514 - val_loss: 153.2180 - val_mae: 10.2011\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 245.4183 - mae: 11.9037 - val_loss: 155.9278 - val_mae: 10.2017\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 241.4153 - mae: 11.7279 - val_loss: 167.6407 - val_mae: 10.2312\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 236.8528 - mae: 11.4298 - val_loss: 171.4857 - val_mae: 10.1287\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 240.9461 - mae: 11.4889 - val_loss: 151.5448 - val_mae: 9.7347\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 233.5517 - mae: 11.4134 - val_loss: 142.7534 - val_mae: 9.4701\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 229.4875 - mae: 11.1243 - val_loss: 132.5115 - val_mae: 9.2510\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 222.7974 - mae: 11.1360 - val_loss: 144.6468 - val_mae: 9.3991\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 222.0653 - mae: 11.0382 - val_loss: 127.3481 - val_mae: 9.0578\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 216.1752 - mae: 10.8999 - val_loss: 120.3343 - val_mae: 9.0053\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 213.1047 - mae: 10.8940 - val_loss: 118.5130 - val_mae: 8.9758\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 212.8720 - mae: 10.8618 - val_loss: 118.1987 - val_mae: 8.7979\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 207.7185 - mae: 10.6496 - val_loss: 114.3928 - val_mae: 8.7218\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 206.4225 - mae: 10.6241 - val_loss: 114.5021 - val_mae: 8.6830\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 203.8713 - mae: 10.5315 - val_loss: 120.0482 - val_mae: 8.6929\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 200.0604 - mae: 10.4502 - val_loss: 108.9099 - val_mae: 8.5207\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 200.6204 - mae: 10.4653 - val_loss: 109.0904 - val_mae: 8.8045\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 197.4824 - mae: 10.6322 - val_loss: 125.2148 - val_mae: 8.6290\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 197.7796 - mae: 10.2511 - val_loss: 105.1174 - val_mae: 8.3039\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 189.4704 - mae: 10.1132 - val_loss: 101.3866 - val_mae: 8.3329\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 192.2034 - mae: 10.2371 - val_loss: 110.4565 - val_mae: 8.2858\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 193.2630 - mae: 10.2087 - val_loss: 105.3364 - val_mae: 8.1664\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 199.9343 - mae: 10.4092 - val_loss: 97.9547 - val_mae: 8.0448\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 183.2732 - mae: 9.9109 - val_loss: 106.2245 - val_mae: 8.1178\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 180.2023 - mae: 9.8261 - val_loss: 94.3662 - val_mae: 7.9198\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.2263 - mae: 9.7664 - val_loss: 92.0291 - val_mae: 8.0233\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.7068 - mae: 9.8821 - val_loss: 95.2500 - val_mae: 7.8162\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 177.7681 - mae: 9.7699 - val_loss: 115.6710 - val_mae: 8.2766\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 176.9709 - mae: 9.8261 - val_loss: 95.7977 - val_mae: 7.7158\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 173.6978 - mae: 9.6985 - val_loss: 86.4901 - val_mae: 7.8443\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 175.2944 - mae: 9.7369 - val_loss: 85.1267 - val_mae: 7.5591\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 169.8569 - mae: 9.5374 - val_loss: 85.8873 - val_mae: 7.4745\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.5298 - mae: 9.3861 - val_loss: 84.6255 - val_mae: 7.4104\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.3758 - mae: 9.2736 - val_loss: 81.0602 - val_mae: 7.5818\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.2352 - mae: 9.6177 - val_loss: 90.1457 - val_mae: 7.4723\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.1209 - mae: 9.2431 - val_loss: 78.9800 - val_mae: 7.3696\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.2619 - mae: 9.3722 - val_loss: 82.6139 - val_mae: 7.2476\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 158.1388 - mae: 9.1271 - val_loss: 76.2764 - val_mae: 7.2816\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.5892 - mae: 9.2332 - val_loss: 77.9775 - val_mae: 7.1417\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.6024 - mae: 9.0949 - val_loss: 75.3903 - val_mae: 7.1072\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.0028 - mae: 9.1059 - val_loss: 73.3569 - val_mae: 7.1623\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.0999 - mae: 9.1639 - val_loss: 80.3072 - val_mae: 7.0936\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.8690 - mae: 9.0305 - val_loss: 79.3035 - val_mae: 7.0530\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.4437 - mae: 8.9721 - val_loss: 90.3879 - val_mae: 7.4277\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 153.1126 - mae: 9.0003 - val_loss: 71.8371 - val_mae: 6.9484\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.5959 - mae: 8.9465 - val_loss: 75.3880 - val_mae: 6.9170\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.8722 - mae: 8.9309 - val_loss: 70.7166 - val_mae: 7.2728\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.6278 - mae: 9.0044 - val_loss: 71.3749 - val_mae: 6.8094\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.7256 - mae: 8.7823 - val_loss: 67.8894 - val_mae: 7.1037\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.8112 - mae: 8.9535 - val_loss: 79.2767 - val_mae: 7.0217\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.7716 - mae: 8.8144 - val_loss: 64.9060 - val_mae: 6.6499\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.1657 - mae: 8.7389 - val_loss: 63.2861 - val_mae: 6.6170\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.7829 - mae: 8.6468 - val_loss: 81.2846 - val_mae: 7.0434\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.6896 - mae: 8.5425 - val_loss: 77.5693 - val_mae: 7.7455\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.1599 - mae: 8.7732 - val_loss: 65.5938 - val_mae: 6.5720\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.4030 - mae: 8.5818 - val_loss: 60.9057 - val_mae: 6.6474\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.4783 - mae: 8.4637 - val_loss: 64.0819 - val_mae: 7.0034\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.4286 - mae: 8.6702 - val_loss: 60.1917 - val_mae: 6.4079\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.5774 - mae: 8.4689 - val_loss: 64.9429 - val_mae: 6.5007\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.0308 - mae: 8.7109 - val_loss: 60.2462 - val_mae: 6.3751\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.3338 - mae: 8.4679 - val_loss: 57.3165 - val_mae: 6.4857\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.2976 - mae: 8.5727 - val_loss: 57.5240 - val_mae: 6.5412\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.0379 - mae: 8.4663 - val_loss: 56.2747 - val_mae: 6.2599\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.5923 - mae: 8.2961 - val_loss: 62.3570 - val_mae: 6.3913\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.7536 - mae: 8.3059 - val_loss: 57.4636 - val_mae: 6.5941\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.1591 - mae: 8.3068 - val_loss: 62.1282 - val_mae: 6.9213\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.2007 - mae: 8.5857 - val_loss: 54.2317 - val_mae: 6.1933\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.0681 - mae: 8.2883 - val_loss: 57.4757 - val_mae: 6.6406\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 129.2622 - mae: 8.3375 - val_loss: 68.1807 - val_mae: 6.5175\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.7269 - mae: 8.2920 - val_loss: 52.2032 - val_mae: 6.1289\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.0808 - mae: 8.2215 - val_loss: 52.2658 - val_mae: 6.2513\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.0129 - mae: 8.2145 - val_loss: 54.8593 - val_mae: 6.0979\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.2227 - mae: 8.1057 - val_loss: 54.0525 - val_mae: 6.0649\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.0640 - mae: 8.0797 - val_loss: 50.9368 - val_mae: 6.1782\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.0318 - mae: 8.2058 - val_loss: 58.1559 - val_mae: 6.1619\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.7468 - mae: 8.0128 - val_loss: 49.4934 - val_mae: 5.9047\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.1978 - mae: 8.0927 - val_loss: 50.5952 - val_mae: 5.9396\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.3214 - mae: 8.0642 - val_loss: 50.1892 - val_mae: 5.9137\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.9139 - mae: 8.0768 - val_loss: 51.2913 - val_mae: 5.9278\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.0877 - mae: 8.0942 - val_loss: 51.4188 - val_mae: 5.9182\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.4785 - mae: 8.0092 - val_loss: 72.0784 - val_mae: 6.4602\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.0943 - mae: 8.0788 - val_loss: 53.9067 - val_mae: 6.4096\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.2881 - mae: 8.0843 - val_loss: 55.3013 - val_mae: 6.0066\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.3448 - mae: 8.0038 - val_loss: 48.3507 - val_mae: 5.8146\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.5020 - mae: 7.8988 - val_loss: 46.1494 - val_mae: 5.8408\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.2936 - mae: 7.9408 - val_loss: 49.0148 - val_mae: 5.7820\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.4008 - mae: 7.9280 - val_loss: 44.9631 - val_mae: 5.7175\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.9902 - mae: 7.8530 - val_loss: 44.9963 - val_mae: 5.7561\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.2967 - mae: 7.8831 - val_loss: 47.5696 - val_mae: 5.7380\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.2616 - mae: 7.9402 - val_loss: 46.4329 - val_mae: 5.7113\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.0568 - mae: 7.8627 - val_loss: 45.3721 - val_mae: 5.8211\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.1054 - mae: 7.8855 - val_loss: 53.2988 - val_mae: 5.8533\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.9651 - mae: 7.8782 - val_loss: 46.1463 - val_mae: 5.6704\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.6701 - mae: 7.8799 - val_loss: 43.7693 - val_mae: 5.7142\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.7905 - mae: 7.8237 - val_loss: 45.4665 - val_mae: 5.6180\n",
      "5/5 [==============================] - 0s 615us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 9792.4814 - mae: 65.6620 - val_loss: 734.4864 - val_mae: 23.9679\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 580.5439 - mae: 19.9427 - val_loss: 360.9271 - val_mae: 17.3549\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 354.2048 - mae: 15.5619 - val_loss: 250.1675 - val_mae: 13.2232\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 250.6664 - mae: 12.9240 - val_loss: 165.4502 - val_mae: 10.5343\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 205.1896 - mae: 11.4751 - val_loss: 118.7079 - val_mae: 8.6826\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 188.7022 - mae: 10.8164 - val_loss: 89.9122 - val_mae: 7.1584\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 176.3732 - mae: 10.1755 - val_loss: 90.2025 - val_mae: 7.0436\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 176.8834 - mae: 10.2706 - val_loss: 103.1042 - val_mae: 7.6491\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 167.0705 - mae: 10.0537 - val_loss: 103.5269 - val_mae: 7.7487\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 165.2452 - mae: 10.0249 - val_loss: 113.1859 - val_mae: 8.2139\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.3842 - mae: 10.0928 - val_loss: 77.9518 - val_mae: 6.3831\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 164.3970 - mae: 10.0073 - val_loss: 97.1584 - val_mae: 7.5053\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.8736 - mae: 10.0278 - val_loss: 93.5016 - val_mae: 7.3242\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 165.9979 - mae: 10.1192 - val_loss: 87.5023 - val_mae: 6.9921\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.1449 - mae: 9.9298 - val_loss: 82.5206 - val_mae: 6.7108\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.2304 - mae: 9.9285 - val_loss: 82.4604 - val_mae: 6.7205\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.4170 - mae: 9.8296 - val_loss: 102.5139 - val_mae: 7.8015\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.0202 - mae: 9.9550 - val_loss: 83.8496 - val_mae: 6.8367\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.3296 - mae: 9.7032 - val_loss: 91.8347 - val_mae: 7.3034\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 161.1298 - mae: 9.9542 - val_loss: 77.9531 - val_mae: 6.5191\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.6973 - mae: 9.8002 - val_loss: 76.3764 - val_mae: 6.4513\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.6824 - mae: 9.7157 - val_loss: 71.4652 - val_mae: 6.1332\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.6517 - mae: 9.5988 - val_loss: 79.9034 - val_mae: 6.6762\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.8213 - mae: 9.6926 - val_loss: 89.3937 - val_mae: 7.2095\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.0901 - mae: 9.7142 - val_loss: 74.1575 - val_mae: 6.3451\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.9670 - mae: 9.6359 - val_loss: 83.1382 - val_mae: 6.8864\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.6266 - mae: 9.6518 - val_loss: 67.2371 - val_mae: 5.9144\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 156.7727 - mae: 9.7114 - val_loss: 64.3038 - val_mae: 5.7436\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.6172 - mae: 9.4859 - val_loss: 74.4442 - val_mae: 6.3795\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.3876 - mae: 9.5096 - val_loss: 65.2367 - val_mae: 5.8140\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 153.7104 - mae: 9.5114 - val_loss: 63.9659 - val_mae: 5.7345\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 154.5811 - mae: 9.3500 - val_loss: 112.2225 - val_mae: 8.2854\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.1118 - mae: 9.6476 - val_loss: 83.4261 - val_mae: 6.9401\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.2703 - mae: 9.5056 - val_loss: 74.6683 - val_mae: 6.4368\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.7587 - mae: 9.3190 - val_loss: 80.0936 - val_mae: 6.7693\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.0598 - mae: 9.4714 - val_loss: 65.5504 - val_mae: 5.8776\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 151.9372 - mae: 9.5291 - val_loss: 60.4937 - val_mae: 5.5776\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.2370 - mae: 9.3834 - val_loss: 57.5726 - val_mae: 5.4074\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 148.6658 - mae: 9.3175 - val_loss: 70.5454 - val_mae: 6.2191\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.5910 - mae: 9.2255 - val_loss: 85.8835 - val_mae: 7.0771\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.6865 - mae: 9.4613 - val_loss: 57.2867 - val_mae: 5.4113\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.4155 - mae: 9.3471 - val_loss: 56.6941 - val_mae: 5.3765\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 148.1316 - mae: 9.3559 - val_loss: 64.8622 - val_mae: 5.8909\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.0633 - mae: 9.2562 - val_loss: 71.1900 - val_mae: 6.2848\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.9806 - mae: 9.4034 - val_loss: 54.0903 - val_mae: 5.3257\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.6151 - mae: 9.2828 - val_loss: 56.5767 - val_mae: 5.3653\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 145.0288 - mae: 9.1544 - val_loss: 58.9304 - val_mae: 5.5304\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.7458 - mae: 9.1064 - val_loss: 75.6297 - val_mae: 6.5599\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.2567 - mae: 9.1334 - val_loss: 122.7573 - val_mae: 8.7393\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.5605 - mae: 9.4645 - val_loss: 56.5166 - val_mae: 5.3925\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.5100 - mae: 9.2019 - val_loss: 65.4160 - val_mae: 5.9592\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 148.5398 - mae: 9.3647 - val_loss: 78.1622 - val_mae: 6.6938\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.5970 - mae: 9.1638 - val_loss: 64.9394 - val_mae: 5.9353\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.1329 - mae: 9.2360 - val_loss: 52.9687 - val_mae: 5.1642\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 141.7536 - mae: 8.9471 - val_loss: 75.4031 - val_mae: 6.5483\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 146.8100 - mae: 9.3748 - val_loss: 57.0083 - val_mae: 5.4245\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.1449 - mae: 8.9992 - val_loss: 67.5976 - val_mae: 6.1039\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.1691 - mae: 9.0730 - val_loss: 67.3334 - val_mae: 6.0897\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.3635 - mae: 9.1231 - val_loss: 63.6017 - val_mae: 5.8626\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.5188 - mae: 9.0504 - val_loss: 66.0830 - val_mae: 6.0132\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.1947 - mae: 9.1066 - val_loss: 59.7971 - val_mae: 5.6184\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.0452 - mae: 9.0130 - val_loss: 51.3305 - val_mae: 5.0854\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.4743 - mae: 8.9931 - val_loss: 51.0120 - val_mae: 5.0680\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.3871 - mae: 9.0089 - val_loss: 54.1331 - val_mae: 5.2608\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.0374 - mae: 8.7944 - val_loss: 59.9389 - val_mae: 5.6391\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.5930 - mae: 8.9196 - val_loss: 68.0966 - val_mae: 6.1423\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.0376 - mae: 9.0607 - val_loss: 48.5422 - val_mae: 4.9325\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 141.1864 - mae: 8.9023 - val_loss: 69.7409 - val_mae: 6.2349\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 139.4118 - mae: 9.0877 - val_loss: 55.7954 - val_mae: 5.3750\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 137.2392 - mae: 8.8983 - val_loss: 52.4086 - val_mae: 5.1606\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.7216 - mae: 8.9229 - val_loss: 47.3465 - val_mae: 4.9173\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 138.7733 - mae: 8.8018 - val_loss: 66.6772 - val_mae: 6.0573\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.0593 - mae: 9.0191 - val_loss: 57.1971 - val_mae: 5.4683\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.7421 - mae: 8.8254 - val_loss: 79.2607 - val_mae: 6.7574\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 140.6566 - mae: 9.1054 - val_loss: 64.7761 - val_mae: 5.9410\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.1416 - mae: 8.8690 - val_loss: 56.4772 - val_mae: 5.4277\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.0092 - mae: 8.7660 - val_loss: 72.9620 - val_mae: 6.4287\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.7907 - mae: 8.9470 - val_loss: 45.9894 - val_mae: 4.8642\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.4198 - mae: 8.7783 - val_loss: 52.1270 - val_mae: 5.1514\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 137.1643 - mae: 8.8966 - val_loss: 59.8191 - val_mae: 5.6260\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.2310 - mae: 8.9300 - val_loss: 45.5896 - val_mae: 4.8305\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.9297 - mae: 8.7600 - val_loss: 66.5065 - val_mae: 6.0381\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.7750 - mae: 8.8053 - val_loss: 75.0763 - val_mae: 6.5324\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.1826 - mae: 8.9571 - val_loss: 57.4376 - val_mae: 5.4654\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.1839 - mae: 8.6736 - val_loss: 84.6246 - val_mae: 6.9848\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.9802 - mae: 9.0416 - val_loss: 52.7020 - val_mae: 5.1783\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.6699 - mae: 8.7295 - val_loss: 60.1511 - val_mae: 5.6502\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.3300 - mae: 8.9079 - val_loss: 72.1627 - val_mae: 6.3630\n",
      "Epoch 00088: early stopping\n",
      "5/5 [==============================] - 0s 641us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7158.7529 - mae: 68.3478 - val_loss: 1874.7792 - val_mae: 36.7998\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2971.0684 - mae: 41.9939 - val_loss: 780.5989 - val_mae: 22.9844\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1360.8099 - mae: 28.8058 - val_loss: 355.0126 - val_mae: 15.1077\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 651.8373 - mae: 19.1882 - val_loss: 187.6443 - val_mae: 11.0206\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 357.6694 - mae: 14.4630 - val_loss: 145.1525 - val_mae: 9.6861\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 232.0721 - mae: 11.8059 - val_loss: 124.5302 - val_mae: 8.6980\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 179.1978 - mae: 10.5336 - val_loss: 128.0661 - val_mae: 9.3956\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.3878 - mae: 10.1129 - val_loss: 146.5666 - val_mae: 8.7433\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.0643 - mae: 9.2030 - val_loss: 132.6919 - val_mae: 8.5659\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 136.2462 - mae: 9.2250 - val_loss: 138.8913 - val_mae: 8.3736\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 132.3128 - mae: 8.8279 - val_loss: 138.5083 - val_mae: 9.3922\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.0019 - mae: 9.0907 - val_loss: 135.9878 - val_mae: 8.7360\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.4150 - mae: 8.6851 - val_loss: 137.1855 - val_mae: 9.1274\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.8067 - mae: 8.9441 - val_loss: 135.4003 - val_mae: 8.8338\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 127.3878 - mae: 8.7644 - val_loss: 136.5637 - val_mae: 8.5321\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 127.9423 - mae: 8.6740 - val_loss: 133.5059 - val_mae: 8.8231\n",
      "Epoch 00016: early stopping\n",
      "5/5 [==============================] - 0s 646us/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00010: early stopping\n",
      "5/5 [==============================] - 0s 710us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00010: early stopping\n",
      "5/5 [==============================] - 0s 741us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00010: early stopping\n",
      "5/5 [==============================] - 0s 777us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   0.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 636.4366 - mae: 18.6748 - val_loss: 139.5446 - val_mae: 10.4145\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 203.5132 - mae: 11.5339 - val_loss: 127.9247 - val_mae: 9.9393\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 190.0549 - mae: 11.0336 - val_loss: 114.2931 - val_mae: 9.4569\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 177.8904 - mae: 10.6050 - val_loss: 103.3290 - val_mae: 8.8058\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 157.2826 - mae: 9.7053 - val_loss: 83.4960 - val_mae: 7.9774\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 143.7779 - mae: 9.1566 - val_loss: 72.9920 - val_mae: 7.5460\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.8656 - mae: 8.8222 - val_loss: 73.7968 - val_mae: 7.4347\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.6703 - mae: 8.7319 - val_loss: 72.8750 - val_mae: 7.3711\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.3413 - mae: 8.6162 - val_loss: 77.9641 - val_mae: 7.3641\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.0199 - mae: 8.5128 - val_loss: 71.0474 - val_mae: 7.2839\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.8772 - mae: 8.5314 - val_loss: 76.8008 - val_mae: 7.2624\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.2088 - mae: 8.4384 - val_loss: 69.4760 - val_mae: 7.1831\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.3174 - mae: 8.3745 - val_loss: 71.5995 - val_mae: 7.1397\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.9684 - mae: 8.3755 - val_loss: 67.7566 - val_mae: 7.0854\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.1587 - mae: 8.2434 - val_loss: 64.2978 - val_mae: 7.0756\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.2712 - mae: 8.2311 - val_loss: 62.6880 - val_mae: 7.0657\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.0830 - mae: 8.2646 - val_loss: 65.6679 - val_mae: 6.9434\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 120.1696 - mae: 8.0933 - val_loss: 70.9170 - val_mae: 6.9189\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.4230 - mae: 7.9898 - val_loss: 70.6102 - val_mae: 6.8950\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.1080 - mae: 7.9792 - val_loss: 66.1298 - val_mae: 6.7717\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.5205 - mae: 7.9487 - val_loss: 61.9307 - val_mae: 6.7466\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.4086 - mae: 7.9015 - val_loss: 60.2774 - val_mae: 6.7019\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.8821 - mae: 7.8215 - val_loss: 66.0022 - val_mae: 6.6621\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.5092 - mae: 7.8047 - val_loss: 62.7122 - val_mae: 6.5441\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.2364 - mae: 7.7862 - val_loss: 59.8569 - val_mae: 6.5137\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.5792 - mae: 7.6951 - val_loss: 59.7114 - val_mae: 6.4467\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.3098 - mae: 7.6949 - val_loss: 59.7097 - val_mae: 6.4048\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.8333 - mae: 7.6833 - val_loss: 56.6545 - val_mae: 6.3474\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.2029 - mae: 7.5538 - val_loss: 62.6805 - val_mae: 6.4393\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.0078 - mae: 7.5506 - val_loss: 53.1567 - val_mae: 6.2904\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.1878 - mae: 7.4402 - val_loss: 51.5904 - val_mae: 6.2717\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.0699 - mae: 7.4031 - val_loss: 57.2837 - val_mae: 6.2110\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.3282 - mae: 7.3703 - val_loss: 58.7103 - val_mae: 6.2302\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.0552 - mae: 7.3797 - val_loss: 56.1155 - val_mae: 6.1294\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.5418 - mae: 7.3132 - val_loss: 53.7142 - val_mae: 6.0431\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.1953 - mae: 7.2281 - val_loss: 47.5773 - val_mae: 5.9786\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.8559 - mae: 7.1616 - val_loss: 49.4158 - val_mae: 5.9117\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.5156 - mae: 7.1655 - val_loss: 46.5770 - val_mae: 5.8355\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.0034 - mae: 7.0539 - val_loss: 51.3948 - val_mae: 5.8634\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.1691 - mae: 7.1241 - val_loss: 52.3608 - val_mae: 5.8540\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.4871 - mae: 7.1171 - val_loss: 45.1222 - val_mae: 5.6803\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.9554 - mae: 6.9759 - val_loss: 44.0325 - val_mae: 5.6233\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.4229 - mae: 6.9787 - val_loss: 41.4472 - val_mae: 5.5461\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.3303 - mae: 6.8778 - val_loss: 43.1958 - val_mae: 5.5144\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.2750 - mae: 6.9326 - val_loss: 40.4620 - val_mae: 5.4362\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.2559 - mae: 6.8004 - val_loss: 39.8910 - val_mae: 5.3833\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.8815 - mae: 6.8215 - val_loss: 41.3802 - val_mae: 5.3752\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.7571 - mae: 6.7858 - val_loss: 40.5944 - val_mae: 5.3181\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.4449 - mae: 6.6944 - val_loss: 40.9429 - val_mae: 5.3057\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.7094 - mae: 6.6700 - val_loss: 38.5927 - val_mae: 5.2004\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.0311 - mae: 6.5970 - val_loss: 38.7037 - val_mae: 5.1615\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.3451 - mae: 6.6676 - val_loss: 36.5116 - val_mae: 5.0754\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.4923 - mae: 6.5869 - val_loss: 36.9051 - val_mae: 5.0456\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.8889 - mae: 6.7011 - val_loss: 32.5195 - val_mae: 4.9038\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.8933 - mae: 6.5038 - val_loss: 34.9353 - val_mae: 4.9268\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.7053 - mae: 6.5724 - val_loss: 33.3495 - val_mae: 4.8460\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.4468 - mae: 6.5286 - val_loss: 33.3444 - val_mae: 4.8114\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.7651 - mae: 6.5483 - val_loss: 33.8063 - val_mae: 4.7875\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.3590 - mae: 6.6420 - val_loss: 30.2761 - val_mae: 4.6423\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.1088 - mae: 6.4622 - val_loss: 32.1891 - val_mae: 4.6833\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.8834 - mae: 6.5339 - val_loss: 31.0372 - val_mae: 4.6188\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.1036 - mae: 6.5397 - val_loss: 29.4080 - val_mae: 4.5347\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.4113 - mae: 6.4949 - val_loss: 29.7260 - val_mae: 4.5217\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.1759 - mae: 6.4617 - val_loss: 30.9146 - val_mae: 4.5276\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.4021 - mae: 6.5016 - val_loss: 29.5006 - val_mae: 4.4667\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.7456 - mae: 6.4558 - val_loss: 29.3747 - val_mae: 4.4320\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.0609 - mae: 6.4612 - val_loss: 30.3339 - val_mae: 4.4316\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.4145 - mae: 6.5048 - val_loss: 30.1775 - val_mae: 4.3851\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.2801 - mae: 6.5299 - val_loss: 28.6127 - val_mae: 4.2948\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.5425 - mae: 6.4240 - val_loss: 28.6215 - val_mae: 4.2836\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.8159 - mae: 6.5675 - val_loss: 27.2637 - val_mae: 4.3679\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.5699 - mae: 6.3310 - val_loss: 29.7349 - val_mae: 4.3169\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.3932 - mae: 6.5478 - val_loss: 27.3637 - val_mae: 4.3559\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.8892 - mae: 6.4242 - val_loss: 29.6340 - val_mae: 4.3172\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.8380 - mae: 6.4869 - val_loss: 28.6897 - val_mae: 4.3358\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.0239 - mae: 6.4744 - val_loss: 28.1644 - val_mae: 4.3713\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.3839 - mae: 6.4888 - val_loss: 28.1741 - val_mae: 4.3744\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.6893 - mae: 6.4626 - val_loss: 27.9716 - val_mae: 4.3873\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.6250 - mae: 6.4306 - val_loss: 28.0630 - val_mae: 4.4271\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.7248 - mae: 6.4920 - val_loss: 29.9548 - val_mae: 4.4554\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 91.0348 - mae: 6.30 - 0s 3ms/step - loss: 81.1348 - mae: 6.5596 - val_loss: 28.1109 - val_mae: 4.5148\n",
      "Epoch 00081: early stopping\n",
      "5/5 [==============================] - 0s 704us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   2.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 901.1600 - mae: 22.1313 - val_loss: 61.2416 - val_mae: 6.6865\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.7271 - mae: 8.6179 - val_loss: 53.0649 - val_mae: 6.3522\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 127.4992 - mae: 8.2685 - val_loss: 66.7479 - val_mae: 6.4456\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.1911 - mae: 8.1253 - val_loss: 63.2846 - val_mae: 6.2582\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.8747 - mae: 8.1169 - val_loss: 46.7513 - val_mae: 5.6173\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.7718 - mae: 7.8850 - val_loss: 36.8714 - val_mae: 5.0853\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.3774 - mae: 7.6797 - val_loss: 38.2554 - val_mae: 5.1173\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.4644 - mae: 7.6934 - val_loss: 45.9386 - val_mae: 5.6653\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.1936 - mae: 7.4763 - val_loss: 55.7247 - val_mae: 6.3842\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.4721 - mae: 7.5451 - val_loss: 53.8321 - val_mae: 6.3301\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.9061 - mae: 7.6397 - val_loss: 32.7467 - val_mae: 4.8351\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.4340 - mae: 7.3330 - val_loss: 43.9418 - val_mae: 5.7531\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.9546 - mae: 7.4668 - val_loss: 39.7630 - val_mae: 5.5050\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.9938 - mae: 7.5439 - val_loss: 30.5950 - val_mae: 4.7795\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.4146 - mae: 7.2809 - val_loss: 33.1074 - val_mae: 5.0392\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.7721 - mae: 7.2332 - val_loss: 30.6082 - val_mae: 4.8253\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.5469 - mae: 7.1313 - val_loss: 55.2920 - val_mae: 6.5549\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.9598 - mae: 7.3599 - val_loss: 37.4574 - val_mae: 5.4224\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.9500 - mae: 7.1889 - val_loss: 45.1806 - val_mae: 5.9451\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.6474 - mae: 7.2190 - val_loss: 35.7712 - val_mae: 5.3041\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.4114 - mae: 7.1957 - val_loss: 30.0488 - val_mae: 4.8397\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.5193 - mae: 7.0475 - val_loss: 26.1690 - val_mae: 4.4688\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.1433 - mae: 7.0177 - val_loss: 41.0509 - val_mae: 5.6336\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.9597 - mae: 7.0665 - val_loss: 34.1762 - val_mae: 5.1674\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.1993 - mae: 7.0766 - val_loss: 31.9842 - val_mae: 5.0098\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.8406 - mae: 7.0870 - val_loss: 37.7427 - val_mae: 5.4019\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.5575 - mae: 7.1145 - val_loss: 21.2094 - val_mae: 3.8234\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 94.1081 - mae: 7.0332 - val_loss: 22.9092 - val_mae: 4.0835\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.6155 - mae: 6.8961 - val_loss: 27.1002 - val_mae: 4.5929\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 90.0125 - mae: 6.9943 - val_loss: 25.6822 - val_mae: 4.4410\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 92.8061 - mae: 7.1218 - val_loss: 20.7158 - val_mae: 3.5430\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.7042 - mae: 6.7614 - val_loss: 57.1553 - val_mae: 6.6384\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.4259 - mae: 7.1482 - val_loss: 38.7186 - val_mae: 5.4308\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.9261 - mae: 6.9789 - val_loss: 46.2011 - val_mae: 5.9128\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.1928 - mae: 7.0279 - val_loss: 40.1147 - val_mae: 5.5022\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.7884 - mae: 7.0828 - val_loss: 28.1404 - val_mae: 4.6767\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.6641 - mae: 7.0546 - val_loss: 20.6637 - val_mae: 3.7952\n",
      "Epoch 00037: early stopping\n",
      "5/5 [==============================] - 0s 646us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   1.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1653.7155 - mae: 30.7272 - val_loss: 97.4602 - val_mae: 7.8761\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.4242 - mae: 7.6982 - val_loss: 90.1092 - val_mae: 7.0493\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.9263 - mae: 7.4361 - val_loss: 86.2051 - val_mae: 6.9252\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.7848 - mae: 6.9562 - val_loss: 82.7497 - val_mae: 6.8745\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.6438 - mae: 6.8955 - val_loss: 80.5393 - val_mae: 6.4333\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.2818 - mae: 6.6779 - val_loss: 77.8549 - val_mae: 6.3038\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.2855 - mae: 6.6077 - val_loss: 74.8579 - val_mae: 6.6192\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.3163 - mae: 6.6900 - val_loss: 74.7028 - val_mae: 6.0790\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.0009 - mae: 6.3239 - val_loss: 70.1356 - val_mae: 6.0483\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.1133 - mae: 6.2553 - val_loss: 67.3236 - val_mae: 6.0008\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.1093 - mae: 6.1260 - val_loss: 64.9045 - val_mae: 5.9740\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.6619 - mae: 6.1579 - val_loss: 64.4640 - val_mae: 5.9447\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.8325 - mae: 5.8758 - val_loss: 61.6265 - val_mae: 5.8996\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.0566 - mae: 5.9005 - val_loss: 59.4775 - val_mae: 5.9123\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.1258 - mae: 5.9054 - val_loss: 61.0071 - val_mae: 5.8836\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.1074 - mae: 5.7599 - val_loss: 56.7806 - val_mae: 5.8773\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.4198 - mae: 5.8926 - val_loss: 60.3239 - val_mae: 5.8569\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.7787 - mae: 5.6659 - val_loss: 56.2175 - val_mae: 5.7952\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.2623 - mae: 5.6976 - val_loss: 54.8447 - val_mae: 5.7821\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.9413 - mae: 5.6186 - val_loss: 53.4080 - val_mae: 5.7847\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.3752 - mae: 5.7124 - val_loss: 54.1958 - val_mae: 5.7726\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.1288 - mae: 5.5137 - val_loss: 52.1116 - val_mae: 5.7864\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.8480 - mae: 5.6364 - val_loss: 51.8614 - val_mae: 5.7766\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.7445 - mae: 5.5594 - val_loss: 51.8073 - val_mae: 5.8150\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.0027 - mae: 5.6555 - val_loss: 51.5853 - val_mae: 5.7531\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.9392 - mae: 5.4906 - val_loss: 51.0829 - val_mae: 5.8016\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.1700 - mae: 5.7066 - val_loss: 51.8627 - val_mae: 5.7373\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.5607 - mae: 5.6077 - val_loss: 52.0967 - val_mae: 5.7249\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.7755 - mae: 5.4981 - val_loss: 51.5178 - val_mae: 5.7186\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.7916 - mae: 5.5187 - val_loss: 50.6049 - val_mae: 5.7241\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.3959 - mae: 5.5746 - val_loss: 51.4820 - val_mae: 5.7064\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.8474 - mae: 5.4381 - val_loss: 50.0448 - val_mae: 5.7942\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.2331 - mae: 5.6216 - val_loss: 53.4166 - val_mae: 5.6765\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.0532 - mae: 5.5221 - val_loss: 52.0246 - val_mae: 5.6755\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.8838 - mae: 5.4097 - val_loss: 49.4009 - val_mae: 5.7354\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.0064 - mae: 5.5623 - val_loss: 51.4347 - val_mae: 5.6621\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.8419 - mae: 5.5922 - val_loss: 52.5572 - val_mae: 5.6500\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.2534 - mae: 5.4792 - val_loss: 51.4122 - val_mae: 5.6468\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.8023 - mae: 5.4825 - val_loss: 49.3257 - val_mae: 5.6773\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.6798 - mae: 5.4805 - val_loss: 49.7152 - val_mae: 5.6539\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.2187 - mae: 5.5287 - val_loss: 51.6048 - val_mae: 5.6280\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.2765 - mae: 5.2764 - val_loss: 54.2753 - val_mae: 6.1278\n",
      "Epoch 00042: early stopping\n",
      "5/5 [==============================] - 0s 702us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=5, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>; total time=   1.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 820.8484 - mae: 22.6877 - val_loss: 881.6539 - val_mae: 23.5616\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 732.5406 - mae: 21.2138 - val_loss: 781.9116 - val_mae: 21.8582\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 649.7346 - mae: 19.7651 - val_loss: 692.5528 - val_mae: 20.2362\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 578.5155 - mae: 18.3998 - val_loss: 611.6966 - val_mae: 18.6585\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 516.7227 - mae: 17.1646 - val_loss: 540.9202 - val_mae: 17.1896\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 460.1245 - mae: 16.0218 - val_loss: 480.2073 - val_mae: 15.9417\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 414.4311 - mae: 15.0095 - val_loss: 426.1292 - val_mae: 14.8032\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 374.8193 - mae: 14.0912 - val_loss: 378.5387 - val_mae: 13.8336\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 339.4743 - mae: 13.3005 - val_loss: 338.4061 - val_mae: 13.0787\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 310.1376 - mae: 12.6790 - val_loss: 304.8082 - val_mae: 12.4811\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 287.2821 - mae: 12.2039 - val_loss: 274.5450 - val_mae: 11.9566\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 265.8520 - mae: 11.8111 - val_loss: 249.4248 - val_mae: 11.5073\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 248.1522 - mae: 11.4794 - val_loss: 227.5155 - val_mae: 11.1513\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 233.8741 - mae: 11.1990 - val_loss: 208.0706 - val_mae: 10.8049\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 219.7928 - mae: 10.9389 - val_loss: 191.9370 - val_mae: 10.5239\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 208.4784 - mae: 10.7065 - val_loss: 177.3094 - val_mae: 10.2492\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 198.8058 - mae: 10.4737 - val_loss: 163.3750 - val_mae: 9.9704\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 189.6109 - mae: 10.2532 - val_loss: 151.3443 - val_mae: 9.6976\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 181.2569 - mae: 10.0515 - val_loss: 140.9736 - val_mae: 9.4335\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 173.7886 - mae: 9.8471 - val_loss: 131.7616 - val_mae: 9.1693\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.7914 - mae: 9.6437 - val_loss: 123.6989 - val_mae: 8.9178\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 160.6303 - mae: 9.4512 - val_loss: 116.1621 - val_mae: 8.6679\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.0688 - mae: 9.2736 - val_loss: 108.7921 - val_mae: 8.4163\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 149.4299 - mae: 9.0832 - val_loss: 102.3489 - val_mae: 8.1726\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.2336 - mae: 8.8975 - val_loss: 96.5178 - val_mae: 7.9362\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.4001 - mae: 8.7180 - val_loss: 91.0112 - val_mae: 7.7063\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 134.9868 - mae: 8.5474 - val_loss: 85.6409 - val_mae: 7.4733\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.5274 - mae: 8.3721 - val_loss: 80.9050 - val_mae: 7.2491\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.3627 - mae: 8.1942 - val_loss: 76.3059 - val_mae: 7.0296\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.2730 - mae: 8.0246 - val_loss: 72.3265 - val_mae: 6.8193\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.7469 - mae: 7.8632 - val_loss: 68.3304 - val_mae: 6.6192\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.1871 - mae: 7.7083 - val_loss: 64.5174 - val_mae: 6.4232\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.0055 - mae: 7.5636 - val_loss: 61.2111 - val_mae: 6.2378\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.9531 - mae: 7.4167 - val_loss: 58.3585 - val_mae: 6.0663\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.2604 - mae: 7.2972 - val_loss: 55.7281 - val_mae: 5.8999\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.6400 - mae: 7.1719 - val_loss: 53.5341 - val_mae: 5.7492\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.2500 - mae: 7.0599 - val_loss: 51.1554 - val_mae: 5.6007\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.9447 - mae: 6.9594 - val_loss: 49.0244 - val_mae: 5.4735\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.9964 - mae: 6.8667 - val_loss: 46.9626 - val_mae: 5.3516\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.0995 - mae: 6.7800 - val_loss: 45.1923 - val_mae: 5.2520\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.2137 - mae: 6.7036 - val_loss: 43.8685 - val_mae: 5.1897\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.5760 - mae: 6.6384 - val_loss: 42.5116 - val_mae: 5.1137\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 89.9481 - mae: 6.5740 - val_loss: 41.3619 - val_mae: 5.0530\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.5752 - mae: 6.5170 - val_loss: 40.1052 - val_mae: 4.9755\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.2813 - mae: 6.4608 - val_loss: 39.1362 - val_mae: 4.9260\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 86.0712 - mae: 6.4079 - val_loss: 38.1436 - val_mae: 4.8635\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 84.9258 - mae: 6.3648 - val_loss: 37.2433 - val_mae: 4.7983\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.9226 - mae: 6.3168 - val_loss: 36.5267 - val_mae: 4.7511\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 83.0119 - mae: 6.2818 - val_loss: 35.8471 - val_mae: 4.7025\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.1184 - mae: 6.2488 - val_loss: 35.3528 - val_mae: 4.6714\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.3091 - mae: 6.2266 - val_loss: 34.9072 - val_mae: 4.6439\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.6183 - mae: 6.2051 - val_loss: 34.4213 - val_mae: 4.6132\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.8699 - mae: 6.1834 - val_loss: 33.9226 - val_mae: 4.5772\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 79.2712 - mae: 6.1692 - val_loss: 33.7181 - val_mae: 4.5927\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 78.6327 - mae: 6.1474 - val_loss: 33.0220 - val_mae: 4.5293\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.9588 - mae: 6.1192 - val_loss: 32.7127 - val_mae: 4.5195\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.4573 - mae: 6.1056 - val_loss: 32.3307 - val_mae: 4.4960\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.8744 - mae: 6.0825 - val_loss: 31.9752 - val_mae: 4.4694\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.3939 - mae: 6.0750 - val_loss: 31.9143 - val_mae: 4.4869\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.9611 - mae: 6.0657 - val_loss: 31.5217 - val_mae: 4.4570\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.4529 - mae: 6.0412 - val_loss: 31.2291 - val_mae: 4.4374\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 75.1061 - mae: 6.0331 - val_loss: 31.1433 - val_mae: 4.4473\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.7004 - mae: 6.0196 - val_loss: 30.7856 - val_mae: 4.4170\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.2995 - mae: 5.9949 - val_loss: 30.5215 - val_mae: 4.3963\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.0258 - mae: 5.9785 - val_loss: 30.3625 - val_mae: 4.3922\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.6955 - mae: 5.9741 - val_loss: 30.1921 - val_mae: 4.3829\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.3913 - mae: 5.9594 - val_loss: 29.9971 - val_mae: 4.3676\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.1101 - mae: 5.9561 - val_loss: 29.9356 - val_mae: 4.3708\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.8884 - mae: 5.9643 - val_loss: 29.9802 - val_mae: 4.3902\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.5460 - mae: 5.9615 - val_loss: 29.8719 - val_mae: 4.3820\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.3335 - mae: 5.9599 - val_loss: 29.7782 - val_mae: 4.3764\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.1279 - mae: 5.9373 - val_loss: 29.3983 - val_mae: 4.3262\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.9025 - mae: 5.9128 - val_loss: 29.3163 - val_mae: 4.3220\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.7068 - mae: 5.9023 - val_loss: 29.1184 - val_mae: 4.2974\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.5146 - mae: 5.8930 - val_loss: 29.0830 - val_mae: 4.2986\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.3221 - mae: 5.8878 - val_loss: 28.9648 - val_mae: 4.2868\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.1884 - mae: 5.8938 - val_loss: 28.9692 - val_mae: 4.2934\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.9692 - mae: 5.8970 - val_loss: 28.9595 - val_mae: 4.2966\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.8069 - mae: 5.8897 - val_loss: 28.8382 - val_mae: 4.2830\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.6631 - mae: 5.8820 - val_loss: 28.7185 - val_mae: 4.2693\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.5297 - mae: 5.8816 - val_loss: 28.7483 - val_mae: 4.2779\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.3731 - mae: 5.8767 - val_loss: 28.5058 - val_mae: 4.2457\n",
      "Epoch 00082: early stopping\n",
      "5/5 [==============================] - 0s 735us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   2.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 15ms/step - loss: 3245.9368 - mae: 51.4635 - val_loss: 3077.6853 - val_mae: 49.9519\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2952.9492 - mae: 48.7494 - val_loss: 2788.5334 - val_mae: 47.2162\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2679.8030 - mae: 46.0729 - val_loss: 2518.9690 - val_mae: 44.5278\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2422.7488 - mae: 43.4475 - val_loss: 2270.5669 - val_mae: 41.9073\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2184.7063 - mae: 40.8767 - val_loss: 2043.2456 - val_mae: 39.3721\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1969.9105 - mae: 38.3803 - val_loss: 1834.0992 - val_mae: 36.9000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1770.8514 - mae: 35.9772 - val_loss: 1643.5177 - val_mae: 34.5097\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1591.4349 - mae: 33.7147 - val_loss: 1467.8704 - val_mae: 32.2882\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1423.8823 - mae: 31.5572 - val_loss: 1311.1528 - val_mae: 30.1873\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1278.0623 - mae: 29.5632 - val_loss: 1168.8481 - val_mae: 28.2482\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1144.5289 - mae: 27.6597 - val_loss: 1040.7382 - val_mae: 26.3940\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1024.2798 - mae: 25.8610 - val_loss: 927.9542 - val_mae: 24.7194\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 918.1026 - mae: 24.1952 - val_loss: 826.3735 - val_mae: 23.1313\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 826.9589 - mae: 22.6430 - val_loss: 733.8821 - val_mae: 21.6160\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 740.0734 - mae: 21.1752 - val_loss: 654.4977 - val_mae: 20.3548\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 665.2556 - mae: 19.9184 - val_loss: 585.6710 - val_mae: 19.2712\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 601.7576 - mae: 18.8322 - val_loss: 524.2625 - val_mae: 18.2536\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 544.2247 - mae: 17.8829 - val_loss: 470.8393 - val_mae: 17.4377\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 493.7322 - mae: 17.0596 - val_loss: 424.9930 - val_mae: 16.7657\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 450.6515 - mae: 16.2930 - val_loss: 384.7606 - val_mae: 16.1801\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 414.4112 - mae: 15.5886 - val_loss: 348.6300 - val_mae: 15.6015\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 380.3695 - mae: 14.9765 - val_loss: 318.2648 - val_mae: 15.0678\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 351.8301 - mae: 14.4036 - val_loss: 292.2992 - val_mae: 14.6054\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 328.3128 - mae: 13.9155 - val_loss: 269.4800 - val_mae: 14.1763\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 307.4258 - mae: 13.4964 - val_loss: 250.0333 - val_mae: 13.7726\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 288.9179 - mae: 13.1476 - val_loss: 233.0917 - val_mae: 13.3845\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 272.6973 - mae: 12.8076 - val_loss: 218.5091 - val_mae: 13.0167\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 259.8533 - mae: 12.5645 - val_loss: 205.3691 - val_mae: 12.6548\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 247.4120 - mae: 12.3021 - val_loss: 194.3294 - val_mae: 12.3229\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 237.0124 - mae: 12.0822 - val_loss: 184.5287 - val_mae: 12.0480\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 227.7217 - mae: 11.8861 - val_loss: 175.9312 - val_mae: 11.7863\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 219.4707 - mae: 11.6966 - val_loss: 168.3016 - val_mae: 11.5370\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 212.2674 - mae: 11.5246 - val_loss: 161.1893 - val_mae: 11.2890\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 205.1802 - mae: 11.3476 - val_loss: 154.8256 - val_mae: 11.0541\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 199.2498 - mae: 11.1977 - val_loss: 148.8360 - val_mae: 10.8187\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 193.3024 - mae: 11.0274 - val_loss: 143.5541 - val_mae: 10.6001\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 188.1785 - mae: 10.8854 - val_loss: 138.3754 - val_mae: 10.3771\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 183.1625 - mae: 10.7308 - val_loss: 133.5767 - val_mae: 10.1630\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 178.6516 - mae: 10.5856 - val_loss: 129.1156 - val_mae: 9.9560\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 174.1433 - mae: 10.4368 - val_loss: 124.8944 - val_mae: 9.7593\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 170.2175 - mae: 10.2967 - val_loss: 120.9082 - val_mae: 9.5603\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 166.3568 - mae: 10.1547 - val_loss: 117.2356 - val_mae: 9.3747\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 162.7067 - mae: 10.0175 - val_loss: 113.6599 - val_mae: 9.2040\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 159.4286 - mae: 9.8892 - val_loss: 110.2109 - val_mae: 9.0250\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.9964 - mae: 9.7550 - val_loss: 107.1471 - val_mae: 8.8600\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 152.8577 - mae: 9.6275 - val_loss: 104.1768 - val_mae: 8.7044\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 150.0471 - mae: 9.5029 - val_loss: 101.3014 - val_mae: 8.5532\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 147.3592 - mae: 9.3886 - val_loss: 98.5841 - val_mae: 8.4020\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.7012 - mae: 9.2747 - val_loss: 96.0808 - val_mae: 8.2580\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 142.3251 - mae: 9.1670 - val_loss: 93.6562 - val_mae: 8.1046\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 139.9672 - mae: 9.0595 - val_loss: 91.2954 - val_mae: 7.9578\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 137.6531 - mae: 8.9493 - val_loss: 89.1119 - val_mae: 7.8190\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 135.6820 - mae: 8.8593 - val_loss: 86.9547 - val_mae: 7.6797\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.6996 - mae: 8.7638 - val_loss: 84.9911 - val_mae: 7.5498\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 131.7400 - mae: 8.6750 - val_loss: 83.1357 - val_mae: 7.4466\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 130.0138 - mae: 8.5956 - val_loss: 81.3703 - val_mae: 7.3588\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 128.2278 - mae: 8.5142 - val_loss: 79.6866 - val_mae: 7.2737\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 126.7243 - mae: 8.4448 - val_loss: 77.9849 - val_mae: 7.1860\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 125.2069 - mae: 8.3719 - val_loss: 76.2328 - val_mae: 7.0962\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 123.6848 - mae: 8.3052 - val_loss: 74.6990 - val_mae: 7.0145\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 122.3139 - mae: 8.2464 - val_loss: 73.2583 - val_mae: 6.9349\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 121.0685 - mae: 8.1902 - val_loss: 71.7333 - val_mae: 6.8518\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 119.7468 - mae: 8.1327 - val_loss: 70.3226 - val_mae: 6.7729\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 118.6315 - mae: 8.0831 - val_loss: 68.8837 - val_mae: 6.6925\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 117.4908 - mae: 8.0327 - val_loss: 67.5256 - val_mae: 6.6160\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.3115 - mae: 7.9852 - val_loss: 66.2975 - val_mae: 6.5533\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.3179 - mae: 7.9431 - val_loss: 65.1671 - val_mae: 6.4980\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 114.3943 - mae: 7.9046 - val_loss: 64.0552 - val_mae: 6.4420\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 113.3667 - mae: 7.8641 - val_loss: 63.1453 - val_mae: 6.3945\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 112.5463 - mae: 7.8315 - val_loss: 62.0006 - val_mae: 6.3350\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 111.7290 - mae: 7.7999 - val_loss: 61.0819 - val_mae: 6.2848\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.8206 - mae: 7.7669 - val_loss: 60.0328 - val_mae: 6.2289\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 110.0339 - mae: 7.7384 - val_loss: 59.0566 - val_mae: 6.1756\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.3288 - mae: 7.7084 - val_loss: 58.1286 - val_mae: 6.1264\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 108.5827 - mae: 7.6832 - val_loss: 57.4860 - val_mae: 6.0996\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.9828 - mae: 7.6576 - val_loss: 56.6035 - val_mae: 6.0573\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.2349 - mae: 7.6306 - val_loss: 55.9280 - val_mae: 6.0257\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 106.6947 - mae: 7.6130 - val_loss: 55.4247 - val_mae: 6.0032\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.9035 - mae: 7.5881 - val_loss: 54.6721 - val_mae: 5.9649\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 105.3224 - mae: 7.5628 - val_loss: 53.8929 - val_mae: 5.9308\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.7037 - mae: 7.5453 - val_loss: 53.2490 - val_mae: 5.9071\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 104.0317 - mae: 7.5148 - val_loss: 52.4398 - val_mae: 5.8669\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.5066 - mae: 7.4927 - val_loss: 51.7655 - val_mae: 5.8373\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.8924 - mae: 7.4648 - val_loss: 51.2074 - val_mae: 5.8121\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 102.4076 - mae: 7.4472 - val_loss: 50.5602 - val_mae: 5.7827\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.8150 - mae: 7.4222 - val_loss: 50.0644 - val_mae: 5.7598\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 101.3071 - mae: 7.4037 - val_loss: 49.5428 - val_mae: 5.7337\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.8849 - mae: 7.3872 - val_loss: 48.9834 - val_mae: 5.7043\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 100.3871 - mae: 7.3740 - val_loss: 48.6997 - val_mae: 5.6941\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.9848 - mae: 7.3683 - val_loss: 48.3295 - val_mae: 5.6791\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.4971 - mae: 7.3509 - val_loss: 47.8240 - val_mae: 5.6496\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 99.0676 - mae: 7.3286 - val_loss: 47.1974 - val_mae: 5.6066\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.6033 - mae: 7.2993 - val_loss: 46.5512 - val_mae: 5.5595\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 98.1933 - mae: 7.2746 - val_loss: 46.0580 - val_mae: 5.5269\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.7584 - mae: 7.2672 - val_loss: 45.8102 - val_mae: 5.5157\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.3046 - mae: 7.2526 - val_loss: 45.2633 - val_mae: 5.4748\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.8585 - mae: 7.2290 - val_loss: 44.7984 - val_mae: 5.4403\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 96.5099 - mae: 7.2177 - val_loss: 44.5584 - val_mae: 5.4268\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 96.0372 - mae: 7.2034 - val_loss: 44.0943 - val_mae: 5.3936\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 95.6635 - mae: 7.1775 - val_loss: 43.5438 - val_mae: 5.3505\n",
      "5/5 [==============================] - 0s 839us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   3.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3568.5349 - mae: 57.0207 - val_loss: 3686.1467 - val_mae: 57.4658\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3292.2913 - mae: 54.6969 - val_loss: 3408.8237 - val_mae: 55.1638\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3036.9089 - mae: 52.4231 - val_loss: 3144.1521 - val_mae: 52.8779\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2792.7231 - mae: 50.1729 - val_loss: 2895.4141 - val_mae: 50.6383\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2563.5779 - mae: 47.9745 - val_loss: 2661.6833 - val_mae: 48.4477\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2347.2307 - mae: 45.8188 - val_loss: 2444.2124 - val_mae: 46.3174\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2148.5251 - mae: 43.7224 - val_loss: 2241.4355 - val_mae: 44.2429\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1963.1383 - mae: 41.6982 - val_loss: 2053.3140 - val_mae: 42.2356\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1791.2374 - mae: 39.7113 - val_loss: 1878.3759 - val_mae: 40.2800\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1632.7168 - mae: 37.7860 - val_loss: 1715.2645 - val_mae: 38.3731\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1484.8732 - mae: 35.9088 - val_loss: 1565.0133 - val_mae: 36.5292\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1349.5658 - mae: 34.1036 - val_loss: 1426.9429 - val_mae: 34.7517\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1227.7284 - mae: 32.3617 - val_loss: 1297.7065 - val_mae: 33.0074\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1111.2405 - mae: 30.6686 - val_loss: 1180.3998 - val_mae: 31.3545\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1005.6940 - mae: 29.0886 - val_loss: 1073.3672 - val_mae: 29.8480\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 909.5789 - mae: 27.5938 - val_loss: 975.8423 - val_mae: 28.4039\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 821.8115 - mae: 26.1485 - val_loss: 886.4746 - val_mae: 27.0127\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 743.9693 - mae: 24.7697 - val_loss: 802.8504 - val_mae: 25.6412\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 672.0013 - mae: 23.4333 - val_loss: 726.0645 - val_mae: 24.3123\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 604.5986 - mae: 22.1443 - val_loss: 657.2283 - val_mae: 23.0553\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 545.4437 - mae: 20.9273 - val_loss: 594.4141 - val_mae: 21.8415\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 491.6328 - mae: 19.7678 - val_loss: 537.6624 - val_mae: 20.6836\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 442.9355 - mae: 18.6809 - val_loss: 486.6654 - val_mae: 19.5809\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 399.8367 - mae: 17.6318 - val_loss: 440.3147 - val_mae: 18.5212\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 360.1351 - mae: 16.6606 - val_loss: 399.5097 - val_mae: 17.5322\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 325.6076 - mae: 15.7493 - val_loss: 362.6031 - val_mae: 16.5860\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 294.8166 - mae: 14.8962 - val_loss: 329.1030 - val_mae: 15.6775\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 267.6216 - mae: 14.0821 - val_loss: 298.5610 - val_mae: 14.7984\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 242.5628 - mae: 13.3030 - val_loss: 271.6885 - val_mae: 13.9990\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 220.0874 - mae: 12.6037 - val_loss: 247.8605 - val_mae: 13.2717\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 201.4378 - mae: 11.9541 - val_loss: 225.9088 - val_mae: 12.5750\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 184.6598 - mae: 11.3481 - val_loss: 206.2739 - val_mae: 11.9428\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 169.0346 - mae: 10.8191 - val_loss: 189.3683 - val_mae: 11.3633\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.7733 - mae: 10.3264 - val_loss: 174.0850 - val_mae: 10.8503\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 144.4070 - mae: 9.8956 - val_loss: 160.1976 - val_mae: 10.3556\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 133.9336 - mae: 9.4849 - val_loss: 147.8185 - val_mae: 9.9132\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 124.1376 - mae: 9.0939 - val_loss: 137.1620 - val_mae: 9.5156\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 116.4141 - mae: 8.7593 - val_loss: 126.8421 - val_mae: 9.1166\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 109.4647 - mae: 8.4380 - val_loss: 117.5040 - val_mae: 8.7608\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 103.1100 - mae: 8.1436 - val_loss: 109.5223 - val_mae: 8.4365\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 97.5825 - mae: 7.8937 - val_loss: 102.8210 - val_mae: 8.1442\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 93.0103 - mae: 7.6667 - val_loss: 96.8628 - val_mae: 7.8660\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 88.8955 - mae: 7.4571 - val_loss: 91.9620 - val_mae: 7.6351\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 85.5652 - mae: 7.2849 - val_loss: 87.4470 - val_mae: 7.4228\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 82.6450 - mae: 7.1269 - val_loss: 83.3961 - val_mae: 7.2195\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 80.0353 - mae: 6.9834 - val_loss: 79.9369 - val_mae: 7.0582\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 77.6904 - mae: 6.8500 - val_loss: 77.0881 - val_mae: 6.9186\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 76.0025 - mae: 6.7368 - val_loss: 74.2357 - val_mae: 6.7710\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.2639 - mae: 6.6273 - val_loss: 71.6825 - val_mae: 6.6311\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.7315 - mae: 6.5249 - val_loss: 69.5893 - val_mae: 6.5089\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 71.4019 - mae: 6.4357 - val_loss: 67.8678 - val_mae: 6.4029\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 70.3556 - mae: 6.3534 - val_loss: 66.1706 - val_mae: 6.2955\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 69.4510 - mae: 6.2880 - val_loss: 64.5820 - val_mae: 6.2038\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 68.5116 - mae: 6.2197 - val_loss: 63.3437 - val_mae: 6.1321\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.7592 - mae: 6.1656 - val_loss: 62.2831 - val_mae: 6.0689\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 67.0452 - mae: 6.1144 - val_loss: 61.3267 - val_mae: 6.0100\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.4363 - mae: 6.0657 - val_loss: 60.3185 - val_mae: 5.9444\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.8331 - mae: 6.0209 - val_loss: 59.4067 - val_mae: 5.8853\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 65.3273 - mae: 5.9812 - val_loss: 58.5871 - val_mae: 5.8291\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.7992 - mae: 5.9424 - val_loss: 57.8795 - val_mae: 5.7843\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 64.3886 - mae: 5.9108 - val_loss: 57.1787 - val_mae: 5.7357\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.9021 - mae: 5.8756 - val_loss: 56.6236 - val_mae: 5.7001\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.5389 - mae: 5.8469 - val_loss: 56.0720 - val_mae: 5.6640\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.1870 - mae: 5.8174 - val_loss: 55.4697 - val_mae: 5.6191\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.7824 - mae: 5.7867 - val_loss: 54.9885 - val_mae: 5.5911\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.4494 - mae: 5.7595 - val_loss: 54.4742 - val_mae: 5.5547\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 62.1083 - mae: 5.7326 - val_loss: 53.9930 - val_mae: 5.5218\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.7750 - mae: 5.7081 - val_loss: 53.6249 - val_mae: 5.5026\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.4708 - mae: 5.6821 - val_loss: 53.1522 - val_mae: 5.4668\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 61.1448 - mae: 5.6585 - val_loss: 52.7254 - val_mae: 5.4432\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.8525 - mae: 5.6369 - val_loss: 52.2783 - val_mae: 5.4118\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.5157 - mae: 5.6122 - val_loss: 51.9695 - val_mae: 5.3948\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 60.2743 - mae: 5.5915 - val_loss: 51.5490 - val_mae: 5.3664\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.9827 - mae: 5.5688 - val_loss: 51.2044 - val_mae: 5.3539\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.7066 - mae: 5.5509 - val_loss: 50.9102 - val_mae: 5.3437\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.4283 - mae: 5.5295 - val_loss: 50.5795 - val_mae: 5.3282\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.1782 - mae: 5.5124 - val_loss: 50.2945 - val_mae: 5.3230\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.9082 - mae: 5.4994 - val_loss: 49.9829 - val_mae: 5.3122\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.6965 - mae: 5.4886 - val_loss: 49.7101 - val_mae: 5.3098\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.4053 - mae: 5.4722 - val_loss: 49.3470 - val_mae: 5.2908\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 58.1813 - mae: 5.4507 - val_loss: 48.9803 - val_mae: 5.2707\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.9280 - mae: 5.4310 - val_loss: 48.7075 - val_mae: 5.2620\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.7349 - mae: 5.4187 - val_loss: 48.4105 - val_mae: 5.2502\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.5159 - mae: 5.4050 - val_loss: 48.1653 - val_mae: 5.2447\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.3288 - mae: 5.3960 - val_loss: 47.9779 - val_mae: 5.2435\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.1076 - mae: 5.3818 - val_loss: 47.7260 - val_mae: 5.2309\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.9306 - mae: 5.3664 - val_loss: 47.3791 - val_mae: 5.2091\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.7312 - mae: 5.3492 - val_loss: 47.1520 - val_mae: 5.1994\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.5499 - mae: 5.3336 - val_loss: 46.8960 - val_mae: 5.1860\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.3901 - mae: 5.3205 - val_loss: 46.6338 - val_mae: 5.1746\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.2043 - mae: 5.3095 - val_loss: 46.4661 - val_mae: 5.1700\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 56.0505 - mae: 5.2997 - val_loss: 46.2905 - val_mae: 5.1673\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.9012 - mae: 5.2953 - val_loss: 46.1186 - val_mae: 5.1686\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.7609 - mae: 5.2935 - val_loss: 45.9372 - val_mae: 5.1670\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.5961 - mae: 5.2865 - val_loss: 45.7067 - val_mae: 5.1540\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.4756 - mae: 5.2749 - val_loss: 45.4367 - val_mae: 5.1414\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.3151 - mae: 5.2672 - val_loss: 45.3169 - val_mae: 5.1383\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.1743 - mae: 5.2583 - val_loss: 45.1204 - val_mae: 5.1293\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 55.0504 - mae: 5.2499 - val_loss: 44.8750 - val_mae: 5.1144\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 54.9262 - mae: 5.2395 - val_loss: 44.6926 - val_mae: 5.1072\n",
      "5/5 [==============================] - 0s 739us/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>; total time=   3.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00010: early stopping\n",
      "5/5 [==============================] - 0s 769us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00010: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 00010: early stopping\n",
      "5/5 [==============================] - 0s 723us/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>; total time=   0.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:700: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1089, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/scikeras/wrappers.py\", line 1688, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/metrics/_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 721, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  UserWarning,\n",
      "/opt/anaconda3/envs/tensorflowEnv/lib/python3.6/site-packages/sklearn/model_selection/_search.py:925: UserWarning: One or more of the test scores are non-finite: [-2.82116136e+00             nan -6.78336352e+00  1.22822415e-01\n",
      "  2.70128631e-01 -2.73316353e+01  1.22793741e-01  2.07905418e-01\n",
      "  2.27569530e-01  2.02920066e-01 -3.30514537e+01 -2.43486194e+00\n",
      " -5.04760967e+00             nan  8.40629419e-02  2.42572425e-01\n",
      " -3.00650742e+09 -6.55520483e-01 -4.72404152e-01 -3.18789174e+01\n",
      "             nan -3.10945236e+02 -1.55563023e+01  2.14808564e-01\n",
      " -3.37232153e+01 -3.91956423e-01             nan  1.44896679e-01\n",
      "  1.04460362e-01             nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 10ms/step - loss: 631.7487 - mae: 21.0473 - val_loss: 282.1006 - val_mae: 15.0685\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 297.4659 - mae: 14.1535 - val_loss: 166.9690 - val_mae: 11.0536\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 209.7504 - mae: 11.4012 - val_loss: 109.7118 - val_mae: 8.7071\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 166.6617 - mae: 9.7230 - val_loss: 80.9416 - val_mae: 7.3247\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 140.6457 - mae: 8.8718 - val_loss: 60.3580 - val_mae: 6.2856\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 119.4611 - mae: 8.1387 - val_loss: 51.7250 - val_mae: 5.7813\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 107.7082 - mae: 7.6334 - val_loss: 44.6361 - val_mae: 5.4706\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 101.7135 - mae: 7.4384 - val_loss: 41.4774 - val_mae: 5.2686\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 98.1214 - mae: 7.2683 - val_loss: 37.8116 - val_mae: 5.0020\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 93.9030 - mae: 7.0985 - val_loss: 35.9581 - val_mae: 4.8814\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 91.9428 - mae: 7.0809 - val_loss: 34.5054 - val_mae: 4.7541\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 89.9043 - mae: 6.9271 - val_loss: 34.2302 - val_mae: 4.7334\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 88.4829 - mae: 6.8848 - val_loss: 33.3649 - val_mae: 4.6836\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 86.6520 - mae: 6.8850 - val_loss: 32.5002 - val_mae: 4.5980\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 85.6530 - mae: 6.8060 - val_loss: 31.8463 - val_mae: 4.5600\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.6689 - mae: 6.7365 - val_loss: 31.6816 - val_mae: 4.5294\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 83.0208 - mae: 6.5981 - val_loss: 44.6696 - val_mae: 5.5675\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 84.7180 - mae: 6.9456 - val_loss: 30.7480 - val_mae: 4.4837\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 82.5362 - mae: 6.6129 - val_loss: 31.0078 - val_mae: 4.5463\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 81.6822 - mae: 6.6926 - val_loss: 30.1999 - val_mae: 4.4088\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.7568 - mae: 6.5610 - val_loss: 39.1729 - val_mae: 5.1751\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 81.6062 - mae: 6.6671 - val_loss: 34.1108 - val_mae: 4.8156\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.7330 - mae: 6.6415 - val_loss: 29.4924 - val_mae: 4.3820\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.8895 - mae: 6.5889 - val_loss: 29.6368 - val_mae: 4.4161\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 80.0650 - mae: 6.5387 - val_loss: 30.7583 - val_mae: 4.5808\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.5064 - mae: 6.4999 - val_loss: 29.1135 - val_mae: 4.3706\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.3952 - mae: 6.4858 - val_loss: 29.1310 - val_mae: 4.3009\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.6782 - mae: 6.4156 - val_loss: 29.6951 - val_mae: 4.4981\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 78.2688 - mae: 6.4947 - val_loss: 28.8010 - val_mae: 4.2965\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.9289 - mae: 6.3434 - val_loss: 30.5447 - val_mae: 4.5931\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.5301 - mae: 6.4416 - val_loss: 30.5189 - val_mae: 4.5886\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 76.5793 - mae: 6.4012 - val_loss: 30.9949 - val_mae: 4.6240\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 77.1696 - mae: 6.4771 - val_loss: 28.7358 - val_mae: 4.4155\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7f9eb9bae588>], model=<function build_model at 0x7f9ed4d107b8>),\n",
       "                   n_iter=30,\n",
       "                   param_distributions={'model__learning_rate': [1e-06, 1e-05,\n",
       "                                                                 0.0001],\n",
       "                                        'model__momentum': [0.1, 0.5, 0.9],\n",
       "                                        'model__n_hidden': [0, 1, 2, 3],\n",
       "                                        'model__n_neurons': [5, 25, 125],\n",
       "                                        'model__optimizer': [<keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ece2983c8>,\n",
       "                                                             <keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795278>,\n",
       "                                                             <keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ebb795f98>,\n",
       "                                                             <keras.optimizer_v2.adam.Adam object at 0x7f9ed65cceb8>]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs,n_iter=30, cv=3, verbose=2)\n",
    "\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb09fe4b-3eeb-40c7-9523-a153bf965d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__optimizer': <keras.optimizer_v2.gradient_descent.SGD at 0x7f9ece2983c8>,\n",
       " 'model__n_neurons': 25,\n",
       " 'model__n_hidden': 3,\n",
       " 'model__momentum': 0.5,\n",
       " 'model__learning_rate': 1e-06}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71ebc747-be85-416c-bf9b-aaebc5e13bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"rnd_search.pkl\",\"wb\")\n",
    "pickle.dump(rnd_search_cv.best_params_, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "cleanSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100f2cd-9f1a-4e0e-8de8-84f6cb73d121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
